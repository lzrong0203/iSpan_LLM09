# äººå·¥æ™ºæ…§å¤§å‹èªè¨€æ¨¡å‹å¯¦ä½œæ‡‰ç”¨ç­
## Large Language Model Practical Application

**èª²ç¨‹æ™‚é–“ï¼š2025å¹´9æœˆ13-14æ—¥**  
**ç¸½æ™‚æ•¸ï¼š12å°æ™‚**

---

> âš ï¸ **é‡è¦æé†’ / Important Notice**
> 
> æœ¬æ•™æå…§å®¹ç‚º AI (Claude Opus 4.1) è‡ªå‹•ç”Ÿæˆçš„åˆæ­¥ç‰ˆæœ¬ï¼Œåƒ…ä¾›åƒè€ƒä½¿ç”¨ã€‚
> 
> å¯¦éš›èª²ç¨‹å…§å®¹å°‡æ ¹æ“šæˆèª²éœ€æ±‚æŒçºŒèª¿æ•´èˆ‡å„ªåŒ–ï¼Œè«‹ä»¥æœ€çµ‚ç‰ˆæœ¬ç‚ºæº–ã€‚
> 
> *This material is an AI-generated preliminary version for reference only.*
> 
> *The actual course content will be continuously adjusted and optimized based on teaching requirements.*

---

## ğŸ“ èª²ç¨‹è³‡æ–™

- **æŠ•å½±ç‰‡ï¼š** [2025-0913 LLM.pdf](./2025-0913%20LLM.pdf)
- **æ›´æ–°æ™‚é–“ï¼š** 2025å¹´9æœˆ9æ—¥

---

# èª²ç¨‹å¤§ç¶±ç¸½è¦½

## ç¬¬ä¸€å¤© (6å°æ™‚)
- **09:00-10:30** - LLMåŸºæœ¬æ¦‚å¿µèˆ‡æ¶æ§‹
- **10:45-12:00** - ç’°å¢ƒè¨­ç½®èˆ‡å·¥å…·æº–å‚™
- **13:00-14:30** - ä¸ä½¿ç”¨æ¡†æ¶é–‹ç™¼Chatbot (Part 1)
- **14:45-16:00** - ä¸ä½¿ç”¨æ¡†æ¶é–‹ç™¼Chatbot (Part 2)

## ç¬¬äºŒå¤© (6å°æ™‚)
- **09:00-10:30** - å¾®èª¿Llamaæ¨¡å‹
- **10:45-12:00** - LoRAæŠ€è¡“èˆ‡Prompt Engineering
- **13:00-14:30** - RAGæŠ€è¡“æ·±å…¥æ‡‰ç”¨
- **14:45-16:00** - LLM Agentå¯¦ä½œèˆ‡éƒ¨ç½²

---

# æ¨¡çµ„ä¸€ï¼šå¤§å‹èªè¨€æ¨¡å‹åŸºæœ¬æ¦‚å¿µ

## ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹ï¼Ÿ

**å®šç¾©ï¼š** åŸºæ–¼æ·±åº¦å­¸ç¿’çš„è‡ªç„¶èªè¨€è™•ç†æ¨¡å‹ï¼Œèƒ½å¤ ç†è§£å’Œç”Ÿæˆäººé¡èªè¨€

**ç‰¹é»ï¼š** åƒæ•¸é‡é¾å¤§ï¼ˆæ•¸åå„„åˆ°æ•¸åƒå„„ï¼‰ã€é è¨“ç·´-å¾®èª¿æ¶æ§‹ã€å¼·å¤§çš„æ³›åŒ–èƒ½åŠ›

## LLMçš„æ ¸å¿ƒæ¶æ§‹

```
è¼¸å…¥æ–‡æœ¬ â†’ Tokenization â†’ Transformer â†’ è¼¸å‡ºé æ¸¬
```

### é—œéµçµ„ä»¶èªªæ˜ï¼š
- **Tokenizationï¼š** å°‡æ–‡æœ¬è½‰æ›ç‚ºæ¨¡å‹å¯ç†è§£çš„æ•¸å­—åºåˆ—
- **Embedding Layerï¼š** å°‡tokenæ˜ å°„åˆ°é«˜ç¶­å‘é‡ç©ºé–“
- **Transformer Blocksï¼š** è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ + å‰é¥‹ç¶²çµ¡
- **Output Layerï¼š** ç”Ÿæˆä¸‹ä¸€å€‹tokençš„æ¦‚ç‡åˆ†å¸ƒ

---

# Transformeræ¶æ§‹æ·±å…¥å‰–æ

## Self-Attentionæ©Ÿåˆ¶

```python
# Self-Attention è¨ˆç®—æµç¨‹
def attention(Q, K, V):
    """
    Q (Query): æŸ¥è©¢å‘é‡
    K (Key): éµå‘é‡  
    V (Value): å€¼å‘é‡
    d_k: éµå‘é‡çš„ç¶­åº¦
    """
    d_k = K.shape[-1]
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    attention_weights = torch.softmax(scores, dim=-1)
    output = torch.matmul(attention_weights, V)
    return output
```

## Multi-Head Attention
- å¹³è¡ŒåŸ·è¡Œå¤šå€‹attentionæ“ä½œ
- æ•æ‰ä¸åŒå±¤æ¬¡çš„èªç¾©é—œä¿‚
- å¢å¼·æ¨¡å‹çš„è¡¨é”èƒ½åŠ›

## Position Encoding

**ç‚ºä»€éº¼éœ€è¦ä½ç½®ç·¨ç¢¼ï¼Ÿ**
Transformeræœ¬èº«æ²’æœ‰é †åºæ¦‚å¿µï¼Œéœ€è¦é€šéä½ç½®ç·¨ç¢¼ä¾†æ³¨å…¥åºåˆ—ä¿¡æ¯

---

# ä¸»æµLLMæ¨¡å‹æ¯”è¼ƒ

| æ¨¡å‹ç³»åˆ— | é–‹ç™¼è€… | åƒæ•¸é‡ | ç‰¹é» | æ‡‰ç”¨å ´æ™¯ |
|---------|--------|--------|------|----------|
| **GPT-4** | OpenAI | ç´„1.8T | å¤šæ¨¡æ…‹ã€å¼·å¤§æ¨ç†èƒ½åŠ› | é€šç”¨å°è©±ã€ç¨‹å¼ç¢¼ç”Ÿæˆ |
| **Llama 3** | Meta | 8B-70B | é–‹æºã€å¯æœ¬åœ°éƒ¨ç½² | ä¼æ¥­ç§æœ‰åŒ–éƒ¨ç½² |
| **Claude** | Anthropic | æœªå…¬é–‹ | å®‰å…¨æ€§é«˜ã€é•·æ–‡æœ¬è™•ç† | æ–‡æª”åˆ†æã€å®‰å…¨æ‡‰ç”¨ |
| **Gemini** | Google | å¤šç‰ˆæœ¬ | åŸç”Ÿå¤šæ¨¡æ…‹ | å¤šåª’é«”ç†è§£èˆ‡ç”Ÿæˆ |

## é¸æ“‡è€ƒé‡å› ç´ 
- **æˆæœ¬ï¼š** APIè²»ç”¨ vs æœ¬åœ°éƒ¨ç½²æˆæœ¬
- **éš±ç§ï¼š** è³‡æ–™å®‰å…¨æ€§éœ€æ±‚
- **æ•ˆèƒ½ï¼š** æ¨ç†é€Ÿåº¦èˆ‡æº–ç¢ºåº¦å¹³è¡¡
- **å®¢è£½åŒ–ï¼š** æ˜¯å¦éœ€è¦å¾®èª¿

---

# æ¨¡çµ„äºŒï¼šç’°å¢ƒè¨­ç½®èˆ‡å·¥å…·æº–å‚™

## 1. Pythonç’°å¢ƒé…ç½®

```bash
# å»ºè­°ä½¿ç”¨Python 3.9+
# å‰µå»ºè™›æ“¬ç’°å¢ƒ
python -m venv llm_env
source llm_env/bin/activate  # Linux/Mac
# æˆ–
llm_env\Scripts\activate  # Windows

# å®‰è£å¿…è¦å¥—ä»¶
pip install torch torchvision transformers
pip install numpy pandas matplotlib
pip install sentencepiece protobuf
pip install accelerate bitsandbytes
```

## 2. GPUç’°å¢ƒæª¢æŸ¥

```python
import torch

print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"GPUæ•¸é‡: {torch.cuda.device_count()}")
    print(f"GPUåç¨±: {torch.cuda.get_device_name(0)}")
```

**âš ï¸ æ³¨æ„ï¼š** é‹è¡Œå¤§å‹æ¨¡å‹å»ºè­°è‡³å°‘16GB VRAMï¼Œå¦‚ç„¡GPUå¯ä½¿ç”¨Google Colab

---

# OpenAI APIè¨­ç½®èˆ‡ä½¿ç”¨

## API Keyç”³è«‹æµç¨‹
1. è¨ªå• platform.openai.com
2. è¨»å†Š/ç™»å…¥å¸³è™Ÿ
3. å‰å¾€ API Keys é é¢
4. å‰µå»ºæ–°çš„ API Key
5. å®‰å…¨ä¿å­˜ Keyï¼ˆåªé¡¯ç¤ºä¸€æ¬¡ï¼‰

## åŸºç¤APIèª¿ç”¨ç¯„ä¾‹

```python
import openai
import os

# è¨­ç½®API Key
openai.api_key = os.getenv("OPENAI_API_KEY")

# åŸºæœ¬å°è©±èª¿ç”¨
def chat_with_gpt(prompt, model="gpt-4"):
    response = openai.ChatCompletion.create(
        model=model,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„AIåŠ©æ‰‹"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=1000
    )
    return response.choices[0].message.content

# ä½¿ç”¨ç¯„ä¾‹
result = chat_with_gpt("è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’")
print(result)
```

---

# Llama 3 æœ¬åœ°éƒ¨ç½²å¯¦æˆ°

## æ–¹æ³•ä¸€ï¼šä½¿ç”¨Hugging Face

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# è¼‰å…¥æ¨¡å‹å’Œtokenizer
model_name = "meta-llama/Llama-3-8B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# ç”Ÿæˆæ–‡æœ¬
def generate_text(prompt, max_length=100):
    inputs = tokenizer(prompt, return_tensors="pt")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=0.7,
            do_sample=True
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## æ–¹æ³•äºŒï¼šä½¿ç”¨llama.cpp (æ›´çœè³‡æº)

```python
# å®‰è£llama-cpp-python
# pip install llama-cpp-python

from llama_cpp import Llama

# è¼‰å…¥é‡åŒ–æ¨¡å‹
llm = Llama(
    model_path="./models/llama-3-8b.Q4_K_M.gguf",
    n_ctx=2048,
    n_threads=8
)

# ç”Ÿæˆå›æ‡‰
output = llm(
    "Q: ä»€éº¼æ˜¯äººå·¥æ™ºæ…§? A:", 
    max_tokens=256
)
print(output['choices'][0]['text'])
```

---

# æ¨¡çµ„ä¸‰ï¼šä¸ä½¿ç”¨æ¡†æ¶é–‹ç™¼Chatbot (Part 1)

## ç‚ºä»€éº¼ä¸ä½¿ç”¨LangChainï¼Ÿ

### å„ªå‹¢
- **æ›´æ·±å…¥ç†è§£ï¼š** æŒæ¡åº•å±¤é‹ä½œåŸç†
- **éˆæ´»æ€§æ›´é«˜ï¼š** å¯å®Œå…¨å®¢è£½åŒ–æµç¨‹
- **æ•ˆèƒ½å„ªåŒ–ï¼š** é¿å…ä¸å¿…è¦çš„æŠ½è±¡å±¤
- **é™¤éŒ¯æ›´å®¹æ˜“ï¼š** ç›´æ¥æ§åˆ¶æ¯å€‹æ­¥é©Ÿ

## åŸºç¤èŠå¤©æ©Ÿå™¨äººæ¶æ§‹

```python
class SimpleChatbot:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.conversation_history = []
        
    def add_message(self, role, content):
        self.conversation_history.append({
            "role": role,
            "content": content
        })
        
    def generate_response(self, user_input):
        # æ·»åŠ ç”¨æˆ¶è¼¸å…¥åˆ°æ­·å²
        self.add_message("user", user_input)
        
        # æ§‹å»ºprompt
        prompt = self._build_prompt()
        
        # ç”Ÿæˆå›æ‡‰
        response = self._generate(prompt)
        
        # æ·»åŠ å›æ‡‰åˆ°æ­·å²
        self.add_message("assistant", response)
        
        return response
        
    def _build_prompt(self):
        # å°‡å°è©±æ­·å²è½‰æ›ç‚ºæ¨¡å‹è¼¸å…¥æ ¼å¼
        prompt = ""
        for msg in self.conversation_history:
            if msg["role"] == "user":
                prompt += f"ç”¨æˆ¶: {msg['content']}\n"
            else:
                prompt += f"åŠ©æ‰‹: {msg['content']}\n"
        prompt += "åŠ©æ‰‹: "
        return prompt
```

---

# è³‡æ–™è™•ç†èˆ‡å‘é‡åµŒå…¥

## æ–‡æœ¬é è™•ç†

```python
import re
from typing import List

class TextProcessor:
    def __init__(self):
        self.chunk_size = 512
        self.overlap = 50
        
    def clean_text(self, text: str) -> str:
        # ç§»é™¤å¤šé¤˜ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s\u4e00-\u9fffã€‚ï¼Œï¼ï¼Ÿ]', '', text)
        return text.strip()
    
    def split_into_chunks(self, text: str) -> List[str]:
        """å°‡é•·æ–‡æœ¬åˆ‡åˆ†ç‚ºå›ºå®šå¤§å°çš„chunks"""
        chunks = []
        text = self.clean_text(text)
        
        for i in range(0, len(text), self.chunk_size - self.overlap):
            chunk = text[i:i + self.chunk_size]
            if chunk:
                chunks.append(chunk)
        
        return chunks
    
    def load_documents(self, file_paths: List[str]) -> List[str]:
        """è¼‰å…¥ä¸¦è™•ç†å¤šå€‹æ–‡æª”"""
        all_chunks = []
        for path in file_paths:
            with open(path, 'r', encoding='utf-8') as f:
                text = f.read()
                chunks = self.split_into_chunks(text)
                all_chunks.extend(chunks)
        return all_chunks
```

## ç”Ÿæˆå‘é‡åµŒå…¥

```python
from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingGenerator:
    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)
        
    def encode_texts(self, texts: List[str]) -> np.ndarray:
        """å°‡æ–‡æœ¬åˆ—è¡¨è½‰æ›ç‚ºå‘é‡"""
        embeddings = self.model.encode(
            texts,
            convert_to_numpy=True,
            show_progress_bar=True
        )
        return embeddings
    
    def compute_similarity(self, query_embedding, doc_embeddings):
        """è¨ˆç®—æŸ¥è©¢èˆ‡æ–‡æª”çš„ç›¸ä¼¼åº¦"""
        from sklearn.metrics.pairwise import cosine_similarity
        similarities = cosine_similarity(
            query_embedding.reshape(1, -1),
            doc_embeddings
        )[0]
        return similarities
```

---

# æ¨¡çµ„å››ï¼šRAGç³»çµ±å¯¦ä½œ

## ä»€éº¼æ˜¯RAG (Retrieval Augmented Generation)?

### RAGå·¥ä½œæµç¨‹
```
ç”¨æˆ¶æŸ¥è©¢ â†’ æª¢ç´¢ç›¸é—œæ–‡æª” â†’ ä¸Šä¸‹æ–‡å¢å¼· â†’ ç”Ÿæˆå›ç­”
```

## å®Œæ•´RAGç³»çµ±å¯¦ä½œ

```python
class RAGSystem:
    def __init__(self, llm_model, embedding_model):
        self.llm = llm_model
        self.embedder = EmbeddingGenerator(embedding_model)
        self.documents = []
        self.doc_embeddings = None
        
    def index_documents(self, documents: List[str]):
        """ç´¢å¼•æ–‡æª”ä¸¦ç”ŸæˆåµŒå…¥"""
        self.documents = documents
        self.doc_embeddings = self.embedder.encode_texts(documents)
        print(f"å·²ç´¢å¼• {len(documents)} å€‹æ–‡æª”ç‰‡æ®µ")
        
    def retrieve_relevant_docs(self, query: str, top_k: int = 3):
        """æª¢ç´¢æœ€ç›¸é—œçš„æ–‡æª”"""
        query_embedding = self.embedder.encode_texts([query])
        similarities = self.embedder.compute_similarity(
            query_embedding,
            self.doc_embeddings
        )
        
        # ç²å–top-kæœ€ç›¸é—œçš„æ–‡æª”
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        relevant_docs = [self.documents[i] for i in top_indices]
        scores = [similarities[i] for i in top_indices]
        
        return relevant_docs, scores
    
    def generate_answer(self, query: str, context_docs: List[str]):
        """åŸºæ–¼æª¢ç´¢çš„æ–‡æª”ç”Ÿæˆç­”æ¡ˆ"""
        # æ§‹å»ºå¢å¼·çš„prompt
        context = "\n\n".join(context_docs)
        prompt = f"""
        åŸºæ–¼ä»¥ä¸‹ç›¸é—œæ–‡æª”å›ç­”å•é¡Œï¼š
        
        æ–‡æª”å…§å®¹ï¼š
        {context}
        
        å•é¡Œï¼š{query}
        
        è«‹æ ¹æ“šæä¾›çš„æ–‡æª”å…§å®¹å›ç­”ï¼Œå¦‚æœæ–‡æª”ä¸­æ²’æœ‰ç›¸é—œä¿¡æ¯ï¼Œè«‹èªªæ˜ã€‚
        
        å›ç­”ï¼š
        """
        
        # ä½¿ç”¨LLMç”Ÿæˆå›ç­”
        response = self.llm.generate(prompt)
        return response
        
    def query(self, question: str):
        """å®Œæ•´çš„RAGæŸ¥è©¢æµç¨‹"""
        # 1. æª¢ç´¢ç›¸é—œæ–‡æª”
        relevant_docs, scores = self.retrieve_relevant_docs(question)
        
        # 2. ç”Ÿæˆç­”æ¡ˆ
        answer = self.generate_answer(question, relevant_docs)
        
        return {
            "question": question,
            "answer": answer,
            "sources": relevant_docs,
            "scores": scores
        }
```

---

# æ¨¡çµ„äº”ï¼šä½¿ç”¨è‡ªæœ‰è³‡æ–™å¾®èª¿Llamaæ¨¡å‹

## å¾®èª¿çš„æ„ç¾©èˆ‡æ‡‰ç”¨å ´æ™¯
- **é ˜åŸŸé©æ‡‰ï¼š** è®“æ¨¡å‹å­¸ç¿’ç‰¹å®šé ˜åŸŸçŸ¥è­˜
- **ä»»å‹™å„ªåŒ–ï¼š** é‡å°ç‰¹å®šä»»å‹™æå‡è¡¨ç¾
- **é¢¨æ ¼èª¿æ•´ï¼š** èª¿æ•´å›æ‡‰é¢¨æ ¼ç¬¦åˆéœ€æ±‚

## æº–å‚™è¨“ç·´è³‡æ–™

```python
import json
import pandas as pd

class DatasetPreparer:
    def __init__(self):
        self.data = []
        
    def create_instruction_dataset(self, qa_pairs):
        """å‰µå»ºæŒ‡ä»¤å¾®èª¿è³‡æ–™é›†"""
        formatted_data = []
        
        for qa in qa_pairs:
            entry = {
                "instruction": qa["question"],
                "input": "",
                "output": qa["answer"]
            }
            formatted_data.append(entry)
        
        return formatted_data
    
    def save_dataset(self, data, output_path):
        """ä¿å­˜è³‡æ–™é›†ç‚ºJSONæ ¼å¼"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def create_conversation_format(self, conversations):
        """å‰µå»ºå°è©±æ ¼å¼çš„è¨“ç·´è³‡æ–™"""
        formatted = []
        for conv in conversations:
            messages = []
            for turn in conv:
                messages.append({
                    "role": turn["role"],
                    "content": turn["content"]
                })
            formatted.append({"messages": messages})
        
        return formatted
```

## è³‡æ–™æ ¼å¼ç¯„ä¾‹

```json
{
  "instruction": "ä»€éº¼æ˜¯æ·±åº¦å­¸ç¿’ï¼Ÿ",
  "input": "",
  "output": "æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„åˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²çµ¡..."
}
```

---

# å¾®èª¿è¨“ç·´å¯¦ä½œ

## ä½¿ç”¨Transformersé€²è¡Œå¾®èª¿

```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset

class LlamaFineTuner:
    def __init__(self, model_name, output_dir):
        self.model_name = model_name
        self.output_dir = output_dir
        self.model = None
        self.tokenizer = None
        
    def load_model(self):
        """è¼‰å…¥é è¨“ç·´æ¨¡å‹"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # æ·»åŠ padding token
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
    def prepare_dataset(self, data):
        """æº–å‚™è¨“ç·´è³‡æ–™é›†"""
        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"],
                truncation=True,
                padding="max_length",
                max_length=512
            )
        
        dataset = Dataset.from_dict({"text": data})
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        return tokenized_dataset
    
    def setup_training_args(self):
        """è¨­ç½®è¨“ç·´åƒæ•¸"""
        return TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=4,
            per_device_eval_batch_size=4,
            warmup_steps=100,
            weight_decay=0.01,
            logging_dir='./logs',
            logging_steps=10,
            save_steps=500,
            evaluation_strategy="steps",
            eval_steps=500,
            save_total_limit=2,
            load_best_model_at_end=True,
            gradient_accumulation_steps=4,
            fp16=True,
        )
    
    def train(self, train_dataset, eval_dataset=None):
        """åŸ·è¡Œè¨“ç·´"""
        training_args = self.setup_training_args()
        
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False
            ),
        )
        
        # é–‹å§‹è¨“ç·´
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        self.tokenizer.save_pretrained(self.output_dir)
```

---

# æ¨¡çµ„å…­ï¼šLoRAæŠ€è¡“æ‡‰ç”¨

## ä»€éº¼æ˜¯LoRAï¼Ÿ

**LoRA (Low-Rank Adaptation)**
ä¸€ç¨®åƒæ•¸é«˜æ•ˆçš„å¾®èª¿æ–¹æ³•ï¼Œé€šéåœ¨åŸå§‹æ¨¡å‹ä¸­æ’å…¥å¯è¨“ç·´çš„ä½ç§©çŸ©é™£ï¼Œå¤§å¹…æ¸›å°‘éœ€è¦è¨“ç·´çš„åƒæ•¸é‡

## LoRAçš„å„ªå‹¢
- âš¡ **è¨“ç·´æ•ˆç‡é«˜ï¼š** åªéœ€è¨“ç·´å°‘é‡åƒæ•¸ï¼ˆé€šå¸¸< 1%ï¼‰
- ğŸ’¾ **å„²å­˜ç©ºé–“å°ï¼š** LoRAæ¬Šé‡æª”æ¡ˆé€šå¸¸åªæœ‰å¹¾MB
- ğŸ”„ **åˆ‡æ›æ–¹ä¾¿ï¼š** å¯ä»¥å¿«é€Ÿåˆ‡æ›ä¸åŒçš„LoRAé©é…å™¨
- ğŸ¯ **ä»»å‹™ç‰¹å®šï¼š** ç‚ºä¸åŒä»»å‹™è¨“ç·´ä¸åŒçš„LoRA

## LoRAå¯¦ä½œç¨‹å¼ç¢¼

```python
from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForCausalLM

class LoRATrainer:
    def __init__(self, base_model_name):
        self.base_model_name = base_model_name
        
    def setup_lora_model(self):
        """è¨­ç½®LoRAé…ç½®ä¸¦å‰µå»ºPEFTæ¨¡å‹"""
        # è¼‰å…¥åŸºç¤æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # LoRAé…ç½®
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=16,  # LoRAç§©
            lora_alpha=32,  # LoRAç¸®æ”¾åƒæ•¸
            lora_dropout=0.1,  # Dropoutç‡
            target_modules=[  # è¦æ‡‰ç”¨LoRAçš„æ¨¡çµ„
                "q_proj",
                "v_proj",
                "k_proj",
                "o_proj"
            ],
            bias="none"
        )
        
        # å‰µå»ºPEFTæ¨¡å‹
        peft_model = get_peft_model(base_model, lora_config)
        
        # é¡¯ç¤ºå¯è¨“ç·´åƒæ•¸çµ±è¨ˆ
        peft_model.print_trainable_parameters()
        
        return peft_model
    
    def merge_and_save(self, peft_model, output_path):
        """åˆä½µLoRAæ¬Šé‡ä¸¦ä¿å­˜"""
        # åˆä½µæ¬Šé‡
        merged_model = peft_model.merge_and_unload()
        
        # ä¿å­˜åˆä½µå¾Œçš„æ¨¡å‹
        merged_model.save_pretrained(output_path)
        
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
```

---

# æ¨¡çµ„ä¸ƒï¼šPrompt Engineeringç²¾è¦

## Promptè¨­è¨ˆåŸå‰‡

### ğŸ¯ æ ¸å¿ƒåŸå‰‡
1. **æ˜ç¢ºæ€§ï¼š** æ¸…æ¥šè¡¨é”ä½ çš„éœ€æ±‚
2. **å…·é«”æ€§ï¼š** æä¾›å…·é«”çš„æŒ‡ç¤ºå’Œç¯„ä¾‹
3. **çµæ§‹åŒ–ï¼š** ä½¿ç”¨æ¸…æ™°çš„æ ¼å¼å’Œåˆ†éš”ç¬¦
4. **è§’è‰²è¨­å®šï¼š** è³¦äºˆæ¨¡å‹æ˜ç¢ºçš„è§’è‰²

## å¸¸ç”¨PromptæŠ€å·§

| æŠ€å·§ | èªªæ˜ | ç¯„ä¾‹ |
|-----|------|------|
| **Few-shot** | æä¾›ç¯„ä¾‹ | è¼¸å…¥ï¼šè˜‹æœâ†’æ°´æœ<br>è¼¸å…¥ï¼šæ±½è»Šâ†’? |
| **Chain-of-Thought** | æ­¥é©Ÿæ€è€ƒ | è®“æˆ‘å€‘ä¸€æ­¥æ­¥æ€è€ƒé€™å€‹å•é¡Œ... |
| **Role Playing** | è§’è‰²æ‰®æ¼” | ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ•¸æ“šåˆ†æå¸«... |
| **Output Format** | æŒ‡å®šæ ¼å¼ | è«‹ä»¥JSONæ ¼å¼å›ç­”... |

## é€²éšPromptç¯„ä¾‹

```python
class PromptTemplates:
    @staticmethod
    def create_system_prompt(role, constraints):
        return f"""
        ä½ æ˜¯ä¸€ä½{role}ã€‚
        
        ä½ çš„ä»»å‹™è¦ç¯„ï¼š
        {constraints}
        
        è«‹å§‹çµ‚éµå®ˆä»¥ä¸Šè¦ç¯„å›ç­”å•é¡Œã€‚
        """
    
    @staticmethod
    def create_cot_prompt(question):
        return f"""
        å•é¡Œï¼š{question}
        
        è®“æˆ‘å€‘ä¸€æ­¥æ­¥åˆ†æé€™å€‹å•é¡Œï¼š
        1. é¦–å…ˆï¼Œç†è§£å•é¡Œçš„é—œéµè¦ç´ 
        2. å…¶æ¬¡ï¼Œåˆ†æå¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ
        3. ç„¶å¾Œï¼Œè©•ä¼°æ¯å€‹æ–¹æ¡ˆçš„å„ªç¼ºé»
        4. æœ€å¾Œï¼Œçµ¦å‡ºæœ€ä½³å»ºè­°
        
        åˆ†æéç¨‹ï¼š
        """
    
    @staticmethod
    def create_few_shot_prompt(examples, query):
        prompt = "ä»¥ä¸‹æ˜¯ä¸€äº›ç¯„ä¾‹ï¼š\n\n"
        for ex in examples:
            prompt += f"è¼¸å…¥ï¼š{ex['input']}\n"
            prompt += f"è¼¸å‡ºï¼š{ex['output']}\n\n"
        prompt += f"ç¾åœ¨ï¼Œè«‹è™•ç†ä»¥ä¸‹è¼¸å…¥ï¼š\n"
        prompt += f"è¼¸å…¥ï¼š{query}\nè¼¸å‡ºï¼š"
        return prompt
```

---

# æ¨¡çµ„å…«ï¼šRAGæŠ€è¡“æ·±å…¥æ‡‰ç”¨

## é€²éšRAGå„ªåŒ–ç­–ç•¥

### 1. Hybrid Search (æ··åˆæª¢ç´¢)

```python
class HybridRAG:
    def __init__(self):
        self.dense_retriever = None  # å‘é‡æª¢ç´¢
        self.sparse_retriever = None  # BM25æª¢ç´¢
        
    def hybrid_search(self, query, alpha=0.5):
        """çµåˆå¯†é›†å’Œç¨€ç–æª¢ç´¢"""
        # å¯†é›†æª¢ç´¢ï¼ˆå‘é‡ç›¸ä¼¼åº¦ï¼‰
        dense_scores = self.dense_retriever.search(query)
        
        # ç¨€ç–æª¢ç´¢ï¼ˆBM25ï¼‰
        sparse_scores = self.sparse_retriever.search(query)
        
        # åˆ†æ•¸èåˆ
        combined_scores = (
            alpha * dense_scores + 
            (1 - alpha) * sparse_scores
        )
        
        return combined_scores
```

### 2. Query Expansion (æŸ¥è©¢æ“´å±•)

```python
def expand_query(original_query, llm):
    """ä½¿ç”¨LLMæ“´å±•æŸ¥è©¢"""
    prompt = f"""
    åŸå§‹æŸ¥è©¢ï¼š{original_query}
    
    è«‹ç”Ÿæˆ3å€‹ç›¸é—œçš„æ“´å±•æŸ¥è©¢ï¼Œä»¥ç²å¾—æ›´å…¨é¢çš„æœç´¢çµæœï¼š
    1.
    2.
    3.
    """
    
    expanded = llm.generate(prompt)
    return [original_query] + expanded.split('\n')
```

### 3. Re-ranking (é‡æ–°æ’åº)

**å…©éšæ®µæª¢ç´¢ç­–ç•¥ï¼š**
- ç¬¬ä¸€éšæ®µï¼šå¿«é€Ÿæª¢ç´¢å¤§é‡å€™é¸æ–‡æª”
- ç¬¬äºŒéšæ®µï¼šä½¿ç”¨æ›´ç²¾ç¢ºçš„æ¨¡å‹é‡æ–°æ’åº

---

# æ¨¡çµ„ä¹ï¼šLLM Agentç³»çµ±è¨­è¨ˆ

## ä»€éº¼æ˜¯LLM Agentï¼Ÿ

LLM Agentæ˜¯èƒ½å¤ è‡ªä¸»åŸ·è¡Œä»»å‹™ã€ä½¿ç”¨å·¥å…·ã€èˆ‡ç’°å¢ƒäº’å‹•çš„æ™ºèƒ½ç³»çµ±

## Agentæ ¸å¿ƒçµ„ä»¶

```
æ„ŸçŸ¥(Perception) â†’ æ¨ç†(Reasoning) â†’ è¡Œå‹•(Action) â†’ è¨˜æ†¶(Memory)
```

## åŸºç¤Agentå¯¦ä½œ

```python
class SimpleLLMAgent:
    def __init__(self, llm, tools):
        self.llm = llm
        self.tools = tools
        self.memory = []
        self.max_iterations = 5
        
    def think(self, task, context):
        """æ¨ç†ä¸‹ä¸€æ­¥è¡Œå‹•"""
        prompt = f"""
        ä»»å‹™ï¼š{task}
        ç•¶å‰ä¸Šä¸‹æ–‡ï¼š{context}
        å¯ç”¨å·¥å…·ï¼š{list(self.tools.keys())}
        
        è«‹æ±ºå®šä¸‹ä¸€æ­¥è¡Œå‹•ï¼š
        æ€è€ƒï¼š
        è¡Œå‹•ï¼š
        å·¥å…·ï¼š
        åƒæ•¸ï¼š
        """
        
        response = self.llm.generate(prompt)
        return self.parse_action(response)
    
    def execute_tool(self, tool_name, params):
        """åŸ·è¡Œå·¥å…·"""
        if tool_name in self.tools:
            return self.tools[tool_name](params)
        else:
            return "å·¥å…·ä¸å­˜åœ¨"
    
    def run(self, task):
        """åŸ·è¡Œä»»å‹™"""
        context = ""
        
        for i in range(self.max_iterations):
            # æ€è€ƒ
            action = self.think(task, context)
            
            # åŸ·è¡Œ
            if action["type"] == "tool":
                result = self.execute_tool(
                    action["tool"],
                    action["params"]
                )
                context += f"\nå·¥å…·çµæœï¼š{result}"
                
            elif action["type"] == "answer":
                return action["content"]
            
            # è¨˜æ†¶
            self.memory.append({
                "iteration": i,
                "action": action,
                "context": context
            })
        
        return "é”åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•¸"
```

---

# Agentå·¥å…·æ•´åˆ

## å¸¸ç”¨å·¥å…·é¡å‹
- ğŸ” **æœç´¢å·¥å…·ï¼š** Google Search APIã€Wikipedia
- ğŸ“Š **æ•¸æ“šåˆ†æï¼š** PythonåŸ·è¡Œå™¨ã€SQLæŸ¥è©¢
- ğŸ“ **æ–‡ä»¶è™•ç†ï¼š** PDFè®€å–ã€Excelæ“ä½œ
- ğŸŒ **ç¶²è·¯è«‹æ±‚ï¼š** APIèª¿ç”¨ã€ç¶²é çˆ¬å–

## å·¥å…·æ•´åˆç¯„ä¾‹

```python
import requests
import pandas as pd
from datetime import datetime

class AgentTools:
    @staticmethod
    def web_search(query):
        """ç¶²è·¯æœç´¢å·¥å…·"""
        # å¯¦ä½œæœç´¢APIèª¿ç”¨
        api_key = "your_api_key"
        url = f"https://api.search.com/search?q={query}"
        response = requests.get(url, headers={"API-Key": api_key})
        return response.json()
    
    @staticmethod
    def calculate(expression):
        """æ•¸å­¸è¨ˆç®—å·¥å…·"""
        try:
            result = eval(expression)
            return f"è¨ˆç®—çµæœï¼š{result}"
        except Exception as e:
            return f"è¨ˆç®—éŒ¯èª¤ï¼š{str(e)}"
    
    @staticmethod
    def read_csv(file_path):
        """CSVè®€å–å·¥å…·"""
        try:
            df = pd.read_csv(file_path)
            return df.describe().to_string()
        except Exception as e:
            return f"è®€å–éŒ¯èª¤ï¼š{str(e)}"
    
    @staticmethod
    def get_current_time():
        """ç²å–ç•¶å‰æ™‚é–“"""
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# è¨»å†Šå·¥å…·
tools = {
    "search": AgentTools.web_search,
    "calculate": AgentTools.calculate,
    "read_csv": AgentTools.read_csv,
    "get_time": AgentTools.get_current_time
}

# å‰µå»ºAgent
agent = SimpleLLMAgent(llm_model, tools)
result = agent.run("å¹«æˆ‘åˆ†æsales.csvçš„æ•¸æ“šè¶¨å‹¢")
```

---

# ç³»çµ±éƒ¨ç½²èˆ‡å„ªåŒ–

## éƒ¨ç½²æ¶æ§‹é¸æ“‡

| éƒ¨ç½²æ–¹å¼ | å„ªé» | ç¼ºé» | é©ç”¨å ´æ™¯ |
|---------|------|------|----------|
| **æœ¬åœ°éƒ¨ç½²** | è³‡æ–™å®‰å…¨ã€ç„¡å»¶é² | ç¡¬é«”è¦æ±‚é«˜ | ä¼æ¥­å…§éƒ¨æ‡‰ç”¨ |
| **é›²ç«¯API** | ç¶­è­·ç°¡å–®ã€æ“´å±•æ€§å¥½ | æˆæœ¬è¼ƒé«˜ã€è³‡æ–™éš±ç§ | å…¬é–‹æœå‹™ |
| **é‚Šç·£éƒ¨ç½²** | ä½å»¶é²ã€é›¢ç·šå¯ç”¨ | æ¨¡å‹å¤§å°å—é™ | ç§»å‹•æ‡‰ç”¨ |

## FastAPIéƒ¨ç½²ç¯„ä¾‹

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

app = FastAPI()

class ChatRequest(BaseModel):
    message: str
    context: list = []

class ChatResponse(BaseModel):
    response: str
    sources: list = []

# åˆå§‹åŒ–RAGç³»çµ±
rag_system = RAGSystem(llm_model, embedding_model)
rag_system.index_documents(documents)

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        # RAGæŸ¥è©¢
        result = rag_system.query(request.message)
        
        return ChatResponse(
            response=result["answer"],
            sources=result["sources"]
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

# æ€§èƒ½å„ªåŒ–ç­–ç•¥

## æ¨¡å‹å„ªåŒ–æŠ€è¡“

### 1. é‡åŒ– (Quantization)

```python
from transformers import BitsAndBytesConfig
import torch

# 4-bité‡åŒ–é…ç½®
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    device_map="auto"
)
```

### 2. æ‰¹æ¬¡è™•ç†

```python
def batch_inference(texts, batch_size=8):
    """æ‰¹æ¬¡æ¨ç†å„ªåŒ–"""
    results = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        inputs = tokenizer(
            batch,
            return_tensors="pt",
            padding=True,
            truncation=True
        )
        
        with torch.no_grad():
            outputs = model.generate(**inputs)
        
        batch_results = tokenizer.batch_decode(
            outputs,
            skip_special_tokens=True
        )
        results.extend(batch_results)
    
    return results
```

### 3. å¿«å–ç­–ç•¥

**å¯¦ä½œå»ºè­°ï¼š**
- ä½¿ç”¨Rediså¿«å–å¸¸è¦‹æŸ¥è©¢
- å¯¦ä½œå‘é‡è³‡æ–™åº«(Pinecone, Weaviate)
- é è¨ˆç®—ä¸¦å„²å­˜embeddings

---

# å¯¦æˆ°ç·´ç¿’å°ˆæ¡ˆ

## å°ˆæ¡ˆï¼šå»ºç«‹ä¼æ¥­çŸ¥è­˜å•ç­”ç³»çµ±

### ğŸ“‹ éœ€æ±‚èªªæ˜
å»ºç«‹ä¸€å€‹èƒ½å¤ å›ç­”å…¬å¸å…§éƒ¨æ–‡æª”å•é¡Œçš„æ™ºèƒ½åŠ©æ‰‹

### ğŸ¯ åŠŸèƒ½è¦æ±‚
- æ”¯æ´PDFã€Wordã€TXTæ–‡æª”ä¸Šå‚³
- å¯¦ä½œRAGæª¢ç´¢å¢å¼·ç”Ÿæˆ
- æä¾›ä¾†æºè¿½æº¯åŠŸèƒ½
- æ”¯æ´ä¸­è‹±æ–‡æ··åˆæŸ¥è©¢

### âš™ï¸ æŠ€è¡“æ¶æ§‹
- å¾Œç«¯ï¼šFastAPI + Llama 3
- å‘é‡è³‡æ–™åº«ï¼šFAISS/ChromaDB
- å‰ç«¯ï¼šStreamlit/Gradio

## å¯¦ä½œæ­¥é©Ÿ
1. **æ–‡æª”è™•ç†ï¼š** å¯¦ä½œå¤šæ ¼å¼æ–‡æª”è§£æå™¨
2. **å‘é‡åŒ–ï¼š** ä½¿ç”¨sentence-transformersç”Ÿæˆembeddings
3. **ç´¢å¼•å»ºç«‹ï¼š** å»ºç«‹å‘é‡è³‡æ–™åº«ç´¢å¼•
4. **æŸ¥è©¢è™•ç†ï¼š** å¯¦ä½œèªæ„æœç´¢èˆ‡é—œéµå­—æœç´¢
5. **ç­”æ¡ˆç”Ÿæˆï¼š** æ•´åˆLlamaæ¨¡å‹ç”Ÿæˆå›ç­”
6. **ä»‹é¢é–‹ç™¼ï¼š** å»ºç«‹ä½¿ç”¨è€…å‹å–„çš„Webä»‹é¢

### è©•åˆ†æ¨™æº–
- åŠŸèƒ½å®Œæ•´æ€§ (30%)
- å›ç­”æº–ç¢ºåº¦ (30%)
- ç³»çµ±æ•ˆèƒ½ (20%)
- ç¨‹å¼ç¢¼å“è³ª (20%)

---

# å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

## å•é¡Œ1ï¼šè¨˜æ†¶é«”ä¸è¶³ (OOM)

**è§£æ±ºæ–¹æ¡ˆï¼š**
- ä½¿ç”¨é‡åŒ–æŠ€è¡“ (4-bit/8-bit)
- æ¸›å°‘batch size
- ä½¿ç”¨gradient checkpointing
- è€ƒæ…®ä½¿ç”¨æ›´å°çš„æ¨¡å‹

## å•é¡Œ2ï¼šæ¨ç†é€Ÿåº¦æ…¢

**è§£æ±ºæ–¹æ¡ˆï¼š**
- å¯¦ä½œæ‰¹æ¬¡è™•ç†
- ä½¿ç”¨Flash Attention
- éƒ¨ç½²æ¨¡å‹åˆ°GPU
- ä½¿ç”¨ONNXå„ªåŒ–

## å•é¡Œ3ï¼šç”Ÿæˆå“è³ªä¸ä½³

**è§£æ±ºæ–¹æ¡ˆï¼š**
- å„ªåŒ–Promptè¨­è¨ˆ
- èª¿æ•´temperatureå’Œtop_påƒæ•¸
- å¢åŠ ç›¸é—œä¸Šä¸‹æ–‡
- è€ƒæ…®å¾®èª¿æ¨¡å‹

## å•é¡Œ4ï¼šä¸­æ–‡è™•ç†æ•ˆæœå·®

**è§£æ±ºæ–¹æ¡ˆï¼š**
- é¸æ“‡æ”¯æ´ä¸­æ–‡çš„æ¨¡å‹ (å¦‚ChatGLM)
- ä½¿ç”¨ä¸­æ–‡ç‰¹å®šçš„tokenizer
- æº–å‚™é«˜å“è³ªä¸­æ–‡è¨“ç·´è³‡æ–™
- èª¿æ•´åˆ†è©ç­–ç•¥

---

# LLMæŠ€è¡“æœªä¾†ç™¼å±•è¶¨å‹¢

## ğŸš€ æŠ€è¡“è¶¨å‹¢
- **å¤šæ¨¡æ…‹èåˆï¼š** æ–‡æœ¬ã€åœ–åƒã€éŸ³è¨Šçµ±ä¸€è™•ç†
- **é•·æ–‡æœ¬è™•ç†ï¼š** æ”¯æ´ç™¾è¬tokenç´šåˆ¥è¼¸å…¥
- **å¯¦æ™‚å­¸ç¿’ï¼š** æ¨¡å‹èƒ½å¤ å³æ™‚æ›´æ–°çŸ¥è­˜
- **è‡ªä¸»Agentï¼š** æ›´æ™ºèƒ½çš„ä»»å‹™è¦åŠƒèˆ‡åŸ·è¡Œ

## ğŸ“ˆ æ‡‰ç”¨æ–¹å‘
- **å‚ç›´é ˜åŸŸæ·±åŒ–ï¼š** é†«ç™‚ã€æ³•å¾‹ã€é‡‘èå°ˆæ¥­æ¨¡å‹
- **å€‹äººåŒ–AIåŠ©ç†ï¼š** å®Œå…¨å®¢è£½åŒ–çš„æ™ºèƒ½åŠ©æ‰‹
- **ç¨‹å¼ç¢¼ç”Ÿæˆï¼š** å¾éœ€æ±‚ç›´æ¥ç”Ÿæˆå®Œæ•´æ‡‰ç”¨
- **ç§‘å­¸ç ”ç©¶ï¼š** åŠ é€Ÿç§‘å­¸ç™¼ç¾èˆ‡å‰µæ–°

## âš¡ æ•ˆèƒ½å„ªåŒ–
- **æ¨¡å‹å£“ç¸®ï¼š** æ›´å°ä½†æ›´å¼·å¤§çš„æ¨¡å‹
- **ç¡¬é«”åŠ é€Ÿï¼š** å°ˆç”¨AIæ™¶ç‰‡å„ªåŒ–
- **åˆ†æ•£å¼æ¨ç†ï¼š** å¤šè¨­å‚™å”åŒè¨ˆç®—
- **é‚Šç·£éƒ¨ç½²ï¼š** æ‰‹æ©Ÿç«¯é‹è¡Œå¤§æ¨¡å‹

### å­¸ç¿’å»ºè­°
æŒçºŒé—œæ³¨æœ€æ–°è«–æ–‡ã€åƒèˆ‡é–‹æºå°ˆæ¡ˆã€å¯¦è¸çœŸå¯¦å ´æ™¯æ‡‰ç”¨

---

# èª²ç¨‹ç¸½çµèˆ‡å±•æœ›

## âœ… æˆ‘å€‘å­¸åˆ°äº†ä»€éº¼
- æ·±å…¥ç†è§£LLMçš„åŸç†èˆ‡æ¶æ§‹
- æŒæ¡ä¸ä¾è³´æ¡†æ¶çš„é–‹ç™¼æ–¹æ³•
- å¯¦ä½œå®Œæ•´çš„RAGç³»çµ±
- å­¸æœƒæ¨¡å‹å¾®èª¿èˆ‡å„ªåŒ–æŠ€è¡“
- å»ºæ§‹å¯éƒ¨ç½²çš„LLMæ‡‰ç”¨

## ğŸ¯ æ ¸å¿ƒèƒ½åŠ›

### æŠ€è¡“å±¤é¢
- ç¨ç«‹é–‹ç™¼LLMæ‡‰ç”¨çš„èƒ½åŠ›
- è§£æ±ºå¯¦éš›å•é¡Œçš„å·¥ç¨‹èƒ½åŠ›
- ç³»çµ±å„ªåŒ–èˆ‡èª¿è©¦èƒ½åŠ›

### æ€ç¶­å±¤é¢
- ç†è§£AIç³»çµ±çš„è¨­è¨ˆæ€ç¶­
- è©•ä¼°æŠ€è¡“é¸å‹çš„æ±ºç­–èƒ½åŠ›
- æŒçºŒå­¸ç¿’çš„æ–¹æ³•è«–

## ğŸ“š å¾ŒçºŒå­¸ç¿’è³‡æº
- **è«–æ–‡é–±è®€ï¼š** ArXivã€Papers with Code
- **é–‹æºå°ˆæ¡ˆï¼š** Hugging Faceã€GitHub
- **ç¤¾ç¾¤äº¤æµï¼š** Reddit r/MachineLearningã€Discord
- **å¯¦æˆ°å¹³å°ï¼š** Kaggleã€Google Colab

---

# ğŸŒŸ çµèª

> LLMæŠ€è¡“æ­£åœ¨æ”¹è®Šä¸–ç•Œï¼Œè€Œä½ å·²ç¶“æŒæ¡äº†é–‹å•Ÿé€™æ‰‡å¤§é–€çš„é‘°åŒ™ã€‚
> 
> æŒçºŒæ¢ç´¢ã€å‹‡æ–¼å‰µæ–°ï¼Œæˆç‚ºAIæ™‚ä»£çš„å»ºè¨­è€…ï¼

---

*Generated with Claude Opus 4.1*