# Day 2 ä¸Šåˆï¼šå®Œæ•´RAGç³»çµ±
# 10:30-11:30 å¯¦ä½œå®Œæ•´çš„RAGç³»çµ±

import os
import json
from typing import List, Dict, Tuple
from dotenv import load_dotenv
from openai import OpenAI
import numpy as np

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# === Step 1: åˆå§‹åŒ– ===
print("=== å®Œæ•´RAGç³»çµ±å¯¦ä½œ ===")
print()

client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY")
)

# === Step 2: æ–‡ä»¶è™•ç†é¡ ===
class DocumentProcessor:
    """æ–‡ä»¶è™•ç†å™¨"""
    
    @staticmethod
    def split_text(text: str, chunk_size: int = 200) -> List[str]:
        """
        å°‡é•·æ–‡æœ¬åˆ‡åˆ†æˆå°å¡Š
        """
        words = text.split()
        chunks = []
        current_chunk = []
        current_size = 0
        
        for word in words:
            current_chunk.append(word)
            current_size += len(word) + 1
            
            if current_size >= chunk_size:
                chunks.append(" ".join(current_chunk))
                current_chunk = []
                current_size = 0
        
        if current_chunk:
            chunks.append(" ".join(current_chunk))
        
        return chunks

# === Step 3: å‘é‡è³‡æ–™åº« ===
class VectorDatabase:
    """ç°¡å–®çš„å‘é‡è³‡æ–™åº«"""
    
    def __init__(self):
        self.documents = []
        self.embeddings = []
        self.metadata = []
    
    def add_document(self, text: str, embedding: List[float], metadata: Dict = None):
        """æ·»åŠ æ–‡ä»¶åˆ°è³‡æ–™åº«"""
        self.documents.append(text)
        self.embeddings.append(embedding)
        self.metadata.append(metadata or {})
    
    def search(self, query_embedding: List[float], top_k: int = 3) -> List[Tuple[str, float, Dict]]:
        """å‘é‡æœå°‹"""
        similarities = []
        
        for i, doc_embedding in enumerate(self.embeddings):
            sim = self._cosine_similarity(query_embedding, doc_embedding)
            similarities.append((
                self.documents[i],
                sim,
                self.metadata[i]
            ))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
    
    def save(self, filename: str):
        """å„²å­˜è³‡æ–™åº«"""
        data = {
            "documents": self.documents,
            "embeddings": self.embeddings,
            "metadata": self.metadata
        }
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"è³‡æ–™åº«å·²å„²å­˜åˆ° {filename}")
    
    def load(self, filename: str):
        """è¼‰å…¥è³‡æ–™åº«"""
        with open(filename, 'r', encoding='utf-8') as f:
            data = json.load(f)
        self.documents = data["documents"]
        self.embeddings = data["embeddings"]
        self.metadata = data["metadata"]
        print(f"å¾ {filename} è¼‰å…¥äº† {len(self.documents)} å€‹æ–‡ä»¶")

# === Step 4: RAGç³»çµ±ä¸»é¡ ===
class RAGSystem:
    """å®Œæ•´çš„RAGç³»çµ±"""
    
    def __init__(self, model="gpt-3.5-turbo"):
        self.vector_db = VectorDatabase()
        self.model = model
        self.doc_processor = DocumentProcessor()
    
    def add_knowledge(self, text: str, source: str = "unknown"):
        """æ·»åŠ çŸ¥è­˜åˆ°ç³»çµ±"""
        print(f"æ·»åŠ çŸ¥è­˜ä¾†æºï¼š{source}")
        
        # åˆ‡åˆ†æ–‡æœ¬
        chunks = self.doc_processor.split_text(text)
        print(f"  åˆ‡åˆ†æˆ {len(chunks)} å€‹ç‰‡æ®µ")
        
        # ç‚ºæ¯å€‹ç‰‡æ®µç”Ÿæˆå‘é‡ä¸¦å„²å­˜
        for i, chunk in enumerate(chunks):
            try:
                # ç”Ÿæˆå‘é‡
                response = client.embeddings.create(
                    input=chunk,
                    model="text-embedding-ada-002"
                )
                embedding = response.data[0].embedding
                
                # æ·»åŠ åˆ°è³‡æ–™åº«
                self.vector_db.add_document(
                    text=chunk,
                    embedding=embedding,
                    metadata={"source": source, "chunk_id": i}
                )
            except Exception as e:
                print(f"  éŒ¯èª¤è™•ç†ç‰‡æ®µ {i}: {e}")
                # ä½¿ç”¨éš¨æ©Ÿå‘é‡ä½œç‚ºå‚™æ¡ˆ
                embedding = np.random.random(1536).tolist()
                self.vector_db.add_document(
                    text=chunk,
                    embedding=embedding,
                    metadata={"source": source, "chunk_id": i}
                )
    
    def answer_question(self, question: str) -> Dict:
        """å›ç­”å•é¡Œ"""
        print(f"\nå•é¡Œï¼š{question}")
        
        # Step 1: ç”Ÿæˆå•é¡Œå‘é‡
        try:
            response = client.embeddings.create(
                input=question,
                model="text-embedding-ada-002"
            )
            query_embedding = response.data[0].embedding
        except Exception as e:
            print(f"ç”Ÿæˆå•é¡Œå‘é‡å¤±æ•—ï¼š{e}")
            query_embedding = np.random.random(1536).tolist()
        
        # Step 2: æª¢ç´¢ç›¸é—œæ–‡ä»¶
        results = self.vector_db.search(query_embedding, top_k=3)
        
        if not results:
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘æ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ä¾†å›ç­”ä½ çš„å•é¡Œã€‚",
                "sources": [],
                "confidence": 0.0
            }
        
        # Step 3: æº–å‚™ä¸Šä¸‹æ–‡
        context_parts = []
        sources = []
        total_score = 0
        
        for doc, score, meta in results:
            context_parts.append(f"è³‡æ–™{len(context_parts)+1}: {doc}")
            sources.append(meta.get("source", "unknown"))
            total_score += score
        
        context = "\n\n".join(context_parts)
        avg_score = total_score / len(results)
        
        # Step 4: ç”Ÿæˆç­”æ¡ˆ
        prompt = f"""åŸºæ–¼ä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œï¼š

{context}

å•é¡Œï¼š{question}

è«‹æ ¹æ“šæä¾›çš„è³‡æ–™æº–ç¢ºå›ç­”ã€‚å¦‚æœè³‡æ–™ä¸è¶³ï¼Œè«‹èªªæ˜ã€‚"""
        
        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹æº–ç¢ºçš„å•ç­”åŠ©æ‰‹ï¼Œåªæ ¹æ“šæä¾›çš„è³‡æ–™å›ç­”å•é¡Œã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=200
            )
            answer = response.choices[0].message.content
        except Exception as e:
            answer = f"ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"
        
        return {
            "answer": answer,
            "sources": list(set(sources)),
            "confidence": avg_score,
            "retrieved_docs": len(results)
        }

# === Step 5: å»ºç«‹çŸ¥è­˜åº« ===
print("=== å»ºç«‹RAGç³»çµ± ===")
print()

rag = RAGSystem()

# æ·»åŠ çŸ¥è­˜
knowledge_base = [
    {
        "text": """Pythonæ˜¯ä¸€ç¨®é«˜éšç¨‹å¼èªè¨€ï¼Œç”±Guido van Rossumåœ¨1991å¹´å‰µé€ ã€‚
        Pythonå¼·èª¿ç¨‹å¼ç¢¼çš„å¯è®€æ€§ï¼Œä½¿ç”¨ç¸®æ’ä¾†å®šç¾©ç¨‹å¼ç¢¼å¡Šã€‚
        Pythonæ”¯æ´å¤šç¨®ç¨‹å¼è¨­è¨ˆç¯„å¼ï¼ŒåŒ…æ‹¬ç‰©ä»¶å°å‘ã€ç¨‹åºå¼å’Œå‡½æ•¸å¼ç¨‹å¼è¨­è¨ˆã€‚
        Pythonæ“æœ‰è±å¯Œçš„æ¨™æº–åº«å’Œç¬¬ä¸‰æ–¹å¥—ä»¶ç”Ÿæ…‹ç³»çµ±ã€‚""",
        "source": "PythonåŸºç¤çŸ¥è­˜"
    },
    {
        "text": """æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹åˆ†æ”¯ï¼Œè®“é›»è…¦èƒ½å¤ å¾è³‡æ–™ä¸­å­¸ç¿’ã€‚
        ç›£ç£å¼å­¸ç¿’éœ€è¦æ¨™è¨˜çš„è¨“ç·´è³‡æ–™ï¼ŒåŒ…æ‹¬åˆ†é¡å’Œå›æ­¸ä»»å‹™ã€‚
        éç›£ç£å¼å­¸ç¿’ä¸éœ€è¦æ¨™è¨˜è³‡æ–™ï¼Œå¸¸ç”¨æ–¼èšé¡å’Œé™ç¶­ã€‚
        å¼·åŒ–å­¸ç¿’é€éèˆ‡ç’°å¢ƒäº’å‹•ä¾†å­¸ç¿’æœ€ä½³ç­–ç•¥ã€‚""",
        "source": "æ©Ÿå™¨å­¸ç¿’æ¦‚è«–"
    },
    {
        "text": """æ·±åº¦å­¸ç¿’ä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†å­¸ç¿’è³‡æ–™çš„è¤‡é›œè¡¨ç¤ºã€‚
        å·ç©ç¥ç¶“ç¶²è·¯(CNN)æ“…é•·è™•ç†åœ–åƒè³‡æ–™ã€‚
        å¾ªç’°ç¥ç¶“ç¶²è·¯(RNN)é©åˆè™•ç†åºåˆ—è³‡æ–™å¦‚æ–‡å­—å’Œæ™‚é–“åºåˆ—ã€‚
        Transformeræ¶æ§‹é©æ–°äº†è‡ªç„¶èªè¨€è™•ç†é ˜åŸŸã€‚""",
        "source": "æ·±åº¦å­¸ç¿’æŠ€è¡“"
    }
]

for kb in knowledge_base:
    rag.add_knowledge(kb["text"], kb["source"])

print()

# === Step 6: æ¸¬è©¦å•ç­” ===
print("=== æ¸¬è©¦RAGå•ç­”ç³»çµ± ===")

questions = [
    "Pythonæ˜¯èª°å‰µé€ çš„ï¼Ÿ",
    "ä»€éº¼æ˜¯ç›£ç£å¼å­¸ç¿’ï¼Ÿ",
    "CNNé©åˆè™•ç†ä»€éº¼é¡å‹çš„è³‡æ–™ï¼Ÿ",
    "Transformerç”¨æ–¼ä»€éº¼é ˜åŸŸï¼Ÿ"
]

for q in questions:
    result = rag.answer_question(q)
    print()
    print("="*50)
    print(f"å•é¡Œï¼š{q}")
    print(f"ç­”æ¡ˆï¼š{result['answer']}")
    print(f"ä¾†æºï¼š{', '.join(result['sources'])}")
    print(f"ä¿¡å¿ƒåº¦ï¼š{result['confidence']:.2%}")
    print(f"æª¢ç´¢æ–‡ä»¶æ•¸ï¼š{result['retrieved_docs']}")

# === Step 7: å„²å­˜å’Œè¼‰å…¥ ===
print("\n" + "="*50)
print("=== å„²å­˜RAGç³»çµ± ===")

# å„²å­˜å‘é‡è³‡æ–™åº«
rag.vector_db.save("rag_database.json")

# === Step 8: äº’å‹•æ¨¡å¼ ===
print("\n" + "="*50)
print("=== äº’å‹•å•ç­”æ¨¡å¼ ===")
print("è¼¸å…¥å•é¡Œä¾†æ¸¬è©¦RAGç³»çµ±ï¼ˆè¼¸å…¥'quit'çµæŸï¼‰")
print()

while True:
    user_question = input("ğŸ‘¤ ä½ çš„å•é¡Œï¼š")
    
    if user_question.lower() == 'quit':
        print("ğŸ‘‹ å†è¦‹ï¼")
        break
    
    result = rag.answer_question(user_question)
    print(f"ğŸ¤– ç­”æ¡ˆï¼š{result['answer']}")
    print(f"ğŸ“š è³‡æ–™ä¾†æºï¼š{', '.join(result['sources'])}")
    print(f"ğŸ“Š ä¿¡å¿ƒåº¦ï¼š{result['confidence']:.2%}")
    print()