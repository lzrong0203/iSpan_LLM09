#!/usr/bin/env python3
"""
æ··åˆæª¢ç´¢ RAG ç³»çµ±ï¼šçµåˆ FAISS å‘é‡æœå°‹èˆ‡ BM25 é—œéµå­—æœå°‹
Hybrid RAG System: Combining FAISS vector search with BM25 keyword search
"""

import json
import os
import pickle
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from collections import Counter
import math

import numpy as np
import faiss
from dotenv import load_dotenv
from openai import OpenAI
from rank_bm25 import BM25Okapi
import jieba  # ä¸­æ–‡åˆ†è©

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# === è³‡æ–™çµæ§‹å®šç¾© ===
@dataclass
class Document:
    """æ–‡æª”è³‡æ–™çµæ§‹"""
    id: str
    content: str
    embedding: Optional[np.ndarray] = None
    metadata: Dict = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class TextProcessor:
    """æ–‡æœ¬è™•ç†å™¨ï¼šæ”¯æ´ä¸­è‹±æ–‡"""

    def __init__(self, language: str = "mixed"):
        """
        åˆå§‹åŒ–æ–‡æœ¬è™•ç†å™¨

        Args:
            language: "chinese", "english", or "mixed"
        """
        self.language = language

    def tokenize(self, text: str) -> List[str]:
        """
        æ–‡æœ¬åˆ†è©

        Args:
            text: è¼¸å…¥æ–‡æœ¬

        Returns:
            åˆ†è©å¾Œçš„åˆ—è¡¨
        """
        if self.language == "chinese" or self.language == "mixed":
            # ä½¿ç”¨ jieba é€²è¡Œä¸­æ–‡åˆ†è©
            tokens = list(jieba.cut(text))
            # éæ¿¾åœç”¨è©å’Œç©ºç™½
            tokens = [t.strip() for t in tokens if t.strip() and len(t.strip()) > 1]
        else:
            # è‹±æ–‡åˆ†è©
            tokens = text.lower().split()
            # ç°¡å–®çš„åœç”¨è©éæ¿¾
            stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'}
            tokens = [t for t in tokens if t not in stop_words]

        return tokens

    def split_text(self, text: str, chunk_size: int = 200, overlap: int = 50) -> List[str]:
        """
        å°‡é•·æ–‡æœ¬åˆ‡åˆ†æˆé‡ç–Šçš„å°å¡Š

        Args:
            text: è¼¸å…¥æ–‡æœ¬
            chunk_size: æ¯å¡Šçš„å­—ç¬¦æ•¸
            overlap: é‡ç–Šå­—ç¬¦æ•¸

        Returns:
            æ–‡æœ¬å¡Šåˆ—è¡¨
        """
        chunks = []
        start = 0
        text_length = len(text)

        while start < text_length:
            end = min(start + chunk_size, text_length)
            chunk = text[start:end]

            # å˜—è©¦åœ¨å¥è™Ÿè™•æ–·é–‹
            if end < text_length:
                last_period = chunk.rfind('ã€‚')
                if last_period == -1:
                    last_period = chunk.rfind('.')
                if last_period != -1 and last_period > chunk_size * 0.5:
                    chunk = chunk[:last_period + 1]
                    end = start + last_period + 1

            chunks.append(chunk.strip())
            start = end - overlap if end < text_length else text_length

        return chunks


class BM25Retriever:
    """BM25 é—œéµå­—æª¢ç´¢å™¨"""

    def __init__(self, k1: float = 1.5, b: float = 0.75):
        """
        åˆå§‹åŒ– BM25 æª¢ç´¢å™¨

        Args:
            k1: BM25 åƒæ•¸ï¼Œæ§åˆ¶è©é »é£½å’Œåº¦ (é€šå¸¸ 1.2-2.0)
            b: BM25 åƒæ•¸ï¼Œæ§åˆ¶æ–‡æª”é•·åº¦æ­¸ä¸€åŒ– (é€šå¸¸ 0.75)
        """
        self.k1 = k1
        self.b = b
        self.corpus = []
        self.tokenized_corpus = []
        self.bm25 = None
        self.text_processor = TextProcessor(language="mixed")

    def add_documents(self, documents: List[str]):
        """
        æ·»åŠ æ–‡æª”åˆ° BM25 ç´¢å¼•

        Args:
            documents: æ–‡æª”åˆ—è¡¨
        """
        self.corpus = documents
        self.tokenized_corpus = [self.text_processor.tokenize(doc) for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus, k1=self.k1, b=self.b)

    def search(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:
        """
        BM25 æœå°‹

        Args:
            query: æŸ¥è©¢æ–‡æœ¬
            top_k: è¿”å›å‰ k å€‹çµæœ

        Returns:
            [(æ–‡æª”ç´¢å¼•, BM25åˆ†æ•¸)] åˆ—è¡¨
        """
        if self.bm25 is None:
            return []

        tokenized_query = self.text_processor.tokenize(query)
        scores = self.bm25.get_scores(tokenized_query)

        # ç²å–å‰ k å€‹æœ€é«˜åˆ†çš„æ–‡æª”
        top_indices = np.argsort(scores)[::-1][:top_k]
        results = [(int(idx), float(scores[idx])) for idx in top_indices if scores[idx] > 0]

        return results

    def save(self, filepath: str):
        """å„²å­˜ BM25 ç´¢å¼•"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'corpus': self.corpus,
                'tokenized_corpus': self.tokenized_corpus,
                'k1': self.k1,
                'b': self.b
            }, f)

    def load(self, filepath: str):
        """è¼‰å…¥ BM25 ç´¢å¼•"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        self.corpus = data['corpus']
        self.tokenized_corpus = data['tokenized_corpus']
        self.k1 = data['k1']
        self.b = data['b']
        self.bm25 = BM25Okapi(self.tokenized_corpus, k1=self.k1, b=self.b)


class FAISSRetriever:
    """FAISS å‘é‡æª¢ç´¢å™¨"""

    def __init__(self, dimension: int = 1536, index_type: str = "flat"):
        """
        åˆå§‹åŒ– FAISS æª¢ç´¢å™¨

        Args:
            dimension: å‘é‡ç¶­åº¦
            index_type: ç´¢å¼•é¡å‹ ("flat", "ivf", "hnsw")
        """
        self.dimension = dimension
        self.index_type = index_type
        self.index = self._create_index()
        self.documents = []

    def _create_index(self) -> faiss.Index:
        """
        å»ºç«‹ FAISS ç´¢å¼•

        Returns:
            FAISS ç´¢å¼•å°è±¡
        """
        if self.index_type == "flat":
            # ç²¾ç¢ºæœå°‹ï¼Œé©åˆå°è¦æ¨¡è³‡æ–™
            index = faiss.IndexFlatIP(self.dimension)  # å…§ç©ï¼ˆç”¨æ–¼æ­£è¦åŒ–å‘é‡ï¼‰
        elif self.index_type == "ivf":
            # IVF ç´¢å¼•ï¼Œé©åˆä¸­ç­‰è¦æ¨¡
            quantizer = faiss.IndexFlatIP(self.dimension)
            index = faiss.IndexIVFFlat(quantizer, self.dimension, 100, faiss.METRIC_INNER_PRODUCT)
        elif self.index_type == "hnsw":
            # HNSW ç´¢å¼•ï¼Œé©åˆå¤§è¦æ¨¡ä¸”éœ€è¦å¿«é€ŸæŸ¥è©¢
            index = faiss.IndexHNSWFlat(self.dimension, 32, faiss.METRIC_INNER_PRODUCT)
        else:
            raise ValueError(f"Unknown index type: {self.index_type}")

        return index

    def add_documents(self, embeddings: np.ndarray, documents: List[str]):
        """
        æ·»åŠ æ–‡æª”å‘é‡åˆ° FAISS ç´¢å¼•

        Args:
            embeddings: æ–‡æª”å‘é‡çŸ©é™£ (n_docs, dimension)
            documents: å°æ‡‰çš„æ–‡æª”åˆ—è¡¨
        """
        # æ­£è¦åŒ–å‘é‡ï¼ˆç”¨æ–¼é¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰
        faiss.normalize_L2(embeddings)

        # è¨“ç·´ç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.index_type == "ivf" and not self.index.is_trained:
            self.index.train(embeddings)

        # æ·»åŠ å‘é‡
        self.index.add(embeddings)
        self.documents.extend(documents)

    def search(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Tuple[int, float]]:
        """
        å‘é‡æœå°‹

        Args:
            query_embedding: æŸ¥è©¢å‘é‡
            top_k: è¿”å›å‰ k å€‹çµæœ

        Returns:
            [(æ–‡æª”ç´¢å¼•, ç›¸ä¼¼åº¦åˆ†æ•¸)] åˆ—è¡¨
        """
        # ç¢ºä¿æŸ¥è©¢å‘é‡æ˜¯ 2D çš„
        if query_embedding.ndim == 1:
            query_embedding = query_embedding.reshape(1, -1)

        # æ­£è¦åŒ–æŸ¥è©¢å‘é‡
        faiss.normalize_L2(query_embedding)

        # æœå°‹
        scores, indices = self.index.search(query_embedding, top_k)

        # è¿”å›çµæœ
        results = [(int(idx), float(score)) for idx, score in zip(indices[0], scores[0]) if idx != -1]
        return results

    def save(self, index_path: str, docs_path: str):
        """å„²å­˜ FAISS ç´¢å¼•å’Œæ–‡æª”"""
        faiss.write_index(self.index, index_path)
        with open(docs_path, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)

    def load(self, index_path: str, docs_path: str):
        """è¼‰å…¥ FAISS ç´¢å¼•å’Œæ–‡æª”"""
        self.index = faiss.read_index(index_path)
        with open(docs_path, 'r', encoding='utf-8') as f:
            self.documents = json.load(f)


class HybridRAGSystem:
    """
    æ··åˆæª¢ç´¢ RAG ç³»çµ±
    çµåˆ FAISS å‘é‡æª¢ç´¢å’Œ BM25 é—œéµå­—æª¢ç´¢
    """

    def __init__(self,
                 model: str = "gpt-3.5-turbo",
                 embedding_model: str = "text-embedding-ada-002",
                 vector_weight: float = 0.7,
                 keyword_weight: float = 0.3):
        """
        åˆå§‹åŒ–æ··åˆ RAG ç³»çµ±

        Args:
            model: OpenAI ç”Ÿæˆæ¨¡å‹
            embedding_model: OpenAI åµŒå…¥æ¨¡å‹
            vector_weight: å‘é‡æª¢ç´¢æ¬Šé‡
            keyword_weight: é—œéµå­—æª¢ç´¢æ¬Šé‡
        """
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = model
        self.embedding_model = embedding_model

        # æª¢ç´¢æ¬Šé‡
        self.vector_weight = vector_weight
        self.keyword_weight = keyword_weight

        # åˆå§‹åŒ–æª¢ç´¢å™¨
        self.faiss_retriever = FAISSRetriever(dimension=1536, index_type="flat")
        self.bm25_retriever = BM25Retriever()

        # æ–‡æœ¬è™•ç†å™¨
        self.text_processor = TextProcessor(language="mixed")

        # æ–‡æª”å­˜å„²
        self.documents = []
        self.doc_embeddings = []

    def add_knowledge(self, text: str, source: str = "unknown", chunk_size: int = 200):
        """
        æ·»åŠ çŸ¥è­˜åˆ°ç³»çµ±

        Args:
            text: æ–‡æœ¬å…§å®¹
            source: ä¾†æºæ¨™è­˜
            chunk_size: åˆ†å¡Šå¤§å°
        """
        print(f"æ·»åŠ çŸ¥è­˜ä¾†æºï¼š{source}")

        # æ–‡æœ¬åˆ†å¡Š
        chunks = self.text_processor.split_text(text, chunk_size=chunk_size, overlap=50)
        print(f"  åˆ‡åˆ†æˆ {len(chunks)} å€‹ç‰‡æ®µ")

        # ç”ŸæˆåµŒå…¥å‘é‡
        embeddings = []
        for i, chunk in enumerate(chunks):
            try:
                # èª¿ç”¨ OpenAI API ç”ŸæˆåµŒå…¥
                response = self.client.embeddings.create(
                    input=chunk,
                    model=self.embedding_model
                )
                embedding = np.array(response.data[0].embedding)
            except Exception as e:
                print(f"  ç”ŸæˆåµŒå…¥å¤±æ•— (ç‰‡æ®µ {i}): {e}")
                # ä½¿ç”¨éš¨æ©Ÿå‘é‡ä½œç‚ºå‚™æ¡ˆ
                embedding = np.random.randn(1536)

            embeddings.append(embedding)

            # å‰µå»ºæ–‡æª”å°è±¡
            doc = Document(
                id=f"{source}_{i}",
                content=chunk,
                embedding=embedding,
                metadata={"source": source, "chunk_id": i}
            )
            self.documents.append(doc)

        # æ›´æ–°æª¢ç´¢å™¨
        embeddings_array = np.array(embeddings).astype('float32')
        self.faiss_retriever.add_documents(embeddings_array, chunks)
        self.bm25_retriever.add_documents(chunks)

        print(f"  æˆåŠŸæ·»åŠ  {len(chunks)} å€‹æ–‡æª”ç‰‡æ®µ")

    def hybrid_search(self, query: str, top_k: int = 5) -> List[Tuple[str, float, Dict]]:
        """
        æ··åˆæª¢ç´¢ï¼šçµåˆå‘é‡å’Œé—œéµå­—æœå°‹

        Args:
            query: æŸ¥è©¢æ–‡æœ¬
            top_k: è¿”å›å‰ k å€‹çµæœ

        Returns:
            [(æ–‡æª”å…§å®¹, ç¶œåˆåˆ†æ•¸, å…ƒæ•¸æ“š)] åˆ—è¡¨
        """
        # 1. å‘é‡æª¢ç´¢
        try:
            response = self.client.embeddings.create(
                input=query,
                model=self.embedding_model
            )
            query_embedding = np.array(response.data[0].embedding).astype('float32')
        except Exception as e:
            print(f"ç”ŸæˆæŸ¥è©¢åµŒå…¥å¤±æ•—: {e}")
            query_embedding = np.random.randn(1536).astype('float32')

        vector_results = self.faiss_retriever.search(query_embedding, top_k=top_k*2)

        # 2. BM25 é—œéµå­—æª¢ç´¢
        keyword_results = self.bm25_retriever.search(query, top_k=top_k*2)

        # 3. åˆ†æ•¸èåˆï¼ˆReciprocal Rank Fusionï¼‰
        doc_scores = {}

        # è™•ç†å‘é‡æª¢ç´¢çµæœ
        for rank, (idx, score) in enumerate(vector_results):
            if idx < len(self.documents):
                doc_id = f"doc_{idx}"
                # RRF åˆ†æ•¸ï¼š1 / (k + rank)ï¼Œk=60 æ˜¯å¸¸ç”¨å€¼
                rrf_score = 1 / (60 + rank + 1)
                doc_scores[doc_id] = doc_scores.get(doc_id, 0) + self.vector_weight * rrf_score

        # è™•ç† BM25 æª¢ç´¢çµæœ
        for rank, (idx, score) in enumerate(keyword_results):
            if idx < len(self.documents):
                doc_id = f"doc_{idx}"
                rrf_score = 1 / (60 + rank + 1)
                doc_scores[doc_id] = doc_scores.get(doc_id, 0) + self.keyword_weight * rrf_score

        # 4. æ’åºä¸¦è¿”å›å‰ k å€‹çµæœ
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]

        results = []
        for doc_id, score in sorted_docs:
            idx = int(doc_id.split('_')[1])
            if idx < len(self.documents):
                doc = self.documents[idx]
                results.append((doc.content, score, doc.metadata))

        return results

    def rerank_with_cross_encoder(self, query: str, documents: List[str], top_k: int = 3) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨äº¤å‰ç·¨ç¢¼å™¨é‡æ–°æ’åºï¼ˆä½¿ç”¨ GPT é€²è¡Œç›¸é—œæ€§è©•åˆ†ï¼‰

        Args:
            query: æŸ¥è©¢æ–‡æœ¬
            documents: å€™é¸æ–‡æª”åˆ—è¡¨
            top_k: è¿”å›å‰ k å€‹çµæœ

        Returns:
            é‡æ–°æ’åºå¾Œçš„ [(æ–‡æª”, åˆ†æ•¸)] åˆ—è¡¨
        """
        reranked = []

        for doc in documents[:10]:  # é™åˆ¶é‡æ’åºçš„æ–‡æª”æ•¸é‡ä»¥æ§åˆ¶æˆæœ¬
            # ä½¿ç”¨ GPT è©•ä¼°ç›¸é—œæ€§
            prompt = f"""è«‹è©•ä¼°ä»¥ä¸‹æ–‡æª”èˆ‡æŸ¥è©¢çš„ç›¸é—œæ€§ï¼Œè¿”å› 0-10 çš„åˆ†æ•¸ã€‚
åªè¿”å›æ•¸å­—ï¼Œä¸è¦å…¶ä»–è§£é‡‹ã€‚

æŸ¥è©¢ï¼š{query}

æ–‡æª”ï¼š{doc}

ç›¸é—œæ€§åˆ†æ•¸ï¼ˆ0-10ï¼‰ï¼š"""

            try:
                response = self.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹æ–‡æª”ç›¸é—œæ€§è©•åˆ†å™¨ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0,
                    max_tokens=10
                )

                score_text = response.choices[0].message.content.strip()
                score = float(score_text) / 10.0  # æ­£è¦åŒ–åˆ° 0-1
            except:
                score = 0.5  # é è¨­åˆ†æ•¸

            reranked.append((doc, score))

        # æ’åºä¸¦è¿”å›å‰ k å€‹
        reranked.sort(key=lambda x: x[1], reverse=True)
        return reranked[:top_k]

    def answer_question(self,
                       question: str,
                       use_reranking: bool = False,
                       top_k: int = 5) -> Dict:
        """
        å›ç­”å•é¡Œ

        Args:
            question: ç”¨æˆ¶å•é¡Œ
            use_reranking: æ˜¯å¦ä½¿ç”¨é‡æ–°æ’åº
            top_k: æª¢ç´¢æ–‡æª”æ•¸é‡

        Returns:
            åŒ…å«ç­”æ¡ˆå’Œç›¸é—œä¿¡æ¯çš„å­—å…¸
        """
        print(f"\nå•é¡Œï¼š{question}")

        # 1. æ··åˆæª¢ç´¢
        search_results = self.hybrid_search(question, top_k=top_k)

        if not search_results:
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘æ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ä¾†å›ç­”ä½ çš„å•é¡Œã€‚",
                "sources": [],
                "confidence": 0.0,
                "retrieval_method": "none"
            }

        # 2. å¯é¸ï¼šé‡æ–°æ’åº
        if use_reranking:
            docs_to_rerank = [doc for doc, _, _ in search_results]
            reranked = self.rerank_with_cross_encoder(question, docs_to_rerank, top_k=3)

            # ä½¿ç”¨é‡æ–°æ’åºçš„çµæœ
            context_docs = [doc for doc, _ in reranked]
            sources = [search_results[i][2].get("source", "unknown") for i in range(len(reranked))]
            avg_score = np.mean([score for _, score in reranked])
        else:
            # ä½¿ç”¨åŸå§‹æª¢ç´¢çµæœ
            context_docs = [doc for doc, _, _ in search_results[:3]]
            sources = [meta.get("source", "unknown") for _, _, meta in search_results[:3]]
            avg_score = np.mean([score for _, score, _ in search_results[:3]])

        # 3. æ§‹å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([f"è³‡æ–™{i+1}: {doc}" for i, doc in enumerate(context_docs)])

        # 4. ç”Ÿæˆç­”æ¡ˆ
        prompt = f"""åŸºæ–¼ä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œã€‚è«‹ç¢ºä¿ç­”æ¡ˆæº–ç¢ºä¸”åŸºæ–¼æä¾›çš„è³‡æ–™ã€‚

ç›¸é—œè³‡æ–™ï¼š
{context}

å•é¡Œï¼š{question}

è«‹æä¾›æ¸…æ™°ã€æº–ç¢ºçš„ç­”æ¡ˆï¼š"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€å€‹æº–ç¢ºçš„å•ç­”åŠ©æ‰‹ï¼Œåªæ ¹æ“šæä¾›çš„è³‡æ–™å›ç­”å•é¡Œã€‚å¦‚æœè³‡æ–™ä¸è¶³ä»¥å›ç­”å•é¡Œï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚"
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=300
            )
            answer = response.choices[0].message.content
        except Exception as e:
            answer = f"ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

        return {
            "answer": answer,
            "sources": list(set(sources)),
            "confidence": float(avg_score),
            "retrieval_method": "hybrid_with_reranking" if use_reranking else "hybrid",
            "retrieved_docs": len(search_results)
        }

    def save_system(self, directory: str):
        """
        å„²å­˜æ•´å€‹ç³»çµ±

        Args:
            directory: å„²å­˜ç›®éŒ„
        """
        os.makedirs(directory, exist_ok=True)

        # å„²å­˜ FAISS ç´¢å¼•
        self.faiss_retriever.save(
            os.path.join(directory, "faiss.index"),
            os.path.join(directory, "faiss_docs.json")
        )

        # å„²å­˜ BM25 ç´¢å¼•
        self.bm25_retriever.save(os.path.join(directory, "bm25.pkl"))

        # å„²å­˜æ–‡æª”
        docs_data = []
        for doc in self.documents:
            docs_data.append({
                "id": doc.id,
                "content": doc.content,
                "metadata": doc.metadata
            })

        with open(os.path.join(directory, "documents.json"), 'w', encoding='utf-8') as f:
            json.dump(docs_data, f, ensure_ascii=False, indent=2)

        # å„²å­˜é…ç½®
        config = {
            "vector_weight": self.vector_weight,
            "keyword_weight": self.keyword_weight,
            "model": self.model,
            "embedding_model": self.embedding_model
        }

        with open(os.path.join(directory, "config.json"), 'w') as f:
            json.dump(config, f, indent=2)

        print(f"ç³»çµ±å·²å„²å­˜åˆ° {directory}")

    def load_system(self, directory: str):
        """
        è¼‰å…¥æ•´å€‹ç³»çµ±

        Args:
            directory: è¼‰å…¥ç›®éŒ„
        """
        # è¼‰å…¥é…ç½®
        with open(os.path.join(directory, "config.json"), 'r') as f:
            config = json.load(f)

        self.vector_weight = config["vector_weight"]
        self.keyword_weight = config["keyword_weight"]
        self.model = config["model"]
        self.embedding_model = config["embedding_model"]

        # è¼‰å…¥ FAISS ç´¢å¼•
        self.faiss_retriever.load(
            os.path.join(directory, "faiss.index"),
            os.path.join(directory, "faiss_docs.json")
        )

        # è¼‰å…¥ BM25 ç´¢å¼•
        self.bm25_retriever.load(os.path.join(directory, "bm25.pkl"))

        # è¼‰å…¥æ–‡æª”
        with open(os.path.join(directory, "documents.json"), 'r', encoding='utf-8') as f:
            docs_data = json.load(f)

        self.documents = []
        for doc_data in docs_data:
            doc = Document(
                id=doc_data["id"],
                content=doc_data["content"],
                metadata=doc_data["metadata"]
            )
            self.documents.append(doc)

        print(f"å¾ {directory} è¼‰å…¥äº† {len(self.documents)} å€‹æ–‡æª”")


def main():
    """ä¸»å‡½æ•¸ï¼šç¤ºç¯„æ··åˆ RAG ç³»çµ±çš„ä½¿ç”¨"""

    print("=== æ··åˆæª¢ç´¢ RAG ç³»çµ± (FAISS + BM25) ===\n")

    # åˆå§‹åŒ–ç³»çµ±
    rag = HybridRAGSystem(
        model="gpt-3.5-turbo",
        vector_weight=0.6,  # å‘é‡æª¢ç´¢æ¬Šé‡
        keyword_weight=0.4   # é—œéµå­—æª¢ç´¢æ¬Šé‡
    )

    # æ·»åŠ çŸ¥è­˜åº«
    knowledge_base = [
        {
            "text": """Pythonæ˜¯ä¸€ç¨®é«˜éšç¨‹å¼èªè¨€ï¼Œç”±Guido van Rossumåœ¨1991å¹´å‰µé€ ã€‚
            Pythonå¼·èª¿ç¨‹å¼ç¢¼çš„å¯è®€æ€§ï¼Œä½¿ç”¨ç¸®æ’ä¾†å®šç¾©ç¨‹å¼ç¢¼å¡Šã€‚
            Pythonæ”¯æ´å¤šç¨®ç¨‹å¼è¨­è¨ˆç¯„å¼ï¼ŒåŒ…æ‹¬ç‰©ä»¶å°å‘ã€ç¨‹åºå¼å’Œå‡½æ•¸å¼ç¨‹å¼è¨­è¨ˆã€‚
            Pythonæ“æœ‰è±å¯Œçš„æ¨™æº–åº«å’Œç¬¬ä¸‰æ–¹å¥—ä»¶ç”Ÿæ…‹ç³»çµ±ï¼Œå¦‚NumPyã€Pandasã€TensorFlowç­‰ã€‚
            Pythonçš„æ‡‰ç”¨é ˜åŸŸåŒ…æ‹¬ç¶²é é–‹ç™¼ã€è³‡æ–™ç§‘å­¸ã€äººå·¥æ™ºæ…§ã€è‡ªå‹•åŒ–è…³æœ¬ç­‰ã€‚""",
            "source": "PythonåŸºç¤çŸ¥è­˜"
        },
        {
            "text": """æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹åˆ†æ”¯ï¼Œè®“é›»è…¦èƒ½å¤ å¾è³‡æ–™ä¸­å­¸ç¿’ã€‚
            ç›£ç£å¼å­¸ç¿’éœ€è¦æ¨™è¨˜çš„è¨“ç·´è³‡æ–™ï¼ŒåŒ…æ‹¬åˆ†é¡å’Œå›æ­¸ä»»å‹™ã€‚
            éç›£ç£å¼å­¸ç¿’ä¸éœ€è¦æ¨™è¨˜è³‡æ–™ï¼Œå¸¸ç”¨æ–¼èšé¡å’Œé™ç¶­ã€‚
            å¼·åŒ–å­¸ç¿’é€éèˆ‡ç’°å¢ƒäº’å‹•ä¾†å­¸ç¿’æœ€ä½³ç­–ç•¥ã€‚
            å¸¸è¦‹çš„æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•åŒ…æ‹¬æ±ºç­–æ¨¹ã€éš¨æ©Ÿæ£®æ—ã€æ”¯æ´å‘é‡æ©Ÿã€ç¥ç¶“ç¶²è·¯ç­‰ã€‚
            æ©Ÿå™¨å­¸ç¿’çš„æ‡‰ç”¨åŒ…æ‹¬åœ–åƒè­˜åˆ¥ã€è‡ªç„¶èªè¨€è™•ç†ã€æ¨è–¦ç³»çµ±ã€ç•°å¸¸æª¢æ¸¬ç­‰ã€‚""",
            "source": "æ©Ÿå™¨å­¸ç¿’æ¦‚è«–"
        },
        {
            "text": """æ·±åº¦å­¸ç¿’ä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†å­¸ç¿’è³‡æ–™çš„è¤‡é›œè¡¨ç¤ºã€‚
            å·ç©ç¥ç¶“ç¶²è·¯(CNN)æ“…é•·è™•ç†åœ–åƒè³‡æ–™ï¼Œé€éå·ç©å±¤å’Œæ± åŒ–å±¤æå–ç‰¹å¾µã€‚
            å¾ªç’°ç¥ç¶“ç¶²è·¯(RNN)é©åˆè™•ç†åºåˆ—è³‡æ–™å¦‚æ–‡å­—å’Œæ™‚é–“åºåˆ—ã€‚
            é•·çŸ­æœŸè¨˜æ†¶ç¶²è·¯(LSTM)å’Œé–€æ§å¾ªç’°å–®å…ƒ(GRU)è§£æ±ºäº†RNNçš„æ¢¯åº¦æ¶ˆå¤±å•é¡Œã€‚
            Transformeræ¶æ§‹é©æ–°äº†è‡ªç„¶èªè¨€è™•ç†é ˜åŸŸï¼Œå¼•å…¥äº†è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ã€‚
            BERTã€GPTç³»åˆ—æ¨¡å‹éƒ½æ˜¯åŸºæ–¼Transformeræ¶æ§‹çš„é è¨“ç·´èªè¨€æ¨¡å‹ã€‚""",
            "source": "æ·±åº¦å­¸ç¿’æŠ€è¡“"
        },
        {
            "text": """å‘é‡è³‡æ–™åº«æ˜¯å°ˆé–€ç”¨æ–¼å„²å­˜å’Œæª¢ç´¢é«˜ç¶­å‘é‡çš„è³‡æ–™åº«ç³»çµ±ã€‚
            FAISSæ˜¯Facebooké–‹ç™¼çš„é«˜æ•ˆç›¸ä¼¼åº¦æœå°‹åº«ï¼Œæ”¯æ´åå„„ç´šå‘é‡æª¢ç´¢ã€‚
            å‘é‡è³‡æ–™åº«çš„æ‡‰ç”¨åŒ…æ‹¬èªç¾©æœå°‹ã€æ¨è–¦ç³»çµ±ã€åœ–åƒæª¢ç´¢ç­‰ã€‚
            å¸¸è¦‹çš„å‘é‡è³‡æ–™åº«åŒ…æ‹¬Pineconeã€Weaviateã€Milvusã€Chromaç­‰ã€‚
            å‘é‡æª¢ç´¢é€šå¸¸ä½¿ç”¨é¤˜å¼¦ç›¸ä¼¼åº¦ã€æ­æ°è·é›¢æˆ–å…§ç©ä¾†è¡¡é‡ç›¸ä¼¼æ€§ã€‚
            ç´¢å¼•æŠ€è¡“å¦‚LSHã€HNSWã€IVFå¯ä»¥åŠ é€Ÿå¤§è¦æ¨¡å‘é‡æª¢ç´¢ã€‚""",
            "source": "å‘é‡è³‡æ–™åº«æŠ€è¡“"
        },
        {
            "text": """BM25æ˜¯ä¸€ç¨®ç¶“å…¸çš„è³‡è¨Šæª¢ç´¢æ’åºå‡½æ•¸ï¼ŒåŸºæ–¼æ¦‚ç‡æª¢ç´¢æ¨¡å‹ã€‚
            BM25è€ƒæ…®äº†è©é »(TF)ã€é€†æ–‡æª”é »ç‡(IDF)å’Œæ–‡æª”é•·åº¦æ­¸ä¸€åŒ–ã€‚
            ç›¸æ¯”TF-IDFï¼ŒBM25å¼•å…¥äº†è©é »é£½å’Œåº¦çš„æ¦‚å¿µï¼Œé¿å…é«˜é »è©éåº¦å½±éŸ¿ã€‚
            BM25çš„åƒæ•¸k1æ§åˆ¶è©é »é£½å’Œåº¦ï¼Œbæ§åˆ¶æ–‡æª”é•·åº¦æ­¸ä¸€åŒ–ç¨‹åº¦ã€‚
            BM25åœ¨å‚³çµ±æœå°‹å¼•æ“å¦‚Elasticsearchä¸­å»£æ³›ä½¿ç”¨ã€‚
            æ··åˆæª¢ç´¢çµåˆäº†BM25çš„ç²¾ç¢ºåŒ¹é…å’Œå‘é‡æª¢ç´¢çš„èªç¾©ç†è§£å„ªå‹¢ã€‚""",
            "source": "BM25æª¢ç´¢æŠ€è¡“"
        }
    ]

    print("=== å»ºç«‹çŸ¥è­˜åº« ===")
    for kb in knowledge_base:
        rag.add_knowledge(kb["text"], kb["source"])

    print("\n=== æ¸¬è©¦å•ç­”ï¼ˆæ··åˆæª¢ç´¢ï¼‰ ===")

    # æ¸¬è©¦å•é¡Œ
    test_questions = [
        "Pythonæ˜¯èª°å‰µé€ çš„ï¼Ÿå®ƒæœ‰å“ªäº›æ‡‰ç”¨é ˜åŸŸï¼Ÿ",
        "ä»€éº¼æ˜¯ç›£ç£å¼å­¸ç¿’ï¼Ÿè«‹èˆ‰ä¾‹èªªæ˜ã€‚",
        "CNNå’ŒRNNåˆ†åˆ¥é©åˆè™•ç†ä»€éº¼é¡å‹çš„è³‡æ–™ï¼Ÿ",
        "FAISSæ˜¯ä»€éº¼ï¼Ÿå®ƒæœ‰ä»€éº¼ç”¨é€”ï¼Ÿ",
        "BM25ç›¸æ¯”TF-IDFæœ‰ä»€éº¼å„ªå‹¢ï¼Ÿ",
        "Transformeræ¶æ§‹ç‚ºä»€éº¼é‡è¦ï¼Ÿ",
        "å‘é‡è³‡æ–™åº«æœ‰å“ªäº›å¸¸è¦‹çš„æ‡‰ç”¨å ´æ™¯ï¼Ÿ"
    ]

    # æ¸¬è©¦ä¸åŒæª¢ç´¢æ¨¡å¼
    for i, question in enumerate(test_questions[:3], 1):
        print(f"\n{'='*60}")
        print(f"æ¸¬è©¦ {i}: {question}")

        # æ··åˆæª¢ç´¢ï¼ˆä¸ä½¿ç”¨é‡æ’åºï¼‰
        result = rag.answer_question(question, use_reranking=False)
        print(f"\n[æ··åˆæª¢ç´¢]")
        print(f"ç­”æ¡ˆï¼š{result['answer']}")
        print(f"ä¾†æºï¼š{', '.join(result['sources'])}")
        print(f"ä¿¡å¿ƒåº¦ï¼š{result['confidence']:.2%}")

        # æ··åˆæª¢ç´¢ï¼ˆä½¿ç”¨é‡æ’åºï¼‰
        # result_reranked = rag.answer_question(question, use_reranking=True)
        # print(f"\n[æ··åˆæª¢ç´¢ + é‡æ’åº]")
        # print(f"ç­”æ¡ˆï¼š{result_reranked['answer']}")
        # print(f"ä¿¡å¿ƒåº¦ï¼š{result_reranked['confidence']:.2%}")

    # å„²å­˜ç³»çµ±
    print("\n=== å„²å­˜ç³»çµ± ===")
    rag.save_system("hybrid_rag_system")

    # æ¸¬è©¦è¼‰å…¥
    print("\n=== æ¸¬è©¦è¼‰å…¥ç³»çµ± ===")
    new_rag = HybridRAGSystem()
    new_rag.load_system("hybrid_rag_system")

    # ä½¿ç”¨è¼‰å…¥çš„ç³»çµ±å›ç­”å•é¡Œ
    test_result = new_rag.answer_question("ä»€éº¼æ˜¯å‘é‡è³‡æ–™åº«ï¼Ÿ")
    print(f"\nè¼‰å…¥å¾Œæ¸¬è©¦å•é¡Œï¼šä»€éº¼æ˜¯å‘é‡è³‡æ–™åº«ï¼Ÿ")
    print(f"ç­”æ¡ˆï¼š{test_result['answer']}")

    # äº’å‹•æ¨¡å¼
    print("\n" + "="*60)
    print("=== äº’å‹•å•ç­”æ¨¡å¼ ===")
    print("è¼¸å…¥å•é¡Œä¾†æ¸¬è©¦æ··åˆRAGç³»çµ±ï¼ˆè¼¸å…¥'quit'çµæŸï¼‰")
    print("è¼¸å…¥'rerank'åˆ‡æ›é‡æ’åºæ¨¡å¼")
    print()

    use_reranking = False

    while True:
        user_input = input("ğŸ‘¤ ä½ çš„å•é¡Œï¼š").strip()

        if user_input.lower() == 'quit':
            print("ğŸ‘‹ å†è¦‹ï¼")
            break

        if user_input.lower() == 'rerank':
            use_reranking = not use_reranking
            print(f"ğŸ“Š é‡æ’åºæ¨¡å¼ï¼š{'é–‹å•Ÿ' if use_reranking else 'é—œé–‰'}")
            continue

        if user_input:
            result = rag.answer_question(user_input, use_reranking=use_reranking)
            print(f"ğŸ¤– ç­”æ¡ˆï¼š{result['answer']}")
            print(f"ğŸ“š è³‡æ–™ä¾†æºï¼š{', '.join(result['sources'])}")
            print(f"ğŸ“Š ä¿¡å¿ƒåº¦ï¼š{result['confidence']:.2%}")
            print(f"ğŸ” æª¢ç´¢æ–¹å¼ï¼š{result['retrieval_method']}")
            print()


if __name__ == "__main__":
    main()