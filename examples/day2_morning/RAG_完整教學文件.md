# ğŸ“ RAG ç³»çµ±å®Œæ•´æ•™å­¸ï¼šå¾é›¶é–‹å§‹å»ºç«‹æ™ºæ…§æ–‡ä»¶å•ç­”ç³»çµ±

## ğŸ“š ç›®éŒ„
1. [ä»€éº¼æ˜¯ RAGï¼Ÿ](#ä»€éº¼æ˜¯-rag)
2. [ç³»çµ±æ¶æ§‹æ¦‚è¦½](#ç³»çµ±æ¶æ§‹æ¦‚è¦½)
3. [ç’°å¢ƒæº–å‚™èˆ‡å®‰è£](#ç’°å¢ƒæº–å‚™èˆ‡å®‰è£)
4. [æ ¸å¿ƒæ¦‚å¿µè©³è§£](#æ ¸å¿ƒæ¦‚å¿µè©³è§£)
5. [ç¨‹å¼ç¢¼é€æ­¥è§£æ](#ç¨‹å¼ç¢¼é€æ­¥è§£æ)
6. [å¯¦ä½œç·´ç¿’](#å¯¦ä½œç·´ç¿’)
7. [å¸¸è¦‹å•é¡Œèˆ‡å„ªåŒ–](#å¸¸è¦‹å•é¡Œèˆ‡å„ªåŒ–)

---

## ğŸ¤” ä»€éº¼æ˜¯ RAGï¼Ÿ

### RAG çš„å…¨å
**R**etrieval-**A**ugmented **G**enerationï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰

### ç‚ºä»€éº¼éœ€è¦ RAGï¼Ÿ

æƒ³åƒä¸€å€‹å ´æ™¯ï¼š
- ä½ æœ‰ä¸€å€‹ AI åŠ©æ‰‹ï¼ˆå¦‚ ChatGPTï¼‰
- ä½ å•å®ƒï¼šã€Œæˆ‘å€‘å…¬å¸æœ€æ–°çš„ç”¢å“è¦æ ¼æ˜¯ä»€éº¼ï¼Ÿã€
- AI å›ç­”ï¼šã€ŒæŠ±æ­‰ï¼Œæˆ‘ä¸çŸ¥é“ä½ å€‘å…¬å¸çš„å…§éƒ¨è³‡æ–™ã€

**å•é¡Œåœ¨å“ªï¼Ÿ**
1. AI æ¨¡å‹çš„çŸ¥è­˜æœ‰æˆªæ­¢æ—¥æœŸï¼ˆä¾‹å¦‚ï¼š2024å¹´ä¹‹å‰ï¼‰
2. AI ä¸çŸ¥é“ä½ çš„ç§æœ‰è³‡æ–™ï¼ˆå…¬å¸æ–‡ä»¶ã€å€‹äººç­†è¨˜ç­‰ï¼‰
3. AI å¯èƒ½æœƒã€Œå¹»æƒ³ã€ï¼ˆHallucinationï¼‰å‡ºéŒ¯èª¤ç­”æ¡ˆ

**RAG å¦‚ä½•è§£æ±ºï¼Ÿ**
```
ä½¿ç”¨è€…å•é¡Œ â†’ æœå°‹ç›¸é—œæ–‡ä»¶ â†’ å°‡æ–‡ä»¶å…§å®¹çµ¦ AI â†’ AI åŸºæ–¼æ–‡ä»¶å›ç­”
```

### ç”Ÿæ´»åŒ–æ¯”å–»

RAG å°±åƒæ˜¯ä¸€å€‹ã€Œé–‹å·è€ƒè©¦ã€ç³»çµ±ï¼š
- **å‚³çµ± AI**ï¼šé–‰å·è€ƒè©¦ï¼Œåªèƒ½ç”¨è…¦ä¸­è¨˜æ†¶å›ç­”
- **RAG ç³»çµ±**ï¼šé–‹å·è€ƒè©¦ï¼Œå¯ä»¥æŸ¥é–±è³‡æ–™å¾Œå›ç­”

---

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹æ¦‚è¦½

### RAG ç³»çµ±çš„äº”å¤§çµ„ä»¶

```mermaid
graph LR
    A[ğŸ“„ PDFæ–‡ä»¶] --> B[ğŸ“ æ–‡å­—è™•ç†å™¨]
    B --> C[ğŸ”¢ å‘é‡åŒ–å™¨]
    C --> D[ğŸ’¾ å‘é‡è³‡æ–™åº«]
    E[â“ ä½¿ç”¨è€…å•é¡Œ] --> F[ğŸ” æœå°‹å¼•æ“]
    F --> D
    D --> G[ğŸ“š ç›¸é—œæ–‡ä»¶]
    G --> H[ğŸ¤– AIæ¨¡å‹]
    E --> H
    H --> I[ğŸ’¡ ç­”æ¡ˆ]
```

### å„çµ„ä»¶åŠŸèƒ½èªªæ˜

| çµ„ä»¶ | åŠŸèƒ½ | æ¯”å–» |
|------|------|------|
| **PDF è™•ç†å™¨** | è®€å–PDFï¼Œæå–æ–‡å­— | åƒæ˜¯æŠŠæ›¸æœ¬å…§å®¹æ‰“å­—åˆ°é›»è…¦ |
| **æ–‡å­—åˆ‡å¡Šå™¨** | å°‡é•·æ–‡åˆ‡æˆå°æ®µè½ | åƒæ˜¯æŠŠé•·æ–‡ç« åˆ†æˆæ®µè½ç­†è¨˜ |
| **å‘é‡åŒ–å™¨** | å°‡æ–‡å­—è½‰æ›æˆæ•¸å­— | åƒæ˜¯çµ¦æ¯æ®µæ–‡å­—ä¸€å€‹åº§æ¨™ |
| **å‘é‡è³‡æ–™åº«** | å„²å­˜ä¸¦æœå°‹å‘é‡ | åƒæ˜¯æ™ºæ…§å‹åœ–æ›¸é¤¨ç´¢å¼• |
| **AI ç”Ÿæˆå™¨** | æ ¹æ“šè³‡æ–™ç”¢ç”Ÿç­”æ¡ˆ | åƒæ˜¯ä¸€å€‹æœƒè®€è³‡æ–™çš„å°ˆå®¶ |

---

## ğŸ’» ç’°å¢ƒæº–å‚™èˆ‡å®‰è£

### æ­¥é©Ÿ 1ï¼šå»ºç«‹è™›æ“¬ç’°å¢ƒ

```bash
# ç‚ºä»€éº¼éœ€è¦è™›æ“¬ç’°å¢ƒï¼Ÿ
# æƒ³åƒè™›æ“¬ç’°å¢ƒå°±åƒæ˜¯ä¸€å€‹ç¨ç«‹çš„å·¥ä½œå®¤ï¼Œ
# ä¸æœƒå½±éŸ¿åˆ°é›»è…¦ä¸Šå…¶ä»–å°ˆæ¡ˆ

# å»ºç«‹è™›æ“¬ç’°å¢ƒ
python -m venv llm_pure

# å•Ÿå‹•è™›æ“¬ç’°å¢ƒï¼ˆLinux/Macï¼‰
source llm_pure/bin/activate

# å•Ÿå‹•è™›æ“¬ç’°å¢ƒï¼ˆWindowsï¼‰
llm_pure\Scripts\activate
```

### æ­¥é©Ÿ 2ï¼šå®‰è£å¿…è¦å¥—ä»¶

```bash
# å®‰è£åŸºæœ¬å¥—ä»¶
pip install sentence-transformers  # æ–‡å­—è½‰å‘é‡å·¥å…·
pip install faiss-cpu             # å‘é‡è³‡æ–™åº«
pip install pypdf                 # PDF è®€å–å·¥å…·
pip install numpy                 # æ•¸å­¸é‹ç®—
pip install requests              # ç¶²è·¯è«‹æ±‚
pip install transformers          # AI æ¨¡å‹æ¡†æ¶
pip install torch                 # æ·±åº¦å­¸ç¿’æ¡†æ¶
```

### å¥—ä»¶åŠŸèƒ½èªªæ˜

```python
# æ¯å€‹å¥—ä»¶çš„ä½œç”¨è§£é‡‹
"""
1. sentence-transformers
   - åŠŸèƒ½ï¼šå°‡æ–‡å­—è½‰æ›æˆå‘é‡ï¼ˆæ•¸å­—è¡¨ç¤ºï¼‰
   - æ¯”å–»ï¼šåƒæ˜¯ç¿»è­¯å®˜ï¼ŒæŠŠäººé¡èªè¨€ç¿»è­¯æˆé›»è…¦æ‡‚çš„æ•¸å­—

2. faiss-cpu
   - åŠŸèƒ½ï¼šFacebook é–‹ç™¼çš„å‘é‡æœå°‹å¼•æ“
   - æ¯”å–»ï¼šåƒæ˜¯è¶…å¿«é€Ÿçš„åœ–æ›¸é¤¨ç›®éŒ„ç³»çµ±

3. pypdf
   - åŠŸèƒ½ï¼šè®€å– PDF æª”æ¡ˆå…§å®¹
   - æ¯”å–»ï¼šåƒæ˜¯ PDF é–±è®€å™¨çš„ç¨‹å¼ç‰ˆæœ¬

4. transformers
   - åŠŸèƒ½ï¼šä½¿ç”¨å„ç¨® AI æ¨¡å‹
   - æ¯”å–»ï¼šåƒæ˜¯ AI æ¨¡å‹çš„ç®¡ç†å™¨
"""
```

---

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µè©³è§£

### æ¦‚å¿µ 1ï¼šæ–‡å­—åµŒå…¥ï¼ˆText Embeddingï¼‰

#### ä»€éº¼æ˜¯åµŒå…¥ï¼Ÿ

```python
# å‚³çµ±æ–¹å¼ï¼šæ–‡å­—å°±æ˜¯æ–‡å­—
text1 = "æˆ‘æ„›åƒè˜‹æœ"
text2 = "æˆ‘å–œæ­¡åƒæ°´æœ"
# é›»è…¦ä¸çŸ¥é“é€™å…©å¥è©±ç›¸é—œ

# åµŒå…¥æ–¹å¼ï¼šæ–‡å­—è®Šæˆå‘é‡ï¼ˆæ•¸å­—åº§æ¨™ï¼‰
vector1 = [0.2, 0.8, 0.5, ...]  # 768å€‹æ•¸å­—
vector2 = [0.3, 0.7, 0.6, ...]  # 768å€‹æ•¸å­—
# é›»è…¦å¯ä»¥è¨ˆç®—å…©å€‹å‘é‡çš„è·é›¢ï¼Œåˆ¤æ–·ç›¸ä¼¼åº¦
```

#### è¦–è¦ºåŒ–ç†è§£

æƒ³åƒæ¯å€‹å¥å­éƒ½æ˜¯ç©ºé–“ä¸­çš„ä¸€å€‹é»ï¼š
- ç›¸ä¼¼çš„å¥å­ï¼šé»é å¾—å¾ˆè¿‘
- ä¸åŒçš„å¥å­ï¼šé»é›¢å¾—å¾ˆé 

```
    3Dç©ºé–“ç¤ºæ„åœ–

         "è²“å’ª"
           â—
          / \
         /   \
    "å°è²“" â— â— "è²“ç§‘å‹•ç‰©"


    â— "æ±½è»Š"ï¼ˆé›¢è²“ç›¸é—œè©å¾ˆé ï¼‰
```

### æ¦‚å¿µ 2ï¼šæ–‡æª”åˆ‡å¡Šï¼ˆChunkingï¼‰

#### ç‚ºä»€éº¼è¦åˆ‡å¡Šï¼Ÿ

```python
# å•é¡Œï¼šä¸€æœ¬æ›¸æœ‰ 500 é 
full_book = "å¾ˆé•·å¾ˆé•·çš„å…§å®¹..." * 10000

# æŒ‘æˆ°ï¼š
# 1. AI æ¨¡å‹æœ‰è¼¸å…¥é•·åº¦é™åˆ¶ï¼ˆåƒæ˜¯ç´™å¼µå¤§å°æœ‰é™ï¼‰
# 2. æœå°‹æ•´æœ¬æ›¸æ•ˆç‡å¾ˆä½
# 3. å¤§éƒ¨åˆ†å…§å®¹å¯èƒ½ä¸ç›¸é—œ

# è§£æ±ºæ–¹æ¡ˆï¼šåˆ‡æˆå°å¡Š
chunk1 = "ç¬¬ä¸€ç« ç¬¬ä¸€æ®µ..."  # 500 å€‹å­—
chunk2 = "ç¬¬ä¸€ç« ç¬¬äºŒæ®µ..."  # 500 å€‹å­—
# æ¯å¡Šéƒ½å¯ä»¥ç¨ç«‹æœå°‹å’Œè™•ç†
```

#### Chunk Size å’Œ Overlap è§£é‡‹

```python
# åŸå§‹æ–‡å­—
text = "A B C D E F G H I J K L M N O"

# Chunk Size = 5, Overlap = 2
chunk1 = "A B C D E"      # ç¬¬1å¡Šï¼šå­— 1-5
chunk2 = "D E F G H"      # ç¬¬2å¡Šï¼šå­— 4-8ï¼ˆèˆ‡ç¬¬1å¡Šé‡ç–Š D Eï¼‰
chunk3 = "G H I J K"      # ç¬¬3å¡Šï¼šå­— 7-11ï¼ˆèˆ‡ç¬¬2å¡Šé‡ç–Š G Hï¼‰

# ç‚ºä»€éº¼è¦é‡ç–Šï¼Ÿ
# é¿å…é‡è¦è³‡è¨Šå‰›å¥½è¢«åˆ‡æ–·åœ¨å…©å¡Šä¹‹é–“
```

### æ¦‚å¿µ 3ï¼šå‘é‡ç›¸ä¼¼åº¦æœå°‹

#### å¦‚ä½•åˆ¤æ–·ç›¸ä¼¼åº¦ï¼Ÿ

```python
import numpy as np

def cosine_similarity(vec1, vec2):
    """
    é¤˜å¼¦ç›¸ä¼¼åº¦ï¼šè¨ˆç®—å…©å€‹å‘é‡çš„å¤¾è§’
    - çµæœæ¥è¿‘ 1ï¼šéå¸¸ç›¸ä¼¼
    - çµæœæ¥è¿‘ 0ï¼šç„¡é—œ
    - çµæœæ¥è¿‘ -1ï¼šç›¸å
    """
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    return dot_product / (norm1 * norm2)

# ç¯„ä¾‹
vec_apple = [0.8, 0.2, 0.5]  # "è˜‹æœ"çš„å‘é‡
vec_fruit = [0.7, 0.3, 0.4]  # "æ°´æœ"çš„å‘é‡
vec_car = [0.1, 0.9, 0.2]    # "æ±½è»Š"çš„å‘é‡

sim1 = cosine_similarity(vec_apple, vec_fruit)  # 0.95ï¼ˆå¾ˆç›¸ä¼¼ï¼‰
sim2 = cosine_similarity(vec_apple, vec_car)    # 0.30ï¼ˆä¸ç›¸ä¼¼ï¼‰
```

---

## ğŸ“– ç¨‹å¼ç¢¼é€æ­¥è§£æ

### Part 1: PDF æ–‡ä»¶è™•ç†

```python
class PDFProcessor:
    """è™•ç† PDF æª”æ¡ˆçš„é¡åˆ¥"""

    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        """
        åˆå§‹åŒ–è¨­å®š

        åƒæ•¸è§£é‡‹ï¼š
        - chunk_size: æ¯å€‹æ–‡å­—å¡Šçš„å¤§å°ï¼ˆå­—æ•¸ï¼‰
          æƒ³åƒæˆï¼šæ¯å¼µç­†è¨˜å¡ç‰‡å¯ä»¥å¯« 500 å€‹å­—

        - chunk_overlap: ç›¸é„°å¡Šçš„é‡ç–Šå­—æ•¸
          æƒ³åƒæˆï¼šç‚ºäº†ä¿æŒé€£è²«ï¼Œä¸‹ä¸€å¼µå¡ç‰‡æœƒé‡è¤‡å‰ä¸€å¼µçš„æœ€å¾Œ 50 å€‹å­—
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def load_pdf(self, pdf_path: str) -> str:
        """
        è®€å– PDF æª”æ¡ˆ

        æµç¨‹ï¼š
        1. é–‹å•Ÿ PDF æª”æ¡ˆ
        2. é€é è®€å–æ–‡å­—
        3. åˆä½µæ‰€æœ‰é é¢çš„æ–‡å­—
        """
        text = ""
        with open(pdf_path, 'rb') as file:
            pdf_reader = PdfReader(file)

            # é€é è™•ç†
            for page_num, page in enumerate(pdf_reader.pages):
                page_text = page.extract_text()
                if page_text:
                    # åŠ å…¥é ç¢¼æ¨™è¨˜ï¼Œæ–¹ä¾¿è¿½è¹¤ä¾†æº
                    text += f"\n[Page {page_num + 1}]\n{page_text}"

        return text

    def chunk_text(self, text: str, source: str) -> List[Document]:
        """
        å°‡é•·æ–‡å­—åˆ‡æˆå°å¡Š

        æ­¥é©Ÿè©³è§£ï¼š
        1. æ¸…ç†æ–‡å­—ï¼ˆç§»é™¤å¤šé¤˜ç©ºç™½ï¼‰
        2. æŒ‰ç…§å­—æ•¸åˆ‡å‰²
        3. ä¿ç•™é‡ç–Šéƒ¨åˆ†
        4. è¨˜éŒ„æ¯å¡Šçš„ä¾†æºè³‡è¨Š
        """
        # æ­¥é©Ÿ1ï¼šæ¸…ç†æ–‡å­—
        text = re.sub(r'\s+', ' ', text)  # å¤šå€‹ç©ºç™½è®Šä¸€å€‹
        text = text.strip()                # ç§»é™¤é ­å°¾ç©ºç™½

        # æ­¥é©Ÿ2ï¼šåˆ†å‰²æˆå­—è©
        words = text.split()

        chunks = []
        # æ­¥é©Ÿ3ï¼šå»ºç«‹æ–‡å­—å¡Š
        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):
            # å–å‡º chunk_size å€‹å­—
            chunk_words = words[i:i + self.chunk_size]
            chunk_text = ' '.join(chunk_words)

            # åªä¿ç•™æœ‰æ„ç¾©çš„å¡Šï¼ˆè‡³å°‘ 50 å€‹å­—å…ƒï¼‰
            if len(chunk_text) > 50:
                doc = Document(
                    content=chunk_text,
                    metadata={
                        'source': source,      # ä¾†æºæª”æ¡ˆ
                        'chunk_id': len(chunks),  # ç¬¬å¹¾å¡Š
                        'start_index': i       # åœ¨åŸæ–‡çš„ä½ç½®
                    }
                )
                chunks.append(doc)

        return chunks
```

### Part 2: æ–‡å­—å‘é‡åŒ–ï¼ˆEmbeddingï¼‰

```python
class EmbeddingModel:
    """å°‡æ–‡å­—è½‰æ›æˆå‘é‡çš„é¡åˆ¥"""

    def __init__(self, model_name: str = "BAAI/bge-large-en-v1.5"):
        """
        è¼‰å…¥åµŒå…¥æ¨¡å‹

        BGE æ¨¡å‹ä»‹ç´¹ï¼š
        - BAAI: åŒ—äº¬æ™ºæºäººå·¥æ™ºèƒ½ç ”ç©¶é™¢
        - bge: BAAI General Embeddingï¼ˆé€šç”¨åµŒå…¥ï¼‰
        - large: æ¨¡å‹å¤§å°ï¼ˆæ•ˆæœè¼ƒå¥½ä½†è¼ƒæ…¢ï¼‰
        - en: è‹±æ–‡ç‰ˆæœ¬ï¼ˆä¹Ÿæœ‰ä¸­æ–‡ç‰ˆ zhï¼‰
        - v1.5: ç‰ˆæœ¬è™Ÿ

        è¼¸å‡ºç¶­åº¦ï¼š1024 ç¶­ï¼ˆ1024å€‹æ•¸å­—è¡¨ç¤ºä¸€æ®µæ–‡å­—ï¼‰
        """
        print(f"è¼‰å…¥åµŒå…¥æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.dimension = 1024  # BGE-large çš„å‘é‡ç¶­åº¦

    def encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """
        å°‡æ–‡å­—åˆ—è¡¨è½‰æ›æˆå‘é‡

        åƒæ•¸èªªæ˜ï¼š
        - texts: è¦è½‰æ›çš„æ–‡å­—åˆ—è¡¨
        - batch_size: æ‰¹æ¬¡è™•ç†å¤§å°ï¼ˆä¸€æ¬¡è™•ç†å¹¾å€‹ï¼‰

        è™•ç†æµç¨‹ï¼š
        1. å°‡æ–‡å­—åˆ†æ‰¹ï¼ˆé¿å…è¨˜æ†¶é«”ä¸è¶³ï¼‰
        2. æ¯æ‰¹è½‰æ›æˆå‘é‡
        3. æ­£è¦åŒ–å‘é‡ï¼ˆè®“é•·åº¦ç‚º1ï¼Œæ–¹ä¾¿è¨ˆç®—ç›¸ä¼¼åº¦ï¼‰
        """
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=True,      # é¡¯ç¤ºé€²åº¦æ¢
            convert_to_numpy=True,       # è½‰æˆ NumPy é™£åˆ—
            normalize_embeddings=True    # æ­£è¦åŒ–ï¼ˆé‡è¦ï¼ï¼‰
        )
        return embeddings
```

### Part 3: å‘é‡è³‡æ–™åº«ï¼ˆFAISSï¼‰

```python
class FAISSIndex:
    """
    FAISS å‘é‡è³‡æ–™åº«

    FAISS æ˜¯ä»€éº¼ï¼Ÿ
    - Facebook AI Similarity Search
    - å°ˆé–€ç”¨ä¾†å¿«é€Ÿæœå°‹ç›¸ä¼¼å‘é‡
    - å¯ä»¥åœ¨ç™¾è¬ç´šå‘é‡ä¸­æ¯«ç§’ç´šæœå°‹
    """

    def __init__(self, dimension: int):
        """
        åˆå§‹åŒ–ç´¢å¼•

        IndexFlatIP è§£é‡‹ï¼š
        - Flat: æš´åŠ›æœå°‹ï¼ˆç²¾ç¢ºä½†é©åˆå°è¦æ¨¡ï¼‰
        - IP: Inner Productï¼ˆå…§ç©ï¼‰ï¼Œç”¨æ–¼æ­£è¦åŒ–å¾Œçš„å‘é‡
        """
        self.dimension = dimension
        self.index = faiss.IndexFlatIP(dimension)
        self.documents = []  # å„²å­˜åŸå§‹æ–‡ä»¶

    def add_documents(self, documents: List[Document], embeddings: np.ndarray):
        """
        å°‡æ–‡ä»¶å’Œå‘é‡åŠ å…¥è³‡æ–™åº«

        æ­¥é©Ÿï¼š
        1. å‘é‡åŠ å…¥ FAISS ç´¢å¼•
        2. æ–‡ä»¶åŠ å…¥åˆ—è¡¨ï¼ˆä¿æŒé †åºä¸€è‡´ï¼‰
        """
        # è½‰æ›æˆ float32ï¼ˆFAISS è¦æ±‚ï¼‰
        self.index.add(embeddings.astype('float32'))
        self.documents.extend(documents)
        print(f"å·²åŠ å…¥ {len(documents)} å€‹æ–‡ä»¶åˆ°ç´¢å¼•")

    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[Document, float]]:
        """
        æœå°‹æœ€ç›¸ä¼¼çš„æ–‡ä»¶

        åƒæ•¸ï¼š
        - query_embedding: å•é¡Œçš„å‘é‡
        - k: è¿”å›å‰ k å€‹æœ€ç›¸ä¼¼çš„çµæœ

        éç¨‹è§£é‡‹ï¼š
        1. è¨ˆç®—å•é¡Œå‘é‡èˆ‡æ‰€æœ‰æ–‡ä»¶å‘é‡çš„ç›¸ä¼¼åº¦
        2. æ’åºæ‰¾å‡ºæœ€ç›¸ä¼¼çš„ k å€‹
        3. è¿”å›æ–‡ä»¶å’Œç›¸ä¼¼åº¦åˆ†æ•¸
        """
        # ç¢ºä¿æ­£ç¢ºçš„å½¢ç‹€å’Œé¡å‹
        query_embedding = query_embedding.reshape(1, -1).astype('float32')

        # æœå°‹ï¼ˆscores: ç›¸ä¼¼åº¦åˆ†æ•¸, indices: æ–‡ä»¶ç´¢å¼•ï¼‰
        scores, indices = self.index.search(query_embedding, k)

        # çµ„åˆçµæœ
        results = []
        for idx, score in zip(indices[0], scores[0]):
            if idx < len(self.documents):
                results.append((self.documents[idx], float(score)))

        return results
```

### Part 4: AI ç”Ÿæˆå™¨ï¼ˆGemma3ï¼‰

```python
class Gemma3Generator:
    """ä½¿ç”¨ Gemma3 æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ"""

    def __init__(self, model_name: str = "google/gemma-3-1b-it"):
        """
        è¼‰å…¥ Gemma3 æ¨¡å‹

        æ¨¡å‹é¸æ“‡ï¼š
        - gemma-3-1b-it: 10å„„åƒæ•¸ï¼Œè¼ƒå¿«ä½†æ•ˆæœä¸€èˆ¬
        - gemma-3-4b-it: 40å„„åƒæ•¸ï¼Œå¹³è¡¡é¸æ“‡
        - gemma-3-12b-it: 120å„„åƒæ•¸ï¼Œæ•ˆæœæœ€å¥½ä½†æœ€æ…¢

        it = instruction-tunedï¼ˆç¶“éæŒ‡ä»¤å¾®èª¿ï¼‰
        """
        print(f"è¼‰å…¥ Gemma3 æ¨¡å‹: {model_name}")

        # æª¢æŸ¥æ˜¯å¦æœ‰ GPU
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è£ç½®: {self.device}")

        # è¼‰å…¥åˆ†è©å™¨å’Œæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto"  # è‡ªå‹•åˆ†é…åˆ°å¯ç”¨è£ç½®
        )

        # è¨­å®šå¡«å……æ¨™è¨˜
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

    def generate(self, prompt: str, max_new_tokens: int = 512) -> str:
        """
        ç”Ÿæˆå›ç­”

        ç”Ÿæˆåƒæ•¸è§£é‡‹ï¼š
        - max_new_tokens: æœ€å¤šç”Ÿæˆå¹¾å€‹è©
        - temperature: å‰µé€ æ€§ï¼ˆ0=ä¿å®ˆ, 1=å‰µæ„ï¼‰
        - top_p: åªè€ƒæ…®æ©Ÿç‡å‰ p% çš„è©
        - do_sample: æ˜¯å¦éš¨æ©Ÿæ¡æ¨£
        """
        # å°‡æç¤ºè©è½‰æ›æˆæ¨¡å‹è¼¸å…¥
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # ç”Ÿæˆå›ç­”
        with torch.no_grad():  # ä¸éœ€è¦è¨ˆç®—æ¢¯åº¦ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.7,    # é©ä¸­çš„å‰µé€ æ€§
                do_sample=True,     # å•Ÿç”¨æ¡æ¨£
                top_p=0.9,         # æ ¸æ¡æ¨£
                pad_token_id=self.tokenizer.pad_token_id
            )

        # è§£ç¢¼æˆæ–‡å­—
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # ç§»é™¤è¼¸å…¥çš„æç¤ºè©ï¼Œåªä¿ç•™ç”Ÿæˆçš„éƒ¨åˆ†
        response = response[len(prompt):].strip()

        return response
```

### Part 5: å®Œæ•´ RAG ç³»çµ±æ•´åˆ

```python
class RAGSystem:
    """æ•´åˆæ‰€æœ‰çµ„ä»¶çš„ RAG ç³»çµ±"""

    def __init__(self):
        """åˆå§‹åŒ–æ‰€æœ‰çµ„ä»¶"""
        print("åˆå§‹åŒ– RAG ç³»çµ±...")

        # 1. PDF è™•ç†å™¨
        self.pdf_processor = PDFProcessor(
            chunk_size=500,      # æ¯å¡Š 500 å€‹å­—
            chunk_overlap=50     # é‡ç–Š 50 å€‹å­—
        )

        # 2. åµŒå…¥æ¨¡å‹
        self.embedding_model = EmbeddingModel()

        # 3. å‘é‡è³‡æ–™åº«
        self.vector_db = FAISSIndex(self.embedding_model.dimension)

        # 4. ç”Ÿæˆæ¨¡å‹
        self.generator = Gemma3Generator()

        # 5. æ–‡ä»¶å„²å­˜
        self.documents = []

    def load_documents(self, directory: str):
        """
        è¼‰å…¥è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰ PDF

        å®Œæ•´æµç¨‹ï¼š
        1. æƒæè³‡æ–™å¤¾æ‰¾ PDF
        2. é€å€‹è®€å– PDF
        3. åˆ‡å¡Šè™•ç†
        4. è½‰æ›æˆå‘é‡
        5. å­˜å…¥è³‡æ–™åº«
        """
        print(f"\nè¼‰å…¥æ–‡ä»¶å¾: {directory}")

        # æ­¥é©Ÿ1: æ‰¾å‡ºæ‰€æœ‰ PDF
        pdf_files = Path(directory).glob('*.pdf')

        all_documents = []
        for pdf_file in pdf_files:
            print(f"è™•ç† {pdf_file.name}...")

            # æ­¥é©Ÿ2: è®€å– PDF
            text = self.pdf_processor.load_pdf(str(pdf_file))

            if text:
                # æ­¥é©Ÿ3: åˆ‡å¡Š
                chunks = self.pdf_processor.chunk_text(text, pdf_file.name)
                all_documents.extend(chunks)
                print(f"  å»ºç«‹äº† {len(chunks)} å€‹æ–‡å­—å¡Š")

        self.documents = all_documents
        print(f"ç¸½å…±è¼‰å…¥: {len(self.documents)} å€‹æ–‡å­—å¡Š")

        if self.documents:
            # æ­¥é©Ÿ4: ç”Ÿæˆå‘é‡
            print("\nç”Ÿæˆå‘é‡åµŒå…¥...")
            texts = [doc.content for doc in self.documents]
            embeddings = self.embedding_model.encode(texts)

            # æ­¥é©Ÿ5: åŠ å…¥è³‡æ–™åº«
            self.vector_db.add_documents(self.documents, embeddings)
            print("æ–‡ä»¶ç´¢å¼•å®Œæˆï¼")

    def retrieve(self, query: str, k: int = 5) -> List[Document]:
        """
        æª¢ç´¢ç›¸é—œæ–‡ä»¶

        æµç¨‹ï¼š
        1. å°‡å•é¡Œè½‰æˆå‘é‡
        2. åœ¨è³‡æ–™åº«ä¸­æœå°‹
        3. è¿”å›æœ€ç›¸é—œçš„ k å€‹æ–‡ä»¶
        """
        # å•é¡Œå‘é‡åŒ–
        query_embedding = self.embedding_model.encode([query])

        # æœå°‹
        results = self.vector_db.search(query_embedding[0], k=k)

        # åªè¿”å›æ–‡ä»¶ï¼ˆä¸å«åˆ†æ•¸ï¼‰
        return [doc for doc, score in results]

    def query(self, question: str, k: int = 5) -> str:
        """
        å®Œæ•´çš„ RAG å•ç­”æµç¨‹

        æ­¥é©Ÿï¼š
        1. æª¢ç´¢ç›¸é—œæ–‡ä»¶
        2. å»ºæ§‹ä¸Šä¸‹æ–‡
        3. çµ„åˆæç¤ºè©
        4. ç”Ÿæˆç­”æ¡ˆ
        """
        print(f"\nå•é¡Œ: {question}")

        # æ­¥é©Ÿ1: æª¢ç´¢
        relevant_docs = self.retrieve(question, k=k)

        if not relevant_docs:
            return "æ‰¾ä¸åˆ°ç›¸é—œæ–‡ä»¶ã€‚"

        # æ­¥é©Ÿ2: å»ºæ§‹ä¸Šä¸‹æ–‡ï¼ˆåªç”¨å‰3å€‹æœ€ç›¸é—œçš„ï¼‰
        context = "\n\n".join([
            f"[ä¾†æº: {doc.metadata['source']}]\n{doc.content[:500]}..."
            for doc in relevant_docs[:3]
        ])

        # æ­¥é©Ÿ3: å»ºæ§‹æç¤ºè©
        prompt = f"""ä½ æ˜¯ä¸€å€‹æœ‰å¹«åŠ©çš„ AI åŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„æ–‡ä»¶å…§å®¹å›ç­”å•é¡Œã€‚
å¦‚æœæ–‡ä»¶ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹èª å¯¦èªªæ˜ã€‚

æ–‡ä»¶å…§å®¹ï¼š
{context}

å•é¡Œï¼š{question}

å›ç­”ï¼š"""

        # æ­¥é©Ÿ4: ç”Ÿæˆç­”æ¡ˆ
        answer = self.generator.generate(prompt)

        return answer
```

---

## ğŸš€ å¯¦ä½œç·´ç¿’

### ç·´ç¿’ 1ï¼šåŸºæœ¬ä½¿ç”¨

```python
# å®Œæ•´ä½¿ç”¨ç¯„ä¾‹
def main():
    # 1. åˆå§‹åŒ–ç³»çµ±
    rag = RAGSystem()

    # 2. è¼‰å…¥æ–‡ä»¶
    rag.load_documents("data/")  # å°‡ PDF æ”¾åœ¨ data è³‡æ–™å¤¾

    # 3. äº’å‹•å¼å•ç­”
    while True:
        question = input("\nè«‹è¼¸å…¥å•é¡Œï¼ˆè¼¸å…¥ 'quit' çµæŸï¼‰: ")

        if question.lower() == 'quit':
            break

        # å–å¾—ç­”æ¡ˆ
        answer = rag.query(question)
        print(f"\nç­”æ¡ˆ: {answer}")

if __name__ == "__main__":
    main()
```

### ç·´ç¿’ 2ï¼šèª¿æ•´åƒæ•¸å¯¦é©—

```python
# å¯¦é©—ä¸åŒçš„ chunk_size
chunk_sizes = [100, 300, 500, 1000]

for size in chunk_sizes:
    print(f"\næ¸¬è©¦ chunk_size = {size}")

    processor = PDFProcessor(chunk_size=size, chunk_overlap=50)
    # è¼‰å…¥ä¸¦æ¸¬è©¦...

    # è§€å¯Ÿï¼š
    # - å° chunkï¼šæ›´ç²¾ç¢ºä½†å¯èƒ½ç¼ºä¹ä¸Šä¸‹æ–‡
    # - å¤§ chunkï¼šæ›´å¤šä¸Šä¸‹æ–‡ä½†å¯èƒ½åŒ…å«ç„¡é—œè³‡è¨Š
```

### ç·´ç¿’ 3ï¼šè©•ä¼°æª¢ç´¢å“è³ª

```python
def evaluate_retrieval(rag_system, test_questions):
    """è©•ä¼°æª¢ç´¢å“è³ª"""

    for question in test_questions:
        print(f"\nå•é¡Œ: {question}")

        # æª¢ç´¢æ–‡ä»¶
        docs = rag_system.retrieve(question, k=5)

        # é¡¯ç¤ºç›¸é—œåº¦
        for i, doc in enumerate(docs, 1):
            print(f"{i}. ä¾†æº: {doc.metadata['source']}")
            print(f"   å…§å®¹é è¦½: {doc.content[:100]}...")

        # äººå·¥è©•ä¼°ï¼šé€™äº›æ–‡ä»¶ç›¸é—œå—ï¼Ÿ

# æ¸¬è©¦å•é¡Œ
test_questions = [
    "ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ",
    "å¦‚ä½•è¨“ç·´ç¥ç¶“ç¶²è·¯ï¼Ÿ",
    "Transformer æ¶æ§‹çš„å„ªé»ï¼Ÿ"
]

evaluate_retrieval(rag, test_questions)
```

---

## ğŸ”§ å¸¸è¦‹å•é¡Œèˆ‡å„ªåŒ–

### å•é¡Œ 1ï¼šè¨˜æ†¶é«”ä¸è¶³

```python
# å•é¡Œï¼šè¼‰å…¥å¤§æ¨¡å‹æ™‚ OOM (Out of Memory)

# è§£æ±ºæ–¹æ¡ˆ 1ï¼šä½¿ç”¨æ›´å°çš„æ¨¡å‹
generator = Gemma3Generator(model_name="google/gemma-3-1b-it")  # 1B è€Œé 4B

# è§£æ±ºæ–¹æ¡ˆ 2ï¼šä½¿ç”¨é‡åŒ–
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,  # 8-bit é‡åŒ–ï¼Œè¨˜æ†¶é«”æ¸›åŠ
    device_map="auto"
)

# è§£æ±ºæ–¹æ¡ˆ 3ï¼šä½¿ç”¨ CPU
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="cpu"  # ä½¿ç”¨ CPUï¼ˆè¼ƒæ…¢ä½†ä¸é™è¨˜æ†¶é«”ï¼‰
)
```

### å•é¡Œ 2ï¼šæª¢ç´¢ä¸æº–ç¢º

```python
# å•é¡Œï¼šæ‰¾åˆ°çš„æ–‡ä»¶ä¸ç›¸é—œ

# è§£æ±ºæ–¹æ¡ˆ 1ï¼šèª¿æ•´ chunk_size
# å¤ªå°ï¼šç¼ºä¹ä¸Šä¸‹æ–‡
# å¤ªå¤§ï¼šåŒ…å«å¤ªå¤šç„¡é—œè³‡è¨Š
# å»ºè­°ï¼š200-1000 ä¹‹é–“æ¸¬è©¦

# è§£æ±ºæ–¹æ¡ˆ 2ï¼šå¢åŠ  overlap
processor = PDFProcessor(
    chunk_size=500,
    chunk_overlap=100  # å¢åŠ åˆ° 100ï¼ˆåŸæœ¬ 50ï¼‰
)

# è§£æ±ºæ–¹æ¡ˆ 3ï¼šä½¿ç”¨æ›´å¥½çš„åµŒå…¥æ¨¡å‹
# è‹±æ–‡ï¼šBAAI/bge-large-en-v1.5
# ä¸­æ–‡ï¼šBAAI/bge-large-zh-v1.5
# å¤šèªè¨€ï¼šintfloat/multilingual-e5-large
```

### å•é¡Œ 3ï¼šç”Ÿæˆå“è³ªä¸ä½³

```python
# å•é¡Œï¼šAI å›ç­”ä¸æº–ç¢ºæˆ–å¤ªç°¡çŸ­

# è§£æ±ºæ–¹æ¡ˆ 1ï¼šæ”¹é€²æç¤ºè©
prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„æŠ€è¡“é¡§å•ã€‚è«‹æ ¹æ“šæä¾›çš„æ–‡ä»¶è©³ç´°å›ç­”ã€‚

è¦æ±‚ï¼š
1. å¼•ç”¨å…·é«”çš„æ–‡ä»¶å…§å®¹
2. æä¾›å®Œæ•´çš„è§£é‡‹
3. å¦‚æœè³‡è¨Šä¸è¶³ï¼Œèªªæ˜éœ€è¦ä»€éº¼é¡å¤–è³‡è¨Š

æ–‡ä»¶ï¼š{context}
å•é¡Œï¼š{question}
è©³ç´°å›ç­”ï¼š"""

# è§£æ±ºæ–¹æ¡ˆ 2ï¼šèª¿æ•´ç”Ÿæˆåƒæ•¸
outputs = model.generate(
    max_new_tokens=1024,  # å¢åŠ é•·åº¦é™åˆ¶
    temperature=0.3,      # é™ä½éš¨æ©Ÿæ€§ï¼Œæ›´æº–ç¢º
    top_p=0.85,          # æ›´ä¿å®ˆçš„æ¡æ¨£
    repetition_penalty=1.2  # é¿å…é‡è¤‡
)

# è§£æ±ºæ–¹æ¡ˆ 3ï¼šä½¿ç”¨æ›´å¤§/æ›´å¥½çš„æ¨¡å‹
# gemma-3-1b-it â†’ gemma-3-4b-it â†’ gemma-3-12b-it
```

### å„ªåŒ–å»ºè­°ç¸½çµ

| å„ªåŒ–é …ç›® | å»ºè­°è¨­å®š | åŸå›  |
|---------|---------|------|
| Chunk Size | 300-500 å­— | å¹³è¡¡ä¸Šä¸‹æ–‡èˆ‡ç²¾ç¢ºåº¦ |
| Overlap | 50-100 å­— | ç¢ºä¿é€£çºŒæ€§ |
| æª¢ç´¢æ•¸é‡ (k) | 3-5 å€‹ | å¤ªå¤šæœƒå¼•å…¥é›œè¨Š |
| åµŒå…¥æ¨¡å‹ | BGE-large | æ•ˆæœèˆ‡é€Ÿåº¦å¹³è¡¡ |
| ç”Ÿæˆæº«åº¦ | 0.3-0.7 | äº‹å¯¦æ€§ vs å‰µé€ æ€§ |

---

## ğŸ“Š æ•ˆèƒ½è©•ä¼°æŒ‡æ¨™

### 1. æª¢ç´¢è©•ä¼°

```python
def evaluate_retrieval_metrics(rag_system, test_set):
    """
    è©•ä¼°æª¢ç´¢å“è³ª

    æŒ‡æ¨™ï¼š
    - Precision@k: å‰ k å€‹çµæœä¸­ç›¸é—œçš„æ¯”ä¾‹
    - Recall@k: æ‰¾åˆ°çš„ç›¸é—œæ–‡ä»¶ä½”æ‰€æœ‰ç›¸é—œæ–‡ä»¶çš„æ¯”ä¾‹
    - MRR: Mean Reciprocal Rankï¼ˆç¬¬ä¸€å€‹ç›¸é—œçµæœçš„æ’åå€’æ•¸ï¼‰
    """

    total_precision = 0
    total_recall = 0
    total_mrr = 0

    for question, relevant_docs in test_set:
        retrieved = rag_system.retrieve(question, k=5)

        # è¨ˆç®— Precision
        relevant_retrieved = 0
        first_relevant_rank = None

        for i, doc in enumerate(retrieved):
            if doc.metadata['source'] in relevant_docs:
                relevant_retrieved += 1
                if first_relevant_rank is None:
                    first_relevant_rank = i + 1

        precision = relevant_retrieved / len(retrieved)
        recall = relevant_retrieved / len(relevant_docs)
        mrr = 1 / first_relevant_rank if first_relevant_rank else 0

        total_precision += precision
        total_recall += recall
        total_mrr += mrr

    n = len(test_set)
    print(f"å¹³å‡ Precision@5: {total_precision/n:.2f}")
    print(f"å¹³å‡ Recall@5: {total_recall/n:.2f}")
    print(f"å¹³å‡ MRR: {total_mrr/n:.2f}")
```

### 2. ç”Ÿæˆè©•ä¼°

```python
def evaluate_generation_quality(rag_system, test_questions):
    """
    è©•ä¼°ç”Ÿæˆå“è³ª

    è©•ä¼°é¢å‘ï¼š
    1. ç›¸é—œæ€§ï¼šå›ç­”æ˜¯å¦ç›¸é—œ
    2. æº–ç¢ºæ€§ï¼šè³‡è¨Šæ˜¯å¦æ­£ç¢º
    3. å®Œæ•´æ€§ï¼šå›ç­”æ˜¯å¦å®Œæ•´
    4. æµæš¢æ€§ï¼šèªè¨€æ˜¯å¦é€šé †
    """

    for question in test_questions:
        answer = rag_system.query(question)

        print(f"\nå•é¡Œ: {question}")
        print(f"å›ç­”: {answer}")
        print("\nè«‹è©•åˆ† (1-5):")
        print("- ç›¸é—œæ€§: ___")
        print("- æº–ç¢ºæ€§: ___")
        print("- å®Œæ•´æ€§: ___")
        print("- æµæš¢æ€§: ___")
```

---

## ğŸ¯ èª²å ‚ç·´ç¿’é¡Œ

### åˆç´šç·´ç¿’

1. **ä¿®æ”¹ Chunk Size**
   - å°‡ chunk_size æ”¹ç‚º 300
   - è§€å¯Ÿå°æª¢ç´¢çµæœçš„å½±éŸ¿
   - è¨˜éŒ„è™•ç†æ™‚é–“çš„è®ŠåŒ–

2. **æ–°å¢ PDF æ–‡ä»¶**
   - åŠ å…¥ä¸€ä»½æ–°çš„ PDF åˆ° data è³‡æ–™å¤¾
   - é‡æ–°åŸ·è¡Œç³»çµ±
   - æ¸¬è©¦æ–°æ–‡ä»¶çš„å…§å®¹æ˜¯å¦èƒ½è¢«æª¢ç´¢

3. **èª¿æ•´æª¢ç´¢æ•¸é‡**
   - ä¿®æ”¹ k å€¼å¾ 5 æ”¹ç‚º 3
   - æ¯”è¼ƒç­”æ¡ˆå“è³ªçš„å·®ç•°

### ä¸­ç´šç·´ç¿’

1. **å¯¦ä½œä¸­æ–‡æ”¯æ´**
```python
# æç¤ºï¼šæ›´æ›åµŒå…¥æ¨¡å‹
embedding_model = EmbeddingModel(
    model_name="BAAI/bge-large-zh-v1.5"  # ä¸­æ–‡æ¨¡å‹
)
```

2. **åŠ å…¥æª¢ç´¢çµæœé¡¯ç¤º**
```python
def query_with_sources(self, question: str):
    """é¡¯ç¤ºç­”æ¡ˆå’Œä¾†æº"""
    relevant_docs = self.retrieve(question)
    answer = self.generate_answer(question, relevant_docs)

    print(f"ç­”æ¡ˆ: {answer}")
    print("\nåƒè€ƒä¾†æº:")
    for doc in relevant_docs:
        print(f"- {doc.metadata['source']}, ç¬¬ {doc.metadata['chunk_id']} å¡Š")
```

3. **å¯¦ä½œç°¡å–®çš„å¿«å–æ©Ÿåˆ¶**
```python
class CachedRAGSystem(RAGSystem):
    def __init__(self):
        super().__init__()
        self.cache = {}  # å•é¡Œ->ç­”æ¡ˆçš„å¿«å–

    def query(self, question: str):
        # æª¢æŸ¥å¿«å–
        if question in self.cache:
            print("(ä½¿ç”¨å¿«å–çµæœ)")
            return self.cache[question]

        # æ­£å¸¸æŸ¥è©¢
        answer = super().query(question)
        self.cache[question] = answer
        return answer
```

### é€²éšç·´ç¿’

1. **å¯¦ä½œæ··åˆæª¢ç´¢**
```python
def hybrid_search(self, query: str):
    """
    çµåˆå‘é‡æœå°‹å’Œé—œéµå­—æœå°‹
    """
    # å‘é‡æœå°‹
    vector_results = self.vector_search(query)

    # é—œéµå­—æœå°‹ (BM25)
    keyword_results = self.keyword_search(query)

    # åˆä½µçµæœ
    combined = self.merge_results(vector_results, keyword_results)
    return combined
```

2. **åŠ å…¥é‡æ’åºæ©Ÿåˆ¶**
```python
def rerank_results(self, query: str, documents: List[Document]):
    """
    ä½¿ç”¨äº¤å‰ç·¨ç¢¼å™¨é‡æ–°æ’åº
    """
    from sentence_transformers import CrossEncoder

    model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

    # è¨ˆç®—æ¯å€‹æ–‡ä»¶èˆ‡å•é¡Œçš„ç›¸é—œæ€§åˆ†æ•¸
    pairs = [[query, doc.content] for doc in documents]
    scores = model.predict(pairs)

    # æŒ‰åˆ†æ•¸é‡æ–°æ’åº
    sorted_docs = [doc for _, doc in sorted(
        zip(scores, documents),
        key=lambda x: x[0],
        reverse=True
    )]

    return sorted_docs
```

---

## ğŸ’¡ å»¶ä¼¸å­¸ç¿’è³‡æº

### è«–æ–‡èˆ‡æ–‡ç»
1. **RAG åŸå§‹è«–æ–‡**
   - "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" (2020)
   - äº†è§£ RAG çš„ç†è«–åŸºç¤

2. **FAISS è«–æ–‡**
   - "Billion-scale similarity search with GPUs" (2017)
   - äº†è§£å‘é‡æœå°‹çš„åŸç†

3. **Sentence-BERT**
   - "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks" (2019)
   - äº†è§£æ–‡å­—åµŒå…¥çš„åŸç†

### ç·šä¸Šè³‡æº
1. **Hugging Face èª²ç¨‹**
   - https://huggingface.co/course
   - å…è²»çš„ NLP å’Œ Transformers èª²ç¨‹

2. **LangChain æ–‡æª”**
   - https://python.langchain.com/
   - æ›´é€²éšçš„ RAG å¯¦ä½œæ¡†æ¶

3. **OpenAI Cookbook**
   - https://cookbook.openai.com/
   - åŒ…å«è¨±å¤š RAG ç›¸é—œç¯„ä¾‹

### å¯¦ä½œå°ˆæ¡ˆå»ºè­°

1. **å€‹äººçŸ¥è­˜åº«åŠ©æ‰‹**
   - å°‡å€‹äººç­†è¨˜ã€æ–‡ç« å»ºç«‹ RAG ç³»çµ±
   - å¯¦ä½œæœå°‹å’Œå•ç­”åŠŸèƒ½

2. **æŠ€è¡“æ–‡æª”å•ç­”ç³»çµ±**
   - æ”¶é›†ç‰¹å®šæŠ€è¡“çš„æ–‡æª”
   - å»ºç«‹å°ˆé–€çš„æŠ€è¡“å•ç­”åŠ©æ‰‹

3. **å¤šèªè¨€ RAG ç³»çµ±**
   - æ”¯æ´ä¸­è‹±æ–‡æ··åˆæª¢ç´¢
   - å¯¦ä½œè·¨èªè¨€å•ç­”

---

## ğŸ“ èª²å¾Œä½œæ¥­

### ä½œæ¥­ 1ï¼šåŸºç¤å¯¦ä½œï¼ˆå¿…åšï¼‰
å»ºç«‹ä¸€å€‹èƒ½è™•ç†è‡³å°‘ 3 å€‹ PDF æ–‡ä»¶çš„ RAG ç³»çµ±ï¼Œä¸¦èƒ½å›ç­”ç›¸é—œå•é¡Œã€‚

**è©•åˆ†æ¨™æº–ï¼š**
- æˆåŠŸè¼‰å…¥ PDFï¼ˆ20%ï¼‰
- æ­£ç¢ºåˆ‡å¡Šå’Œå‘é‡åŒ–ï¼ˆ30%ï¼‰
- èƒ½æª¢ç´¢ç›¸é—œå…§å®¹ï¼ˆ30%ï¼‰
- ç”Ÿæˆåˆç†ç­”æ¡ˆï¼ˆ20%ï¼‰

### ä½œæ¥­ 2ï¼šåŠŸèƒ½å¢å¼·ï¼ˆé¸åšï¼‰
å¾ä»¥ä¸‹é¸é …é¸æ“‡ä¸€å€‹å¯¦ä½œï¼š

1. **åŠ å…¥ç¶²é ä»‹é¢**
   - ä½¿ç”¨ Gradio æˆ– Streamlit
   - æä¾›ä¸Šå‚³ PDF åŠŸèƒ½
   - é¡¯ç¤ºæª¢ç´¢ä¾†æº

2. **æ”¯æ´å¤šç¨®æª”æ¡ˆæ ¼å¼**
   - æ”¯æ´ Wordã€TXT æª”æ¡ˆ
   - çµ±ä¸€è™•ç†æµç¨‹

3. **å¯¦ä½œå°è©±è¨˜æ†¶**
   - è¨˜ä½ä¹‹å‰çš„å•ç­”
   - æ”¯æ´è¿½å•åŠŸèƒ½

### ä½œæ¥­ 3ï¼šæ•ˆèƒ½å„ªåŒ–ï¼ˆé€²éšï¼‰
å„ªåŒ–ç³»çµ±æ•ˆèƒ½ï¼Œé”åˆ°ä»¥ä¸‹ç›®æ¨™ï¼š

- è™•ç† 100 é  PDF åœ¨ 1 åˆ†é˜å…§
- æª¢ç´¢å»¶é² < 100ms
- è¨˜æ†¶é«”ä½¿ç”¨ < 4GB

**æäº¤å…§å®¹ï¼š**
1. ç¨‹å¼ç¢¼ï¼ˆå«è¨»è§£ï¼‰
2. æ¸¬è©¦çµæœæˆªåœ–
3. å¿ƒå¾—å ±å‘Šï¼ˆ500å­—ï¼‰

---

## ğŸ¤ ç¸½çµ

æ­å–œä½ å®Œæˆ RAG ç³»çµ±çš„å­¸ç¿’ï¼

### ä½ å·²ç¶“å­¸æœƒäº†ï¼š
âœ… RAG çš„æ ¸å¿ƒæ¦‚å¿µå’ŒåŸç†
âœ… å¦‚ä½•è™•ç†å’Œåˆ‡å¡Šæ–‡ä»¶
âœ… æ–‡å­—åµŒå…¥å’Œå‘é‡æœå°‹
âœ… æ•´åˆ AI æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
âœ… å„ªåŒ–å’Œè©•ä¼°ç³»çµ±æ•ˆèƒ½

### ä¸‹ä¸€æ­¥å»ºè­°ï¼š
1. å˜—è©¦ä¸åŒçš„æ¨¡å‹çµ„åˆ
2. è™•ç†æ›´å¤§è¦æ¨¡çš„æ–‡ä»¶
3. åŠ å…¥æ›´å¤šé€²éšåŠŸèƒ½
4. éƒ¨ç½²åˆ°å¯¦éš›æ‡‰ç”¨

è¨˜ä½ï¼šRAG ç³»çµ±æ˜¯é€£æ¥ AI èˆ‡ç§æœ‰è³‡æ–™çš„æ©‹æ¨‘ï¼ŒæŒæ¡å®ƒå°‡è®“ä½ èƒ½å»ºç«‹çœŸæ­£æœ‰ç”¨çš„ AI æ‡‰ç”¨ï¼

---

**ç¥å­¸ç¿’æ„‰å¿«ï¼** ğŸ‰

å¦‚æœ‰å•é¡Œï¼Œæ­¡è¿éš¨æ™‚æå•è¨è«–ã€‚