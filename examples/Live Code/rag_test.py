import re
from typing import Any, Dict, List, Tuple, Optional
import pickle
import os

import numpy as np
import faiss
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

class Document:
    """æ–‡ä»¶é¡åˆ¥ï¼Œç”¨æ–¼å„²å­˜æ–‡å­—å…§å®¹å’Œç›¸é—œè³‡è¨Š"""

    def __init__(self, content: str, metadata: Dict[str, Any] = None):
        self.content = content
        self.metadata = metadata or {}


class PDFProcessor:
    """è™•ç† PDF æª”æ¡ˆçš„é¡åˆ¥"""

    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        """
        åˆå§‹åŒ–è¨­å®š

        åƒæ•¸è§£é‡‹ï¼š
        - chunk_size: æ¯å€‹æ–‡å­—å¡Šçš„å¤§å°ï¼ˆå­—æ•¸ï¼‰
          æƒ³åƒæˆï¼šæ¯å¼µç­†è¨˜å¡ç‰‡å¯ä»¥å¯« 500 å€‹å­—

        - chunk_overlap: ç›¸é„°å¡Šçš„é‡ç–Šå­—æ•¸
          æƒ³åƒæˆï¼šç‚ºäº†ä¿æŒé€£è²«ï¼Œä¸‹ä¸€å¼µå¡ç‰‡æœƒé‡è¤‡å‰ä¸€å¼µçš„æœ€å¾Œ 50 å€‹å­—
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def load_pdf(self, pdf_path: str) -> str:
        """
        è®€å– PDF æª”æ¡ˆ

        æµç¨‹ï¼š
        1. é–‹å•Ÿ PDF æª”æ¡ˆ
        2. é€é è®€å–æ–‡å­—
        3. åˆä½µæ‰€æœ‰é é¢çš„æ–‡å­—
        """
        text = ""
        with open(pdf_path, "rb") as file:
            pdf_reader = PdfReader(file)

            # é€é è™•ç†
            for page_num, page in enumerate(pdf_reader.pages):
                page_text = page.extract_text()
                if page_text:
                    # åŠ å…¥é ç¢¼æ¨™è¨˜ï¼Œæ–¹ä¾¿è¿½è¹¤ä¾†æº
                    text += f"\n[Page {page_num + 1}]\n{page_text}"

        return text

    def chunk_text(self, text: str, source: str) -> List[Document]:
        """
        å°‡é•·æ–‡å­—åˆ‡æˆå°å¡Š

        æ­¥é©Ÿè©³è§£ï¼š
        1. æ¸…ç†æ–‡å­—ï¼ˆç§»é™¤å¤šé¤˜ç©ºç™½ï¼‰
        2. æŒ‰ç…§å­—æ•¸åˆ‡å‰²
        3. ä¿ç•™é‡ç–Šéƒ¨åˆ†
        4. è¨˜éŒ„æ¯å¡Šçš„ä¾†æºè³‡è¨Š
        """
        # æ­¥é©Ÿ1ï¼šæ¸…ç†æ–‡å­—
        text = re.sub(r"\s+", " ", text)  # å¤šå€‹ç©ºç™½è®Šä¸€å€‹
        text = text.strip()  # ç§»é™¤é ­å°¾ç©ºç™½

        # æ­¥é©Ÿ2ï¼šåˆ†å‰²æˆå­—è©
        words = text.split()

        chunks = []
        # æ­¥é©Ÿ3ï¼šå»ºç«‹æ–‡å­—å¡Š
        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):
            # å–å‡º chunk_size å€‹å­—
            chunk_words = words[i : i + self.chunk_size]
            chunk_text = " ".join(chunk_words)

            # åªä¿ç•™æœ‰æ„ç¾©çš„å¡Šï¼ˆè‡³å°‘ 50 å€‹å­—å…ƒï¼‰
            if len(chunk_text) > 50:
                doc = Document(
                    content=chunk_text,
                    metadata={
                        "source": source,  # ä¾†æºæª”æ¡ˆ
                        "chunk_id": len(chunks),  # ç¬¬å¹¾å¡Š
                        "start_index": i,  # åœ¨åŸæ–‡çš„ä½ç½®
                    },
                )
                chunks.append(doc)

        return chunks


class EmbeddingModel:
    """å°‡æ–‡å­—è½‰æ›æˆå‘é‡çš„é¡åˆ¥"""

    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """
        è¼‰å…¥åµŒå…¥æ¨¡å‹

        all-MiniLM-L6-v2 æ¨¡å‹ä»‹ç´¹ï¼š
        - sentence-transformers: å°ˆé–€è™•ç†å¥å­åµŒå…¥çš„æ¡†æ¶
        - MiniLM: è¼•é‡åŒ–çš„èªè¨€æ¨¡å‹ï¼ˆMicrosoft é–‹ç™¼ï¼‰
        - L6: 6å±¤ Transformerï¼ˆè¼ƒå°‘å±¤æ•¸ï¼Œæ›´å¿«é€Ÿï¼‰
        - v2: ç¬¬äºŒç‰ˆï¼Œæ”¹é€²çš„ç‰ˆæœ¬

        å„ªé»ï¼š
        - æª”æ¡ˆå°ï¼ˆåªæœ‰ 22MBï¼‰
        - é€Ÿåº¦å¿«ï¼ˆæ¯” BGE-large å¿« 5 å€ï¼‰
        - ä¸éœ€è¦ GPUï¼ŒCPU å°±èƒ½æµæš¢é‹è¡Œ
        - æº–ç¢ºåº¦ä»ç„¶å¾ˆå¥½

        è¼¸å‡ºç¶­åº¦ï¼š384 ç¶­ï¼ˆ384å€‹æ•¸å­—è¡¨ç¤ºä¸€æ®µæ–‡å­—ï¼‰
        """
        print(f"è¼‰å…¥åµŒå…¥æ¨¡å‹: {model_name}")
        # å¼·åˆ¶ä½¿ç”¨ CPUï¼Œé¿å… CUDA ç›¸å®¹æ€§å•é¡Œ
        self.model = SentenceTransformer(model_name, device='cpu')
        self.dimension = 384  # all-MiniLM-L6-v2 çš„å‘é‡ç¶­åº¦

    def encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """
        å°‡æ–‡å­—åˆ—è¡¨è½‰æ›æˆå‘é‡

        åƒæ•¸èªªæ˜ï¼š
        - texts: è¦è½‰æ›çš„æ–‡å­—åˆ—è¡¨
        - batch_size: æ‰¹æ¬¡è™•ç†å¤§å°ï¼ˆä¸€æ¬¡è™•ç†å¹¾å€‹ï¼‰

        è™•ç†æµç¨‹ï¼š
        1. å°‡æ–‡å­—åˆ†æ‰¹ï¼ˆé¿å…è¨˜æ†¶é«”ä¸è¶³ï¼‰
        2. æ¯æ‰¹è½‰æ›æˆå‘é‡
        3. æ­£è¦åŒ–å‘é‡ï¼ˆè®“é•·åº¦ç‚º1ï¼Œæ–¹ä¾¿è¨ˆç®—ç›¸ä¼¼åº¦ï¼‰
        """
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=True,  # é¡¯ç¤ºé€²åº¦æ¢
            convert_to_numpy=True,  # è½‰æˆ NumPy é™£åˆ—
            normalize_embeddings=True,  # æ­£è¦åŒ–ï¼ˆé‡è¦ï¼ï¼‰
        )
        return embeddings


from pathlib import Path


class FAISSVectorStore:
    """FAISS å‘é‡è³‡æ–™åº«é¡åˆ¥"""

    def __init__(self, dimension: int = 384):
        """
        åˆå§‹åŒ– FAISS ç´¢å¼•

        åƒæ•¸ï¼š
        - dimension: å‘é‡ç¶­åº¦ï¼ˆé è¨­ç‚º all-MiniLM-L6-v2 çš„ 384 ç¶­ï¼‰
        """
        self.dimension = dimension
        # ä½¿ç”¨ L2 è·é›¢çš„ç´¢å¼•ï¼ˆä¹Ÿå¯ä»¥æ”¹ç”¨å…§ç©ï¼šfaiss.IndexFlatIPï¼‰
        self.index = faiss.IndexFlatL2(dimension)
        self.documents = []  # å„²å­˜å°æ‡‰çš„æ–‡ä»¶
        self.id_to_doc = {}  # ID å°æ‡‰åˆ°æ–‡ä»¶çš„æ˜ å°„

    def add(self, embeddings: np.ndarray, documents: List[Document]):
        """
        æ·»åŠ å‘é‡å’Œå°æ‡‰çš„æ–‡ä»¶åˆ°ç´¢å¼•

        åƒæ•¸ï¼š
        - embeddings: å‘é‡é™£åˆ— (n_samples, dimension)
        - documents: å°æ‡‰çš„æ–‡ä»¶åˆ—è¡¨
        """
        # ç¢ºä¿ embeddings æ˜¯ float32 é¡å‹ï¼ˆFAISS è¦æ±‚ï¼‰
        if embeddings.dtype != np.float32:
            embeddings = embeddings.astype(np.float32)

        # ç²å–ç•¶å‰ç´¢å¼•å¤§å°
        start_id = self.index.ntotal

        # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
        self.index.add(embeddings)

        # å„²å­˜æ–‡ä»¶ä¸¦å»ºç«‹ ID æ˜ å°„
        for i, doc in enumerate(documents):
            doc_id = start_id + i
            self.documents.append(doc)
            self.id_to_doc[doc_id] = doc

        print(f"å·²æ·»åŠ  {len(documents)} å€‹å‘é‡ï¼Œç´¢å¼•ç¸½æ•¸: {self.index.ntotal}")

    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[Document, float]]:
        """
        æœå°‹æœ€ç›¸ä¼¼çš„å‘é‡

        åƒæ•¸ï¼š
        - query_embedding: æŸ¥è©¢å‘é‡
        - k: è¿”å›çš„çµæœæ•¸é‡

        è¿”å›ï¼š
        - æ–‡ä»¶å’Œè·é›¢çš„åˆ—è¡¨
        """
        # ç¢ºä¿æŸ¥è©¢å‘é‡æ˜¯æ­£ç¢ºçš„å½¢ç‹€å’Œé¡å‹
        if query_embedding.ndim == 1:
            query_embedding = query_embedding.reshape(1, -1)
        if query_embedding.dtype != np.float32:
            query_embedding = query_embedding.astype(np.float32)

        # æœå°‹
        distances, indices = self.index.search(query_embedding, k)

        # æ•´ç†çµæœ
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx < len(self.documents):
                results.append((self.documents[idx], float(dist)))

        return results

    def save(self, index_path: str, metadata_path: str):
        """
        å„²å­˜ FAISS ç´¢å¼•å’Œå…ƒè³‡æ–™

        åƒæ•¸ï¼š
        - index_path: FAISS ç´¢å¼•æª”æ¡ˆè·¯å¾‘
        - metadata_path: å…ƒè³‡æ–™æª”æ¡ˆè·¯å¾‘
        """
        # å„²å­˜ FAISS ç´¢å¼•
        faiss.write_index(self.index, index_path)
        print(f"FAISS ç´¢å¼•å·²å„²å­˜åˆ°: {index_path}")

        # å„²å­˜æ–‡ä»¶å…ƒè³‡æ–™
        metadata = {
            "documents": self.documents,
            "id_to_doc": self.id_to_doc,
            "dimension": self.dimension,
        }
        with open(metadata_path, "wb") as f:
            pickle.dump(metadata, f)
        print(f"å…ƒè³‡æ–™å·²å„²å­˜åˆ°: {metadata_path}")

    def load(self, index_path: str, metadata_path: str):
        """
        è¼‰å…¥ FAISS ç´¢å¼•å’Œå…ƒè³‡æ–™

        åƒæ•¸ï¼š
        - index_path: FAISS ç´¢å¼•æª”æ¡ˆè·¯å¾‘
        - metadata_path: å…ƒè³‡æ–™æª”æ¡ˆè·¯å¾‘
        """
        # è¼‰å…¥ FAISS ç´¢å¼•
        self.index = faiss.read_index(index_path)
        print(f"FAISS ç´¢å¼•å·²è¼‰å…¥ï¼Œå…± {self.index.ntotal} å€‹å‘é‡")

        # è¼‰å…¥æ–‡ä»¶å…ƒè³‡æ–™
        with open(metadata_path, "rb") as f:
            metadata = pickle.load(f)
        self.documents = metadata["documents"]
        self.id_to_doc = metadata["id_to_doc"]
        self.dimension = metadata["dimension"]
        print(f"å…ƒè³‡æ–™å·²è¼‰å…¥ï¼Œå…± {len(self.documents)} å€‹æ–‡ä»¶")


def process_all_pdfs(data_folder: str = "data"):
    """
    è™•ç†è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰ PDF æª”æ¡ˆä¸¦ç”Ÿæˆ embeddings

    åƒæ•¸ï¼š
    - data_folder: PDF æª”æ¡ˆæ‰€åœ¨çš„è³‡æ–™å¤¾

    è¿”å›ï¼š
    - all_results: åŒ…å«æ‰€æœ‰æª”æ¡ˆçš„ chunks å’Œ embeddings çš„å­—å…¸
    """
    # åˆå§‹åŒ–
    pdf_processor = PDFProcessor()
    embedding_model = EmbeddingModel()
    all_results = {}

    # ç²å–æ‰€æœ‰ PDF æª”æ¡ˆ
    pdf_files = list(Path(data_folder).glob("*.pdf"))
    print(f"æ‰¾åˆ° {len(pdf_files)} å€‹ PDF æª”æ¡ˆ")

    # è™•ç†æ¯å€‹ PDF
    for pdf_path in pdf_files:
        print(f"\nè™•ç†æª”æ¡ˆ: {pdf_path.name}")

        try:
            # 1. è®€å– PDF
            text = pdf_processor.load_pdf(str(pdf_path))
            print(f"  - å·²è®€å–æ–‡å­—ï¼Œé•·åº¦: {len(text)} å­—å…ƒ")

            # 2. åˆ‡å‰²æ–‡å­—
            chunks = pdf_processor.chunk_text(text, str(pdf_path))
            print(f"  - åˆ‡å‰²æˆ {len(chunks)} å€‹æ–‡å­—å¡Š")

            # 3. ç”Ÿæˆå‘é‡
            if chunks:
                texts = [chunk.content for chunk in chunks]
                embeddings = embedding_model.encode(texts)
                print(f"  - å·²ç”Ÿæˆ {len(embeddings)} å€‹å‘é‡ï¼Œç¶­åº¦: {embeddings.shape}")

                # å„²å­˜çµæœ
                all_results[pdf_path.name] = {
                    "chunks": chunks,
                    "embeddings": embeddings
                }

        except Exception as e:
            print(f"  - éŒ¯èª¤: {e}")
            continue

    # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
    print(f"\n=== è™•ç†å®Œæˆ ===")
    print(f"ç¸½å…±è™•ç†: {len(pdf_files)} å€‹ PDF æª”æ¡ˆ")

    total_chunks = sum(len(result["chunks"]) for result in all_results.values())
    print(f"ç¸½æ–‡ä»¶å¡Š: {total_chunks} å€‹")

    return all_results


def build_faiss_index(results: Dict[str, Any], save_path: str = None):
    """
    å¾è™•ç†çµæœå»ºç«‹ FAISS ç´¢å¼•

    åƒæ•¸ï¼š
    - results: process_all_pdfs çš„è¿”å›çµæœ
    - save_path: å„²å­˜è·¯å¾‘çš„å‰ç¶´ï¼ˆå¯é¸ï¼‰

    è¿”å›ï¼š
    - vector_store: FAISS å‘é‡å„²å­˜ç‰©ä»¶
    """
    # åˆå§‹åŒ– FAISS å‘é‡å„²å­˜
    vector_store = FAISSVectorStore(dimension=384)  # all-MiniLM-L6-v2 çš„ç¶­åº¦

    # å°‡æ‰€æœ‰çµæœæ·»åŠ åˆ° FAISS
    for filename, data in results.items():
        print(f"\næ·»åŠ  {filename} åˆ° FAISS ç´¢å¼•...")
        vector_store.add(data["embeddings"], data["chunks"])

    # å„²å­˜ç´¢å¼•ï¼ˆå¦‚æœæä¾›äº†è·¯å¾‘ï¼‰
    if save_path:
        vector_store.save(f"{save_path}.index", f"{save_path}.metadata")

    return vector_store


def demo_search(vector_store: FAISSVectorStore, query: str, embedding_model: EmbeddingModel):
    """
    ç¤ºç¯„æœå°‹åŠŸèƒ½

    åƒæ•¸ï¼š
    - vector_store: FAISS å‘é‡å„²å­˜
    - query: æŸ¥è©¢æ–‡å­—
    - embedding_model: åµŒå…¥æ¨¡å‹
    """
    print(f"\næŸ¥è©¢: {query}")

    # ç”ŸæˆæŸ¥è©¢å‘é‡
    query_embedding = embedding_model.encode([query])

    # æœå°‹
    results = vector_store.search(query_embedding[0], k=3)

    # é¡¯ç¤ºçµæœ
    print("\næœå°‹çµæœ:")
    for i, (doc, distance) in enumerate(results, 1):
        print(f"\n--- çµæœ {i} (è·é›¢: {distance:.4f}) ---")
        print(f"ä¾†æº: {doc.metadata['source']}")
        print(f"å¡Š ID: {doc.metadata['chunk_id']}")
        print(f"å…§å®¹é è¦½: {doc.content[:200]}...")


class RAGRetriever:
    """RAG æª¢ç´¢å™¨é¡åˆ¥ï¼Œæ•´åˆæª¢ç´¢å’Œç”ŸæˆåŠŸèƒ½"""

    def __init__(self, vector_store: FAISSVectorStore, embedding_model: EmbeddingModel):
        """
        åˆå§‹åŒ– RAG æª¢ç´¢å™¨

        åƒæ•¸ï¼š
        - vector_store: FAISS å‘é‡å„²å­˜
        - embedding_model: åµŒå…¥æ¨¡å‹
        """
        self.vector_store = vector_store
        self.embedding_model = embedding_model

    def retrieve(self, query: str, k: int = 5, rerank: bool = True) -> List[Document]:
        """
        æª¢ç´¢ç›¸é—œæ–‡ä»¶

        åƒæ•¸ï¼š
        - query: æŸ¥è©¢æ–‡å­—
        - k: è¦æª¢ç´¢çš„æ–‡ä»¶æ•¸é‡
        - rerank: æ˜¯å¦é€²è¡Œé‡æ–°æ’åº

        è¿”å›ï¼š
        - ç›¸é—œæ–‡ä»¶åˆ—è¡¨
        """
        # ç”ŸæˆæŸ¥è©¢å‘é‡
        print(f"æ­£åœ¨ç‚ºæŸ¥è©¢ç”Ÿæˆå‘é‡: '{query}'")
        query_embedding = self.embedding_model.encode([query])[0]

        # å¾å‘é‡è³‡æ–™åº«æœå°‹
        results = self.vector_store.search(query_embedding, k=k*2 if rerank else k)

        # é‡æ–°æ’åºï¼ˆå¯é¸ï¼‰
        if rerank:
            results = self._rerank_results(query, results, top_k=k)
        else:
            results = results[:k]

        # åªè¿”å›æ–‡ä»¶
        documents = [doc for doc, _ in results]
        return documents

    def _rerank_results(self, query: str, results: List[Tuple[Document, float]],
                        top_k: int = 5) -> List[Tuple[Document, float]]:
        """
        é‡æ–°æ’åºæª¢ç´¢çµæœ

        ä½¿ç”¨æ›´ç²¾ç´°çš„ç›¸ä¼¼åº¦è¨ˆç®—é€²è¡Œé‡æ–°æ’åº
        """
        # é€™è£¡å¯ä»¥å¯¦ä½œæ›´è¤‡é›œçš„é‡æ–°æ’åºé‚è¼¯
        # ä¾‹å¦‚ï¼šè€ƒæ…®é—œéµå­—åŒ¹é…ã€æ–‡ä»¶é•·åº¦ç­‰

        reranked = []
        query_lower = query.lower()
        query_words = set(query_lower.split())

        for doc, distance in results:
            # è¨ˆç®—é¡å¤–çš„ç›¸é—œæ€§åˆ†æ•¸
            content_lower = doc.content.lower()
            content_words = set(content_lower.split())

            # é—œéµå­—é‡ç–Šåˆ†æ•¸
            word_overlap = len(query_words & content_words) / len(query_words) if query_words else 0

            # ç²¾ç¢ºåŒ¹é…åŠ åˆ†
            exact_match_bonus = 1.0 if query_lower in content_lower else 0.0

            # ç¶œåˆåˆ†æ•¸ï¼ˆè·é›¢è¶Šå°è¶Šå¥½ï¼Œæ‰€ä»¥ç”¨è² è™Ÿï¼‰
            combined_score = -distance + word_overlap * 0.5 + exact_match_bonus * 0.3

            reranked.append((doc, distance, combined_score))

        # æŒ‰ç¶œåˆåˆ†æ•¸æ’åº
        reranked.sort(key=lambda x: x[2], reverse=True)

        # è¿”å›å‰ k å€‹çµæœ
        return [(doc, dist) for doc, dist, _ in reranked[:top_k]]

    def retrieve_with_context(self, query: str, k: int = 5,
                            context_window: int = 1) -> List[Document]:
        """
        æª¢ç´¢æ–‡ä»¶ä¸¦åŒ…å«ä¸Šä¸‹æ–‡

        åƒæ•¸ï¼š
        - query: æŸ¥è©¢æ–‡å­—
        - k: è¦æª¢ç´¢çš„æ–‡ä»¶æ•¸é‡
        - context_window: ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆå‰å¾Œå„å–å¹¾å€‹å¡Šï¼‰

        è¿”å›ï¼š
        - åŒ…å«ä¸Šä¸‹æ–‡çš„æ–‡ä»¶åˆ—è¡¨
        """
        # å…ˆé€²è¡ŒåŸºæœ¬æª¢ç´¢
        retrieved_docs = self.retrieve(query, k=k)

        # æ”¶é›†éœ€è¦çš„å¡Š ID
        expanded_docs = []
        seen_ids = set()

        for doc in retrieved_docs:
            chunk_id = doc.metadata.get('chunk_id', 0)
            source = doc.metadata.get('source', '')

            # æ·»åŠ ä¸Šä¸‹æ–‡å¡Š
            for offset in range(-context_window, context_window + 1):
                target_id = chunk_id + offset

                # é¿å…é‡è¤‡
                doc_key = (source, target_id)
                if doc_key in seen_ids or target_id < 0:
                    continue

                # å°‹æ‰¾å°æ‡‰çš„æ–‡ä»¶
                for candidate in self.vector_store.documents:
                    if (candidate.metadata.get('source') == source and
                        candidate.metadata.get('chunk_id') == target_id):
                        expanded_docs.append(candidate)
                        seen_ids.add(doc_key)
                        break

        # æŒ‰ä¾†æºå’Œå¡Š ID æ’åº
        expanded_docs.sort(key=lambda x: (x.metadata.get('source', ''),
                                         x.metadata.get('chunk_id', 0)))

        return expanded_docs


class RAGPipeline:
    """å®Œæ•´çš„ RAG ç®¡é“"""

    def __init__(self, retriever: RAGRetriever, openai_api_key: Optional[str] = None,
                 model_name: str = "gpt-4o"):
        """
        åˆå§‹åŒ– RAG ç®¡é“

        åƒæ•¸ï¼š
        - retriever: RAG æª¢ç´¢å™¨
        - openai_api_key: OpenAI API é‡‘é‘°ï¼ˆå¯é¸ï¼Œä¹Ÿå¯å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼‰
        - model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±ï¼ˆé è¨­ç‚º gpt-4oï¼‰
        """
        self.retriever = retriever
        self.model_name = model_name

        # åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯
        api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if api_key:
            self.openai_client = OpenAI(api_key=api_key)
            self.use_llm = True
            print(f"âœ… OpenAI API å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨æ¨¡å‹: {model_name}")
        else:
            self.openai_client = None
            self.use_llm = False
            print("âš ï¸ æœªæä¾› OpenAI API é‡‘é‘°ï¼Œå°‡ä½¿ç”¨ç°¡åŒ–ç‰ˆç­”æ¡ˆç”Ÿæˆ")

    def format_context(self, documents: List[Document]) -> str:
        """
        æ ¼å¼åŒ–æª¢ç´¢åˆ°çš„æ–‡ä»¶ä½œç‚ºä¸Šä¸‹æ–‡

        åƒæ•¸ï¼š
        - documents: æ–‡ä»¶åˆ—è¡¨

        è¿”å›ï¼š
        - æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ä¸²
        """
        context_parts = []
        current_source = None

        for doc in documents:
            source = Path(doc.metadata.get('source', 'Unknown')).name
            chunk_id = doc.metadata.get('chunk_id', 0)

            # å¦‚æœä¾†æºæ”¹è®Šï¼Œæ·»åŠ åˆ†éš”ç¬¦
            if source != current_source:
                if current_source is not None:
                    context_parts.append("\n" + "="*50 + "\n")
                context_parts.append(f"ğŸ“„ ä¾†æº: {source}\n")
                current_source = source

            # æ·»åŠ æ–‡ä»¶å…§å®¹
            context_parts.append(f"[å€å¡Š {chunk_id}]")
            context_parts.append(doc.content)
            context_parts.append("\n")

        return "\n".join(context_parts)

    def generate_answer(self, query: str, context: str, max_tokens: int = 1000,
                       temperature: float = 0.7) -> str:
        """
        åŸºæ–¼ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ

        åƒæ•¸ï¼š
        - query: ä½¿ç”¨è€…æŸ¥è©¢
        - context: æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        - max_tokens: æœ€å¤§ç”Ÿæˆ token æ•¸
        - temperature: ç”Ÿæˆæº«åº¦ï¼ˆ0-2ï¼Œè¶Šé«˜è¶Šæœ‰å‰µæ„ï¼‰

        è¿”å›ï¼š
        - ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        if self.use_llm and self.openai_client:
            try:
                # æ§‹å»ºç³»çµ±æç¤ºè©
                system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„çŸ¥è­˜åŠ©æ‰‹ã€‚è«‹åŸºæ–¼æä¾›çš„ä¸Šä¸‹æ–‡è³‡æ–™ï¼Œæº–ç¢ºä¸”è©³ç´°åœ°å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

é‡è¦è¦å‰‡ï¼š
1. åªä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡è³‡æ–™ä¾†å›ç­”å•é¡Œ
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜
3. å›ç­”è¦çµæ§‹æ¸…æ™°ã€é‚è¼¯åš´è¬¹
4. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
5. å¦‚æœè³‡è¨Šä¾†è‡ªå¤šå€‹ä¾†æºï¼Œè«‹æ•´åˆä¸¦æä¾›å®Œæ•´ç­”æ¡ˆ"""

                # æ§‹å»ºä½¿ç”¨è€…æç¤ºè©
                user_prompt = f"""åŸºæ–¼ä»¥ä¸‹ä¸Šä¸‹æ–‡è³‡æ–™ï¼Œè«‹å›ç­”å•é¡Œã€‚

ã€ä¸Šä¸‹æ–‡è³‡æ–™ã€‘
{context}

ã€å•é¡Œã€‘
{query}

è«‹æä¾›è©³ç´°ä¸”æº–ç¢ºçš„ç­”æ¡ˆï¼š"""

                # èª¿ç”¨ OpenAI API
                response = self.openai_client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=0.95,
                    frequency_penalty=0.2,
                    presence_penalty=0.2
                )

                # æå–ç­”æ¡ˆ
                answer = response.choices[0].message.content.strip()

                # æ·»åŠ æ¨¡å‹è³‡è¨Š
                answer += f"\n\n---\nğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model_name}"
                if hasattr(response.usage, 'total_tokens'):
                    answer += f" | ä½¿ç”¨ tokens: {response.usage.total_tokens}"

                return answer

            except Exception as e:
                print(f"âŒ OpenAI API èª¿ç”¨å¤±æ•—: {e}")
                return self._generate_fallback_answer(query, context)
        else:
            return self._generate_fallback_answer(query, context)

    def _generate_fallback_answer(self, query: str, context: str) -> str:
        """
        å‚™ç”¨ç­”æ¡ˆç”Ÿæˆï¼ˆç•¶ OpenAI API ä¸å¯ç”¨æ™‚ï¼‰

        åƒæ•¸ï¼š
        - query: ä½¿ç”¨è€…æŸ¥è©¢
        - context: æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡

        è¿”å›ï¼š
        - ç°¡åŒ–ç‰ˆç­”æ¡ˆ
        """
        answer = f"""
åŸºæ–¼æª¢ç´¢åˆ°çš„è³‡æ–™ï¼Œé—œæ–¼æ‚¨çš„å•é¡Œã€Œ{query}ã€ï¼š

ğŸ“š ç›¸é—œå…§å®¹æ‘˜è¦ï¼š
{context[:800]}...

âš ï¸ æ³¨æ„ï¼šé€™æ˜¯ç°¡åŒ–ç‰ˆç­”æ¡ˆã€‚è¦ç²å¾—æ›´å¥½çš„å›ç­”ï¼Œè«‹è¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸æˆ–åœ¨åˆå§‹åŒ–æ™‚æä¾› API é‡‘é‘°ã€‚
        """
        return answer

    def query(self, question: str, k: int = 5, use_context_window: bool = True,
             verbose: bool = True) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´çš„ RAG æŸ¥è©¢

        åƒæ•¸ï¼š
        - question: ä½¿ç”¨è€…å•é¡Œ
        - k: æª¢ç´¢çš„æ–‡ä»¶æ•¸é‡
        - use_context_window: æ˜¯å¦ä½¿ç”¨ä¸Šä¸‹æ–‡çª—å£
        - verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°è³‡è¨Š

        è¿”å›ï¼š
        - åŒ…å«ç­”æ¡ˆå’Œç›¸é—œè³‡è¨Šçš„å­—å…¸
        """
        if verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ” è™•ç†æŸ¥è©¢: {question}")
            print(f"{'='*60}")

        # æ­¥é©Ÿ 1: æª¢ç´¢ç›¸é—œæ–‡ä»¶
        if verbose:
            print("\nğŸ“¥ æ­¥é©Ÿ 1: æª¢ç´¢ç›¸é—œæ–‡ä»¶...")

        if use_context_window:
            documents = self.retriever.retrieve_with_context(question, k=k)
        else:
            documents = self.retriever.retrieve(question, k=k)

        if verbose:
            print(f"âœ… æª¢ç´¢åˆ° {len(documents)} å€‹ç›¸é—œæ–‡ä»¶å¡Š")

        # æ­¥é©Ÿ 2: æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        if verbose:
            print("\nğŸ“ æ­¥é©Ÿ 2: æ ¼å¼åŒ–ä¸Šä¸‹æ–‡...")

        context = self.format_context(documents)

        if verbose:
            print(f"âœ… ä¸Šä¸‹æ–‡é•·åº¦: {len(context)} å­—å…ƒ")

        # æ­¥é©Ÿ 3: ç”Ÿæˆç­”æ¡ˆ
        if verbose:
            print("\nğŸ¤– æ­¥é©Ÿ 3: ç”Ÿæˆç­”æ¡ˆ...")

        answer = self.generate_answer(question, context)

        if verbose:
            print("âœ… ç­”æ¡ˆå·²ç”Ÿæˆ")

        # æ•´ç†çµæœ
        result = {
            "question": question,
            "answer": answer,
            "context": context,
            "documents": documents,
            "num_documents": len(documents),
            "sources": list(set(Path(doc.metadata.get('source', 'Unknown')).name
                              for doc in documents))
        }

        return result

    def batch_query(self, questions: List[str], k: int = 5) -> List[Dict[str, Any]]:
        """
        æ‰¹æ¬¡è™•ç†å¤šå€‹æŸ¥è©¢

        åƒæ•¸ï¼š
        - questions: å•é¡Œåˆ—è¡¨
        - k: æ¯å€‹æŸ¥è©¢æª¢ç´¢çš„æ–‡ä»¶æ•¸é‡

        è¿”å›ï¼š
        - çµæœåˆ—è¡¨
        """
        results = []

        for i, question in enumerate(questions, 1):
            print(f"\nè™•ç†ç¬¬ {i}/{len(questions)} å€‹å•é¡Œ...")
            result = self.query(question, k=k, verbose=False)
            results.append(result)

        return results


def create_rag_system(index_path: str = "faiss_index", openai_api_key: Optional[str] = None):
    """
    å»ºç«‹ä¸¦è¼‰å…¥ RAG ç³»çµ±ï¼ˆç”¨æ–¼å·²æœ‰ç´¢å¼•çš„æƒ…æ³ï¼‰

    åƒæ•¸ï¼š
    - index_path: FAISS ç´¢å¼•æª”æ¡ˆè·¯å¾‘å‰ç¶´
    - openai_api_key: OpenAI API é‡‘é‘°

    è¿”å›ï¼š
    - rag_pipeline: é…ç½®å¥½çš„ RAG ç®¡é“
    """
    # è¼‰å…¥å‘é‡å„²å­˜
    vector_store = FAISSVectorStore()
    vector_store.load(f"{index_path}.index", f"{index_path}.metadata")

    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    embedding_model = EmbeddingModel()

    # å»ºç«‹æª¢ç´¢å™¨å’Œç®¡é“
    retriever = RAGRetriever(vector_store, embedding_model)
    rag_pipeline = RAGPipeline(retriever, openai_api_key=openai_api_key)

    return rag_pipeline


def interactive_rag_query(rag_pipeline: RAGPipeline):
    """
    äº’å‹•å¼ RAG æŸ¥è©¢ä»‹é¢

    åƒæ•¸ï¼š
    - rag_pipeline: RAG ç®¡é“
    """
    print("\n" + "="*80)
    print("ğŸ’¬ äº’å‹•å¼ RAG æŸ¥è©¢ç³»çµ±")
    print("="*80)
    print("è¼¸å…¥æ‚¨çš„å•é¡Œï¼ˆè¼¸å…¥ 'quit' æˆ– 'exit' çµæŸï¼‰")

    while True:
        print("\n" + "-"*60)
        query = input("â“ è«‹è¼¸å…¥å•é¡Œ: ").strip()

        if query.lower() in ['quit', 'exit', 'q']:
            print("ğŸ‘‹ æ„Ÿè¬ä½¿ç”¨ï¼Œå†è¦‹ï¼")
            break

        if not query:
            print("âš ï¸ è«‹è¼¸å…¥æœ‰æ•ˆçš„å•é¡Œ")
            continue

        # åŸ·è¡ŒæŸ¥è©¢
        result = rag_pipeline.query(query, k=5, verbose=False)

        # é¡¯ç¤ºç­”æ¡ˆ
        print("\nğŸ“ ç­”æ¡ˆï¼š")
        print(result['answer'])

        # é¡¯ç¤ºä¾†æº
        print(f"\nğŸ“š è³‡æ–™ä¾†æº: {', '.join(result['sources'])}")
        print(f"ğŸ“Š ä½¿ç”¨äº† {result['num_documents']} å€‹æ–‡ä»¶å¡Š")


def test_retrieval_system(vector_store: FAISSVectorStore, embedding_model: EmbeddingModel,
                         openai_api_key: Optional[str] = None):
    """
    æ¸¬è©¦å®Œæ•´çš„æª¢ç´¢ç³»çµ±

    åƒæ•¸ï¼š
    - vector_store: FAISS å‘é‡å„²å­˜
    - embedding_model: åµŒå…¥æ¨¡å‹
    - openai_api_key: OpenAI API é‡‘é‘°ï¼ˆå¯é¸ï¼‰
    """
    print("\n" + "="*80)
    print("ğŸš€ æ¸¬è©¦ RAG æª¢ç´¢ç³»çµ±")
    print("="*80)

    # åˆå§‹åŒ–æª¢ç´¢å™¨å’Œç®¡é“
    retriever = RAGRetriever(vector_store, embedding_model)
    rag_pipeline = RAGPipeline(retriever, openai_api_key=openai_api_key)

    # æ¸¬è©¦æŸ¥è©¢åˆ—è¡¨
    test_queries = [
        "multi agent debate",
        "system performance",
        "data processing",
    ]

    # æ¸¬è©¦å–®ä¸€æŸ¥è©¢ï¼ˆè©³ç´°æ¨¡å¼ï¼‰
    print("\nğŸ“Š å–®ä¸€æŸ¥è©¢æ¸¬è©¦ï¼ˆè©³ç´°æ¨¡å¼ï¼‰")
    result = rag_pipeline.query(test_queries[0], k=3, verbose=True)

    print("\n" + "="*60)
    print("ğŸ“‹ æŸ¥è©¢çµæœæ‘˜è¦ï¼š")
    print(f"å•é¡Œ: {result['question']}")
    print(f"æª¢ç´¢åˆ°çš„æ–‡ä»¶æ•¸: {result['num_documents']}")
    print(f"è³‡æ–™ä¾†æº: {', '.join(result['sources'])}")
    print(f"\nç­”æ¡ˆé è¦½:")
    print(result['answer'][:500] + "...")

    # æ¸¬è©¦æ‰¹æ¬¡æŸ¥è©¢
    print("\n" + "="*60)
    print("ğŸ“Š æ‰¹æ¬¡æŸ¥è©¢æ¸¬è©¦")
    print("="*60)

    batch_results = rag_pipeline.batch_query(test_queries[1:], k=2)

    for i, result in enumerate(batch_results, 1):
        print(f"\næŸ¥è©¢ {i}: {result['question']}")
        print(f"  - æª¢ç´¢æ–‡ä»¶æ•¸: {result['num_documents']}")
        print(f"  - è³‡æ–™ä¾†æº: {', '.join(result['sources'])}")

    print("\nâœ… æª¢ç´¢ç³»çµ±æ¸¬è©¦å®Œæˆï¼")


def simple_rag_query(query: str, vector_store: FAISSVectorStore,
                     embedding_model: EmbeddingModel):
    """
    ç°¡å–®çš„ RAG æŸ¥è©¢å‡½æ•¸

    åƒæ•¸ï¼š
    - query: ä½¿ç”¨è€…æŸ¥è©¢
    - vector_store: FAISS å‘é‡å„²å­˜
    - embedding_model: åµŒå…¥æ¨¡å‹
    - openai_api_key: OpenAI API é‡‘é‘°
    """
    print(f"\nğŸ” æŸ¥è©¢: {query}")
    print("="*60)

    # 1. ç”ŸæˆæŸ¥è©¢å‘é‡
    query_embedding = embedding_model.encode([query])[0]

    # 2. æª¢ç´¢ç›¸é—œæ–‡ä»¶
    results = vector_store.search(query_embedding, k=5)

    # 3. çµ„åˆä¸Šä¸‹æ–‡
    context = "\n\n".join([doc.content for doc, _ in results])

    print(f"âœ… æ‰¾åˆ° {len(results)} å€‹ç›¸é—œæ–‡ä»¶")

    # 4. ç”Ÿæˆç­”æ¡ˆ
    
    try:
        client = OpenAI()

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„çŸ¥è­˜åŠ©æ‰‹ã€‚è«‹åŸºæ–¼æä¾›çš„ä¸Šä¸‹æ–‡è³‡æ–™ï¼Œæº–ç¢ºåœ°å›ç­”å•é¡Œã€‚ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"},
                {"role": "user", "content": f"ä¸Šä¸‹æ–‡ï¼š\n{context}\n\nå•é¡Œï¼š{query}\n\nè«‹å›ç­”ï¼š"}
            ],
            max_tokens=800,
            temperature=0
        )

        answer = response.choices[0].message.content
        print(f"\nğŸ“ GPT-4o ç­”æ¡ˆï¼š\n{answer}")
        print(f"ğŸ“„ ç›¸é—œå…§å®¹ï¼š\n{context[:500]}...")

    except Exception as e:
        print(f"âŒ OpenAI API éŒ¯èª¤: {e}")
        print(f"\nğŸ“„ ç›¸é—œå…§å®¹ï¼š\n{context[:500]}...")
    


if __name__ == "__main__":
    # æª¢æŸ¥æ˜¯å¦å·²æœ‰ç´¢å¼•
    if Path("faiss_index.index").exists() and Path("faiss_index.metadata").exists():
        print("âœ… è¼‰å…¥ç¾æœ‰ç´¢å¼•...")
        vector_store = FAISSVectorStore()
        vector_store.load("faiss_index.index", "faiss_index.metadata")
        embedding_model = EmbeddingModel()
    else:
        print("ğŸ“š å»ºç«‹æ–°ç´¢å¼•...")
        # è™•ç† PDF
        results = process_all_pdfs()
        # å»ºç«‹ç´¢å¼•
        vector_store = build_faiss_index(results, save_path="faiss_index")
        embedding_model = EmbeddingModel()

    # åŸ·è¡ŒæŸ¥è©¢
    queries = [
        "multi agent debate is useful or not",
    ]

    for query in queries:
        simple_rag_query(query, vector_store, embedding_model)

    
