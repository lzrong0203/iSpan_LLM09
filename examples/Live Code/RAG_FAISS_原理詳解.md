# RAG 系統與 FAISS 向量搜尋原理詳解

## 一、為什麼需要 RAG 系統？

### 1.1 傳統 LLM 的限制

大型語言模型（LLM）面臨幾個關鍵問題：

1. **知識截止日期**：模型訓練後無法獲取新資訊
2. **幻覺問題**：可能生成看似合理但實際錯誤的內容
3. **無法存取私有資料**：無法讀取企業內部文件
4. **記憶體限制**：上下文窗口有限，無法處理大量文件

### 1.2 RAG 的解決方案

RAG（Retrieval-Augmented Generation）透過以下方式解決這些問題：

```
使用者查詢 → 檢索相關文件 → 結合查詢與文件 → LLM 生成答案
```

關鍵優勢：
- **實時資訊**：可以檢索最新的文件
- **準確性提升**：基於實際文件內容回答
- **可擴展性**：支援任意數量的文件
- **可追溯性**：可以指出資訊來源

## 二、向量化（Embedding）原理

### 2.1 為什麼要將文字轉換成向量？

```
原因：電腦無法直接理解文字的語義相似度

例子：
"狗是人類的好朋友" vs "犬類是人們的夥伴"
→ 字面上完全不同，但語義相似

解決方案：
將文字映射到高維向量空間，語義相似的文字在空間中距離較近
```

### 2.2 Embedding 模型如何運作？

```python
文字 → Tokenization → Transformer 編碼器 → 向量表示

具體流程：
1. "我喜歡蘋果" → ["我", "喜歡", "蘋果"]  # 分詞
2. ["我", "喜歡", "蘋果"] → [101, 2023, 3456]  # Token IDs
3. [101, 2023, 3456] → Transformer → [0.1, -0.3, 0.5, ...]  # 384維向量
```

### 2.3 為什麼選擇 all-MiniLM-L6-v2？

```
模型架構：
- 6 層 Transformer（L6 = 6 Layers）
- 384 維輸出
- 22M 參數

訓練方式：
1. 對比學習：相似句子拉近，不相似句子推遠
2. 使用 1B+ 句子對訓練
3. 知識蒸餾：從大模型學習，保持小體積

為什麼 384 維？
- 平衡點：維度太低無法捕捉複雜語義，太高則計算成本高
- 384 維足以區分大部分語義差異
- 儲存空間：384 * 4 bytes = 1.5KB per vector
```

## 三、文字分塊（Chunking）原理

### 3.1 為什麼要分塊？

```
問題：
1. Embedding 模型有輸入長度限制（通常 512 tokens）
2. 長文字的向量會失去細節資訊
3. 檢索時需要精確定位相關段落

解決方案：將長文件切割成小塊
```

### 3.2 分塊策略詳解

```python
chunk_size = 500  # 每塊 500 個字
chunk_overlap = 50  # 重疊 50 個字

為什麼要重疊？
原文：...機器學習是人工智慧的一個分支。深度學習是機器學習的子集...

不重疊切割：
塊1：...機器學習是人工智慧的一個分支。深度學
塊2：習是機器學習的子集...
→ "深度學習"被切斷，失去語義完整性

有重疊切割：
塊1：...機器學習是人工智慧的一個分支。深度學習是機器
塊2：一個分支。深度學習是機器學習的子集...
→ 保持語義連續性
```

### 3.3 最佳分塊大小的考量

```
太小（< 100 字）：
- 缺乏上下文
- 增加檢索次數
- 可能錯過重要資訊

太大（> 2000 字）：
- 包含太多不相關資訊
- 降低檢索精確度
- 超出模型輸入限制

500 字的平衡點：
- 足夠的上下文資訊
- 保持主題聚焦
- 適合大多數查詢場景
```

## 四、FAISS 向量資料庫原理

### 4.1 FAISS 是什麼？

FAISS（Facebook AI Similarity Search）是 Meta 開發的向量相似度搜尋庫。

```
核心問題：
給定查詢向量 q 和資料庫中 N 個向量，找出最相似的 K 個向量

暴力搜尋：O(N * D) 複雜度（N=向量數，D=維度）
→ 100萬個向量 × 384維 = 3.84億次運算

FAISS 優化：透過索引結構加速搜尋
```

### 4.2 IndexFlatL2 原理

我們使用的 `IndexFlatL2` 是最基礎但最精確的索引：

```python
距離計算：L2（歐幾里得）距離
d(x, y) = sqrt(Σ(xi - yi)²)

例子：
向量A = [1, 2, 3]
向量B = [4, 5, 6]
距離 = sqrt((4-1)² + (5-2)² + (6-3)²) = sqrt(27) = 5.196

為什麼用 L2 距離？
1. 直觀：就是空間中兩點的直線距離
2. 對異常值敏感：能區分細微差異
3. 數學性質好：滿足三角不等式
```

### 4.3 向量正規化的重要性

```python
為什麼要正規化（normalize_embeddings=True）？

未正規化：
向量A = [1, 2, 3]  長度 = 3.74
向量B = [10, 20, 30]  長度 = 37.4
→ B 的值都是 A 的 10 倍，但語義可能相同

正規化後：
向量A = [0.267, 0.535, 0.802]  長度 = 1
向量B = [0.267, 0.535, 0.802]  長度 = 1
→ 方向相同的向量變成相同，只比較方向不比較大小

效果：
- 餘弦相似度 = 1 - L2距離/2（正規化後）
- 更穩定的相似度計算
```

### 4.4 FAISS 索引的資料結構

```python
class FAISSVectorStore:
    def __init__(self):
        self.index = faiss.IndexFlatL2(384)  # 384維的L2索引
        self.documents = []  # 原始文件
        self.id_to_doc = {}  # ID映射

為什麼需要這三個部分？

1. index：純向量索引
   - 只儲存數值向量
   - 負責快速計算距離
   - 記憶體高效

2. documents：原始內容
   - 儲存文字和元資料
   - 搜尋結果需要返回原文

3. id_to_doc：映射關係
   - FAISS 返回的是索引位置
   - 需要映射到對應文件
```

### 4.5 搜尋流程詳解

```python
def search(query_embedding, k=5):
    # 1. 準備查詢向量
    query_embedding = normalize(query_embedding)  # 正規化
    query_embedding = query_embedding.astype(np.float32)  # FAISS要求float32

    # 2. 執行搜尋
    distances, indices = index.search(query_embedding, k)

    # 3. 返回結果
    # distances: [0.23, 0.45, 0.67, ...]  # 距離越小越相似
    # indices: [42, 156, 89, ...]  # 向量索引位置

    # 4. 映射到文件
    results = [(documents[idx], dist) for idx, dist in zip(indices, distances)]
```

### 4.6 為什麼要儲存索引？

```python
儲存的內容：
1. faiss_index.index (二進位檔案)
   - 所有向量的數值資料
   - 索引結構元資料
   - 可快速載入到記憶體

2. faiss_index.metadata (pickle檔案)
   - 原始文件內容
   - 文件元資料（來源、位置等）
   - ID映射表

優點：
- 避免重複計算embedding（耗時）
- 快速啟動（毫秒級載入）
- 支援增量更新
```

## 五、完整工作流程與原理

### 5.1 建立索引階段

```
PDF檔案
  ↓ [為什麼？需要提取可搜尋的文字]
文字提取（PyPDF2）
  ↓ [為什麼？長文字需要分塊處理]
文字分塊（500字 + 50字重疊）
  ↓ [為什麼？需要數值表示來計算相似度]
向量化（all-MiniLM-L6-v2 → 384維）
  ↓ [為什麼？需要高效的搜尋結構]
FAISS索引（IndexFlatL2）
  ↓ [為什麼？避免重複處理]
儲存到硬碟
```

### 5.2 搜尋階段

```
使用者查詢："機器學習的應用"
  ↓ [相同的向量化過程確保可比較性]
查詢向量化（384維）
  ↓ [利用FAISS高效搜尋]
計算與所有文件向量的L2距離
  ↓ [返回最相關的內容]
返回Top-K個最相似文件塊
  ↓ [可選：用於RAG生成]
將相關文件作為上下文給LLM
```

### 5.3 距離與相似度的關係

```
L2距離範圍：[0, 2]（正規化向量）

距離 → 語義關係：
0.0 - 0.3：幾乎相同的內容
0.3 - 0.6：高度相關
0.6 - 1.0：有一定相關性
1.0 - 1.5：相關性較低
1.5 - 2.0：幾乎無關

實際例子：
"機器學習" vs "深度學習"：距離約 0.4（高度相關）
"機器學習" vs "機器製造"：距離約 0.8（有一定相關）
"機器學習" vs "烹飪食譜"：距離約 1.6（幾乎無關）
```

## 六、效能優化原理

### 6.1 批次處理

```python
# 低效方式：逐個處理
for text in texts:
    embedding = model.encode(text)  # 每次都要初始化

# 高效方式：批次處理
embeddings = model.encode(texts, batch_size=32)

為什麼批次更快？
1. 矩陣運算的並行化
2. 減少 GPU/CPU 切換開銷
3. 更好的快取利用
```

### 6.2 記憶體優化

```python
# float32 vs float64
float64: 8 bytes × 384 維 × 100萬向量 = 3.07 GB
float32: 4 bytes × 384 維 × 100萬向量 = 1.54 GB

# FAISS 強制使用 float32
embeddings = embeddings.astype(np.float32)

精度損失可接受：
- 第7位小數後的差異對相似度影響極小
- 搜尋結果排序幾乎不變
```

### 6.3 索引選擇策略

```
IndexFlatL2（目前使用）：
- 優點：100% 精確搜尋
- 缺點：O(N) 複雜度
- 適用：< 100萬向量

IndexIVFFlat（大規模）：
- 將向量空間劃分為 nlist 個區域
- 搜尋時只在 nprobe 個最近區域搜尋
- 速度提升：10-100倍
- 精度損失：5-10%

IndexHNSW（圖結構）：
- 建立多層圖結構
- 速度：極快
- 記憶體：較大
- 適用：需要即時響應的場景
```

## 七、實際應用場景

### 7.1 問答系統

```
使用者："公司的退休政策是什麼？"
    ↓
檢索相關文件塊
    ↓
找到：員工手冊第 15.3 節
    ↓
LLM 生成答案："根據員工手冊，退休政策包括..."
```

### 7.2 相似文件發現

```
上傳新論文
    ↓
生成向量
    ↓
搜尋相似論文
    ↓
發現可能的抄襲或相關研究
```

### 7.3 智能推薦

```
使用者閱讀文章 A
    ↓
獲取文章 A 的向量
    ↓
找出相似文章
    ↓
推薦相關內容
```

## 八、常見問題與解答

### Q1: 為什麼不直接用關鍵字搜尋？

```
關鍵字搜尋限制：
- "汽車" 搜不到 "轎車"、"車輛"
- 無法理解同義詞、近義詞
- 無法理解語境

向量搜尋優勢：
- 理解語義相似性
- 跨語言搜尋能力
- 理解上下文關係
```

### Q2: 向量維度越高越好嗎？

```
不一定：

384維（all-MiniLM-L6-v2）：
- 速度快，儲存小
- 適合大部分應用

768維（all-mpnet-base-v2）：
- 更高精度
- 速度慢2倍，儲存大2倍
- 適合要求高精度的場景

1536維（OpenAI ada-002）：
- 最高精度
- 成本高（API收費）
- 適合商業應用
```

### Q3: 如何選擇 chunk_size？

```
根據應用場景：

問答系統：200-500 字
- 需要精確定位答案
- 較小的塊更容易匹配問題

文件摘要：500-1000 字
- 需要完整的段落上下文
- 較大的塊保持主題完整性

代碼搜尋：整個函數或類
- 保持代碼邏輯完整性
- 使用語法感知的分塊
```

## 總結

RAG 系統結合了向量搜尋和語言生成的優勢，透過 FAISS 實現高效的相似度檢索。理解這些原理有助於：

1. **優化系統設計**：選擇合適的模型和參數
2. **問題診斷**：理解為什麼某些查詢效果不好
3. **擴展功能**：基於原理開發新功能
4. **效能調優**：知道哪裡可以優化

整個系統的核心在於將非結構化文字轉換為結構化向量，利用數學方法計算語義相似度，實現智能化的資訊檢索。