# Day 2 ä¸‹åˆï¼šOllamaæœ¬åœ°éƒ¨ç½²
# 13:30-14:30 æœ¬åœ°LLMéƒ¨ç½²

import subprocess
import requests
import json
import time

# === Step 1: Ollamaä»‹ç´¹ ===
print("=== Ollamaæœ¬åœ°LLMéƒ¨ç½² ===")
print()
print("Ollama = æœ¬åœ°é‹è¡Œçš„ChatGPT")
print()
print("å„ªé»ï¼š")
print("âœ… å®Œå…¨å…è²»")
print("âœ… è³‡æ–™ä¸å¤–æµ")
print("âœ… é›¢ç·šå¯ç”¨")
print("âœ… æ”¯æ´å¤šç¨®æ¨¡å‹")
print()

# === Step 2: å®‰è£æŒ‡å— ===
print("=== å®‰è£Ollama ===")
print()
print("1. Windows/Macï¼š")
print("   è¨ªå• https://ollama.ai ä¸‹è¼‰å®‰è£æª”")
print()
print("2. Linuxï¼š")
print("   curl -fsSL https://ollama.ai/install.sh | sh")
print()
print("3. é©—è­‰å®‰è£ï¼š")
print("   åœ¨çµ‚ç«¯æ©ŸåŸ·è¡Œï¼šollama --version")
print()

# === Step 3: ä¸‹è¼‰æ¨¡å‹ ===
print("=== ä¸‹è¼‰æ¨¡å‹ ===")
print()
print("å¸¸ç”¨æ¨¡å‹ï¼š")
models = [
    {"name": "llama2", "size": "3.8GB", "desc": "Metaçš„é–‹æºæ¨¡å‹"},
    {"name": "mistral", "size": "4.1GB", "desc": "è¼•é‡ä½†å¼·å¤§"},
    {"name": "codellama", "size": "3.8GB", "desc": "ç¨‹å¼ç¢¼å°ˆç”¨"},
    {"name": "phi", "size": "1.6GB", "desc": "å¾®è»Ÿçš„å°æ¨¡å‹"},
    {"name": "gemma", "size": "1.4GB", "desc": "Googleçš„è¼•é‡æ¨¡å‹"}
]

for model in models:
    print(f"â€¢ {model['name']:10} ({model['size']:5}) - {model['desc']}")

print()
print("ä¸‹è¼‰æ¨¡å‹æŒ‡ä»¤ï¼š")
print("ollama pull llama2")
print()

# === Step 4: Ollama APIå®¢æˆ¶ç«¯ ===
class OllamaClient:
    """Ollama APIå®¢æˆ¶ç«¯"""
    
    def __init__(self, host="http://localhost:11434"):
        self.host = host
        self.api_generate = f"{host}/api/generate"
        self.api_chat = f"{host}/api/chat"
        self.api_tags = f"{host}/api/tags"
    
    def check_connection(self) -> bool:
        """æª¢æŸ¥Ollamaæ˜¯å¦é‹è¡Œ"""
        try:
            response = requests.get(self.api_tags)
            return response.status_code == 200
        except:
            return False
    
    def list_models(self) -> list:
        """åˆ—å‡ºå·²å®‰è£çš„æ¨¡å‹"""
        try:
            response = requests.get(self.api_tags)
            if response.status_code == 200:
                data = response.json()
                return [model['name'] for model in data.get('models', [])]
            return []
        except Exception as e:
            print(f"éŒ¯èª¤ï¼š{e}")
            return []
    
    def generate(self, model: str, prompt: str, stream: bool = False) -> str:
        """ç”Ÿæˆå›æ‡‰"""
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream
        }
        
        try:
            response = requests.post(self.api_generate, json=payload)
            if response.status_code == 200:
                data = response.json()
                return data.get('response', '')
            return f"éŒ¯èª¤ï¼š{response.status_code}"
        except Exception as e:
            return f"éŒ¯èª¤ï¼š{e}"
    
    def chat(self, model: str, messages: list, stream: bool = False) -> str:
        """å°è©±æ¨¡å¼"""
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream
        }
        
        try:
            response = requests.post(self.api_chat, json=payload)
            if response.status_code == 200:
                data = response.json()
                return data.get('message', {}).get('content', '')
            return f"éŒ¯èª¤ï¼š{response.status_code}"
        except Exception as e:
            return f"éŒ¯èª¤ï¼š{e}"

# === Step 5: æ¸¬è©¦Ollama ===
print("=== æ¸¬è©¦Ollamaé€£ç·š ===")
print()

ollama = OllamaClient()

if ollama.check_connection():
    print("âœ… Ollamaæ­£åœ¨é‹è¡Œ")
    
    # åˆ—å‡ºæ¨¡å‹
    models = ollama.list_models()
    if models:
        print(f"\nå·²å®‰è£çš„æ¨¡å‹ï¼š")
        for model in models:
            print(f"  â€¢ {model}")
    else:
        print("\nâŒ å°šæœªå®‰è£ä»»ä½•æ¨¡å‹")
        print("è«‹åŸ·è¡Œï¼šollama pull llama2")
else:
    print("âŒ Ollamaæœªé‹è¡Œ")
    print("\nè«‹å…ˆå•Ÿå‹•Ollamaï¼š")
    print("1. ç¢ºä¿å·²å®‰è£Ollama")
    print("2. åŸ·è¡Œï¼šollama serve")

print()

# === Step 6: ç°¡å–®å°è©±ç¯„ä¾‹ ===
def ollama_chat_demo():
    """Ollamaå°è©±ç¤ºç¯„"""
    print("=== Ollamaå°è©±ç¤ºç¯„ ===")
    print()
    
    # æª¢æŸ¥é€£ç·š
    if not ollama.check_connection():
        print("è«‹å…ˆå•Ÿå‹•Ollamaæœå‹™")
        return
    
    # é¸æ“‡æ¨¡å‹
    models = ollama.list_models()
    if not models:
        print("è«‹å…ˆä¸‹è¼‰æ¨¡å‹ï¼šollama pull llama2")
        return
    
    model = models[0]  # ä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨æ¨¡å‹
    print(f"ä½¿ç”¨æ¨¡å‹ï¼š{model}")
    print()
    
    # å°è©±è¿´åœˆ
    messages = []
    print("é–‹å§‹å°è©±ï¼ˆè¼¸å…¥'quit'çµæŸï¼‰")
    print("-" * 40)
    
    while True:
        user_input = input("ğŸ‘¤ ä½ ï¼š")
        if user_input.lower() == 'quit':
            break
        
        # æ·»åŠ ä½¿ç”¨è€…è¨Šæ¯
        messages.append({
            "role": "user",
            "content": user_input
        })
        
        # å–å¾—å›æ‡‰
        print("ğŸ¤– AIï¼š", end="", flush=True)
        response = ollama.chat(model, messages)
        print(response)
        
        # æ·»åŠ AIå›æ‡‰
        messages.append({
            "role": "assistant",
            "content": response
        })
        print()

# === Step 7: æ¯”è¼ƒä¸åŒæ¨¡å‹ ===
def compare_models():
    """æ¯”è¼ƒä¸åŒæ¨¡å‹çš„å›æ‡‰"""
    print("=== æ¯”è¼ƒä¸åŒæ¨¡å‹ ===")
    print()
    
    if not ollama.check_connection():
        print("Ollamaæœªé‹è¡Œ")
        return
    
    models = ollama.list_models()
    if len(models) < 2:
        print("éœ€è¦è‡³å°‘2å€‹æ¨¡å‹ä¾†æ¯”è¼ƒ")
        return
    
    prompt = "ç”¨ä¸€å¥è©±è§£é‡‹ä»€éº¼æ˜¯äººå·¥æ™ºæ…§"
    
    print(f"æ¸¬è©¦æç¤ºï¼š{prompt}")
    print("=" * 50)
    
    for model in models[:3]:  # æœ€å¤šæ¯”è¼ƒ3å€‹æ¨¡å‹
        print(f"\næ¨¡å‹ï¼š{model}")
        start_time = time.time()
        response = ollama.generate(model, prompt)
        elapsed = time.time() - start_time
        print(f"å›æ‡‰ï¼š{response}")
        print(f"æ™‚é–“ï¼š{elapsed:.2f}ç§’")

# === Step 8: æœ¬åœ°RAGç³»çµ± ===
class LocalRAG:
    """ä½¿ç”¨Ollamaçš„æœ¬åœ°RAGç³»çµ±"""
    
    def __init__(self, model="llama2"):
        self.model = model
        self.ollama = OllamaClient()
        self.knowledge_base = []
    
    def add_knowledge(self, text: str):
        """æ·»åŠ çŸ¥è­˜"""
        self.knowledge_base.append(text)
    
    def answer(self, question: str) -> str:
        """å›ç­”å•é¡Œ"""
        if not self.knowledge_base:
            return "çŸ¥è­˜åº«ç‚ºç©º"
        
        # å»ºç«‹ä¸Šä¸‹æ–‡
        context = "\n".join(self.knowledge_base)
        
        # å»ºç«‹æç¤º
        prompt = f"""åŸºæ–¼ä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œï¼š

è³‡æ–™ï¼š
{context}

å•é¡Œï¼š{question}

ç­”æ¡ˆï¼š"""
        
        return self.ollama.generate(self.model, prompt)

# === Step 9: åŸ·è¡Œç¤ºç¯„ ===
print("=== é¸æ“‡åŠŸèƒ½ ===")
print()
print("1. æ¸¬è©¦Ollamaé€£ç·š")
print("2. äº’å‹•å°è©±")
print("3. æ¯”è¼ƒæ¨¡å‹")
print("4. æœ¬åœ°RAGç³»çµ±")
print()

choice = input("é¸æ“‡ (1-4)ï¼š")

if choice == "1":
    # å·²åœ¨ä¸Šé¢åŸ·è¡Œ
    pass
elif choice == "2":
    ollama_chat_demo()
elif choice == "3":
    compare_models()
elif choice == "4":
    print("\n=== æœ¬åœ°RAGç³»çµ± ===")
    rag = LocalRAG()
    
    # æ·»åŠ çŸ¥è­˜
    rag.add_knowledge("Pythonæ˜¯1991å¹´ç”±Guido van Rossumå‰µé€ çš„ç¨‹å¼èªè¨€")
    rag.add_knowledge("Pythonå¼·èª¿ç¨‹å¼ç¢¼çš„å¯è®€æ€§å’Œç°¡æ½”æ€§")
    
    # æ¸¬è©¦å•ç­”
    question = "Pythonæ˜¯ä»€éº¼æ™‚å€™å‰µé€ çš„ï¼Ÿ"
    print(f"å•é¡Œï¼š{question}")
    print(f"ç­”æ¡ˆï¼š{rag.answer(question)}")

# === Step 10: éƒ¨ç½²å»ºè­° ===
print("\n" + "="*50)
print("=== Ollamaéƒ¨ç½²å»ºè­° ===")
print()

tips = [
    "ğŸ’¾ ç¡¬ç¢Ÿç©ºé–“ï¼šæ¯å€‹æ¨¡å‹éœ€è¦2-8GB",
    "ğŸ’» è¨˜æ†¶é«”ï¼šå»ºè­°è‡³å°‘8GB RAM",
    "ğŸš€ GPUåŠ é€Ÿï¼šæ”¯æ´NVIDIAé¡¯å¡åŠ é€Ÿ",
    "ğŸŒ APIæœå‹™ï¼šå¯ä½œç‚ºAPIæœå‹™ä¾›å…¶ä»–æ‡‰ç”¨å‘¼å«",
    "ğŸ”’ å®‰å…¨æ€§ï¼šé è¨­åªç›£è½localhost",
    "ğŸ“¦ Dockerï¼šæ”¯æ´Dockerå®¹å™¨éƒ¨ç½²"
]

for tip in tips:
    print(tip)

print()
print("ğŸ’¡ Ollamaè®“æ¯å€‹äººéƒ½èƒ½é‹è¡Œè‡ªå·±çš„AIï¼")