#!/usr/bin/env python3
"""
Gemma å¾®èª¿è…³æœ¬ - ä½¿ç”¨ PTT å…«å¦ç‰ˆè³‡æ–™
ä½¿ç”¨ Transformers + PEFT (LoRA) é€²è¡Œé«˜æ•ˆå¾®èª¿
"""

import json
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)
import os
from typing import Dict, List
from google.colab import drive
from huggingface_hub import login




from google.colab import userdata
userdata.get('HUGGING_FACE_TOKEN')

token = userdata.get('HUGGING_FACE_TOKEN')
if token:
    login(token=token)
else:
    print("Hugging Face token not found in environment variables.")


# Mount Google Drive
drive.mount('/content/drive')


# === é…ç½®åƒæ•¸ ===
MODEL_NAME = "google/gemma-3-1b-it"  
# Update the DATASET_PATH to point to the file in Google Drive
DATASET_PATH = "/content/drive/MyDrive/ptt_gossiping_dataset_100.json"
OUTPUT_DIR = "./gemma-ptt-finetuned"

# LoRA é…ç½®åƒæ•¸
LORA_R = 32  # LoRA rank
LORA_ALPHA = 64  # LoRA alpha
LORA_DROPOUT = 0.05  # LoRA dropout

# è¨“ç·´åƒæ•¸
BATCH_SIZE = 4
GRADIENT_ACCUMULATION_STEPS = 4
LEARNING_RATE = 2e-4
NUM_EPOCHS = 10
MAX_LENGTH = 512
WARMUP_STEPS = 100

# === 1. è¼‰å…¥å’Œæº–å‚™è³‡æ–™ ===
def load_ptt_dataset(file_path: str, max_samples: int = None) -> List[Dict]:
    """è¼‰å…¥ PTT è³‡æ–™é›†"""
    print(f"æ­£åœ¨è¼‰å…¥è³‡æ–™é›†: {file_path}")

    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

        for i, line in enumerate(lines):
            if max_samples and i >= max_samples:
                break

            try:
                item = json.loads(line.strip())
                text = item['text']

                # è§£ææ ¼å¼
                if "[INST]" in text and "[/INST]" in text:
                    parts = text.split("[/INST]")
                    instruction_part = parts[0].replace("[INST]", "").strip()
                    response = parts[1].replace("</s>", "").replace("<s>", "").strip()

                    # æå–å•é¡Œ
                    if "å•é¡Œï¼š" in instruction_part:
                        question = instruction_part.split("å•é¡Œï¼š")[1].strip()
                    else:
                        question = instruction_part

                    data.append({
                        "instruction": "ä½ æ˜¯ä¸€å€‹è³‡æ·±çš„ PTT é„‰æ°‘ï¼Œè«‹ç”¨å…«å¦ç‰ˆçš„é¢¨æ ¼ç°¡æ½”æœ‰åŠ›åœ°å›ç­”ä»¥ä¸‹å•é¡Œã€‚",
                        "input": question,
                        "output": response
                    })
            except Exception as e:
                print(f"è·³éç¬¬ {i+1} è¡Œ: {e}")
                continue

    print(f"æˆåŠŸè¼‰å…¥ {len(data)} å€‹è¨“ç·´æ¨£æœ¬")
    return data

def format_prompt(example: Dict) -> str:
    """æ ¼å¼åŒ–è¨“ç·´æç¤º"""
    instruction = example.get("instruction", "")
    input_text = example.get("input", "")
    output = example.get("output", "")

    # Gemma æ ¼å¼
    prompt = f"""<start_of_turn>user
{instruction}
{input_text}<end_of_turn>
<start_of_turn>model
{output}<end_of_turn>"""

    return prompt

def prepare_dataset(data: List[Dict], tokenizer) -> Dataset:
    """æº–å‚™è¨“ç·´è³‡æ–™é›†"""

    def tokenize_function(examples):
        # æ ¼å¼åŒ–æ–‡æœ¬
        texts = []
        for i in range(len(examples["instruction"])):
            text = format_prompt({
                "instruction": examples["instruction"][i],
                "input": examples["input"][i],
                "output": examples["output"][i]
            })
            texts.append(text)

        # Tokenize
        model_inputs = tokenizer(
            texts,
            max_length=MAX_LENGTH,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        # è¨­å®š labels (ç”¨æ–¼è¨ˆç®— loss)
        model_inputs["labels"] = model_inputs["input_ids"].clone()

        return model_inputs

    # è½‰æ›ç‚º HuggingFace Dataset
    dataset = Dataset.from_list(data)

    # Tokenize
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names
    )

    return tokenized_dataset

# === 2. æ¨¡å‹é…ç½® ===
def setup_model_and_tokenizer():
    """è¨­å®šæ¨¡å‹å’Œ tokenizer"""

    print(f"è¼‰å…¥æ¨¡å‹: {MODEL_NAME}")

    # è¼‰å…¥ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # 4-bit é‡åŒ–é…ç½® (æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    # è¼‰å…¥æ¨¡å‹ (ä½¿ç”¨ eager attention ä»¥é¿å…è­¦å‘Š)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        attn_implementation="eager",  # ä½¿ç”¨ eager attention for Gemma3
        use_cache=False,  # é—œé–‰ cache ä»¥ç›¸å®¹ gradient checkpointing
    )

    # æº–å‚™ k-bit è¨“ç·´
    model = prepare_model_for_kbit_training(model)

    # å•Ÿç”¨ gradient checkpointing ä»¥ç¯€çœè¨˜æ†¶é«”
    model.gradient_checkpointing_enable()

    # LoRA é…ç½®
    lora_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ],
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )

    # æ‡‰ç”¨ LoRA
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    return model, tokenizer

# === 3. è¨“ç·´é…ç½® ===
def setup_training_args():
    """è¨­å®šè¨“ç·´åƒæ•¸"""

    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        warmup_steps=WARMUP_STEPS,
        learning_rate=LEARNING_RATE,
        fp16=True,
        logging_steps=10,
        save_strategy="epoch",
        save_total_limit=2,
        load_best_model_at_end=False,
        report_to="none",
        remove_unused_columns=False,
        optim="paged_adamw_8bit",
        lr_scheduler_type="cosine",
        do_eval=False,  # ä¸é€²è¡Œè©•ä¼°
    )

    return training_args

# === 4. ä¸»ç¨‹å¼ ===
def main():
    print("=" * 50)
    print("Gemma PTT å¾®èª¿è…³æœ¬")
    print("=" * 50)

    # æª¢æŸ¥ CUDA
    if torch.cuda.is_available():
        print(f"ä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}")
    else:
        print("è­¦å‘Š: æœªåµæ¸¬åˆ° GPUï¼Œè¨“ç·´å¯èƒ½æœƒå¾ˆæ…¢")

    # è¼‰å…¥è³‡æ–™
    print("\nğŸ“Š è¼‰å…¥ PTT è³‡æ–™é›†...")
    ptt_data = load_ptt_dataset(DATASET_PATH, max_samples=100)  # ä½¿ç”¨å‰ 50 ç­†

    # åˆ†å‰²è¨“ç·´/é©—è­‰é›†
    train_size = int(0.9 * len(ptt_data))
    train_data = ptt_data[:train_size]
    val_data = ptt_data[train_size:]

    print(f"è¨“ç·´é›†: {len(train_data)} ç­†")
    print(f"é©—è­‰é›†: {len(val_data)} ç­†")

    # è¨­å®šæ¨¡å‹å’Œ tokenizer
    print("\nğŸ¤– è¼‰å…¥æ¨¡å‹...")
    model, tokenizer = setup_model_and_tokenizer()

    # æº–å‚™è³‡æ–™é›†
    print("\nğŸ“ æº–å‚™è¨“ç·´è³‡æ–™...")
    train_dataset = prepare_dataset(train_data, tokenizer)

    # è¨­å®šè¨“ç·´åƒæ•¸
    training_args = setup_training_args()

    # è³‡æ–™æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )

    # å»ºç«‹ Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
    )

    # é–‹å§‹è¨“ç·´
    print("\nğŸš€ é–‹å§‹å¾®èª¿...")
    print(f"ç¸½è¨“ç·´æ­¥æ•¸: {len(train_dataset) * NUM_EPOCHS // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)}")

    trainer.train()

    # å„²å­˜æ¨¡å‹
    print("\nğŸ’¾ å„²å­˜å¾®èª¿å¾Œçš„æ¨¡å‹...")
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    print(f"\nâœ… è¨“ç·´å®Œæˆï¼æ¨¡å‹å·²å„²å­˜è‡³: {OUTPUT_DIR}")

    # æ¸¬è©¦ç”Ÿæˆ
    print("\nğŸ§ª æ¸¬è©¦ç”Ÿæˆ...")
    test_prompts = [
        "æœ€è¿‘è‚¡å¸‚æ€éº¼æ¨£ï¼Ÿ",
        "å°åŒ—æˆ¿åƒ¹æœƒè·Œå—ï¼Ÿ",
        "æ¨è–¦ä»€éº¼ç¾é£Ÿï¼Ÿ"
    ]

    model.eval()
    for prompt in test_prompts:
        formatted_prompt = f"""<start_of_turn>user
ä½ æ˜¯ä¸€å€‹è³‡æ·±çš„ PTT é„‰æ°‘ï¼Œè«‹ç”¨å…«å¦ç‰ˆçš„é¢¨æ ¼ç°¡æ½”æœ‰åŠ›åœ°å›ç­”ä»¥ä¸‹å•é¡Œã€‚
{prompt}<end_of_turn>
<start_of_turn>model"""

        inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True,
                top_p=0.95,
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.split("<start_of_turn>model")[-1].strip()

        print(f"\nå•é¡Œ: {prompt}")
        print(f"å›ç­”: {response}")

# === 5. å®‰è£ä¾è³´æç¤º ===
def check_dependencies():
    """æª¢æŸ¥ä¾è³´"""
    required_packages = [
        "transformers",
        "peft",
        "bitsandbytes",
        "accelerate",
        "datasets",
        "torch"
    ]

    print("è«‹ç¢ºä¿å·²å®‰è£ä»¥ä¸‹å¥—ä»¶:")
    print("pip install " + " ".join(required_packages))
    print()

if __name__ == "__main__":
    print("=" * 50)
    print("Gemma PTT å¾®èª¿è…³æœ¬")
    print("=" * 50)

    # æª¢æŸ¥ä¾è³´
    check_dependencies()

    try:
        main()
    except ImportError as e:
        print(f"\nâŒ ç¼ºå°‘å¿…è¦å¥—ä»¶: {e}")
        print("\nè«‹åŸ·è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£:")
        print("pip install transformers peft bitsandbytes accelerate datasets")
    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()